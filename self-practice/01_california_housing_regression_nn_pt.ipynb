{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California Housing in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.12.1+cu102\n",
      "Device: cuda\n",
      "Created date: 2023-06-25 13:50:27.768534\n",
      "Modified date: 2023-06-25 22:24:05.354285\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# created date\n",
    "print(f\"Created date: 2023-06-25 13:50:27.768534\")\n",
    "\n",
    "# modified date\n",
    "print(f\"Modified date: {datetime.now()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dataset\n",
    "\n",
    " California Housing dataset\n",
    " > https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "           37.88      , -122.23      ],\n",
       "        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "           37.86      , -122.22      ],\n",
       "        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "           37.85      , -122.24      ],\n",
       "        ...,\n",
       "        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "           39.43      , -121.22      ],\n",
       "        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "           39.43      , -121.32      ],\n",
       "        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "           39.37      , -121.24      ]]),\n",
       " 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894]),\n",
       " 'frame': None,\n",
       " 'target_names': ['MedHouseVal'],\n",
       " 'feature_names': ['MedInc',\n",
       "  'HouseAge',\n",
       "  'AveRooms',\n",
       "  'AveBedrms',\n",
       "  'Population',\n",
       "  'AveOccup',\n",
       "  'Latitude',\n",
       "  'Longitude'],\n",
       " 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 20640\\n\\n    :Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n    :Attribute Information:\\n        - MedInc        median income in block group\\n        - HouseAge      median house age in block group\\n        - AveRooms      average number of rooms per household\\n        - AveBedrms     average number of bedrooms per household\\n        - Population    block group population\\n        - AveOccup      average number of household members\\n        - Latitude      block group latitude\\n        - Longitude     block group longitude\\n\\n    :Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nAn household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surpinsingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. topic:: References\\n\\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n      Statistics and Probability Letters, 33 (1997) 291-297\\n'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load california housing dataset\n",
    "california_dataset = fetch_california_housing()\n",
    "\n",
    "california_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features\n",
    "data = california_dataset.data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "target = california_dataset.target\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20640, 8), (20640,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to float32\n",
    "X = data.astype(np.float32)\n",
    "y = target.astype(np.float32)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train_set, and test_set\n",
    "raw_X_train, raw_X_test, raw_y_train, raw_y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_X_train: (16512, 8)\n",
      "raw_X_test: (4128, 8)\n",
      "raw_y_train: (16512,)\n",
      "raw_y_test: (4128,)\n"
     ]
    }
   ],
   "source": [
    "# check shapes\n",
    "print(f\"raw_X_train: {raw_X_train.shape}\")\n",
    "print(f\"raw_X_test: {raw_X_test.shape}\")\n",
    "print(f\"raw_y_train: {raw_y_train.shape}\")\n",
    "print(f\"raw_y_test: {raw_y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16512, 8), (4128, 8))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean and stardard deviation\n",
    "X_mean = np.mean(raw_X_train, axis=0)\n",
    "X_stddev = np.std(raw_X_train, axis=0)\n",
    "\n",
    "X_train = (raw_X_train - X_mean) / X_stddev\n",
    "X_test = (raw_X_test - X_mean) / X_stddev\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16512, 1), (4128, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshapes\n",
    "y_train  = raw_y_train.reshape(-1, 1)\n",
    "y_test  = raw_y_test.reshape(-1, 1)\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaliforniaHousingRegressionNeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(out_features=64, in_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_features=64, in_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_features=1, in_features=64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaliforniaHousingRegressionNeuralNetwork(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# model\n",
    "california_model = CaliforniaHousingRegressionNeuralNetwork()\n",
    "\n",
    "# init weights\n",
    "for module in california_model.modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(params= california_model.parameters())\n",
    "\n",
    "# copy model to device\n",
    "california_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/500 | train_loss:  0.9754 | train_loss:  0.4464\n",
      "Epoch: 1/500 | train_loss:  0.4904 | train_loss:  0.3893\n",
      "Epoch: 2/500 | train_loss:  0.3684 | train_loss:  0.3700\n",
      "Epoch: 3/500 | train_loss:  0.3994 | train_loss:  0.3616\n",
      "Epoch: 4/500 | train_loss:  0.3575 | train_loss:  0.3388\n",
      "Epoch: 5/500 | train_loss:  0.3338 | train_loss:  0.3365\n",
      "Epoch: 6/500 | train_loss:  0.3239 | train_loss:  0.3245\n",
      "Epoch: 7/500 | train_loss:  0.3157 | train_loss:  0.3132\n",
      "Epoch: 8/500 | train_loss:  0.3080 | train_loss:  0.3172\n",
      "Epoch: 9/500 | train_loss:  0.3000 | train_loss:  0.3086\n",
      "Epoch: 10/500 | train_loss:  0.3143 | train_loss:  0.3119\n",
      "Epoch: 11/500 | train_loss:  0.3340 | train_loss:  0.3258\n",
      "Epoch: 12/500 | train_loss:  0.2983 | train_loss:  0.3048\n",
      "Epoch: 13/500 | train_loss:  0.2912 | train_loss:  0.2976\n",
      "Epoch: 14/500 | train_loss:  0.2860 | train_loss:  0.2961\n",
      "Epoch: 15/500 | train_loss:  0.2817 | train_loss:  0.2919\n",
      "Epoch: 16/500 | train_loss:  0.2880 | train_loss:  0.3104\n",
      "Epoch: 17/500 | train_loss:  0.2783 | train_loss:  0.3050\n",
      "Epoch: 18/500 | train_loss:  0.2760 | train_loss:  0.3049\n",
      "Epoch: 19/500 | train_loss:  0.2754 | train_loss:  0.3072\n",
      "Epoch: 20/500 | train_loss:  0.2854 | train_loss:  0.2914\n",
      "Epoch: 21/500 | train_loss:  0.2705 | train_loss:  0.2908\n",
      "Epoch: 22/500 | train_loss:  0.2697 | train_loss:  0.2988\n",
      "Epoch: 23/500 | train_loss:  0.2694 | train_loss:  0.2871\n",
      "Epoch: 24/500 | train_loss:  0.2685 | train_loss:  0.2972\n",
      "Epoch: 25/500 | train_loss:  0.2724 | train_loss:  0.2816\n",
      "Epoch: 26/500 | train_loss:  0.2640 | train_loss:  0.2819\n",
      "Epoch: 27/500 | train_loss:  0.2625 | train_loss:  0.2836\n",
      "Epoch: 28/500 | train_loss:  0.2634 | train_loss:  0.2831\n",
      "Epoch: 29/500 | train_loss:  0.2620 | train_loss:  0.2850\n",
      "Epoch: 30/500 | train_loss:  0.2617 | train_loss:  0.2867\n",
      "Epoch: 31/500 | train_loss:  0.2603 | train_loss:  0.2789\n",
      "Epoch: 32/500 | train_loss:  0.2602 | train_loss:  0.2810\n",
      "Epoch: 33/500 | train_loss:  0.2584 | train_loss:  0.2770\n",
      "Epoch: 34/500 | train_loss:  0.2574 | train_loss:  0.2853\n",
      "Epoch: 35/500 | train_loss:  0.2541 | train_loss:  0.2781\n",
      "Epoch: 36/500 | train_loss:  0.2542 | train_loss:  0.2779\n",
      "Epoch: 37/500 | train_loss:  0.2526 | train_loss:  0.2766\n",
      "Epoch: 38/500 | train_loss:  0.2517 | train_loss:  0.2852\n",
      "Epoch: 39/500 | train_loss:  0.2519 | train_loss:  0.2916\n",
      "Epoch: 40/500 | train_loss:  0.2512 | train_loss:  0.2780\n",
      "Epoch: 41/500 | train_loss:  0.2477 | train_loss:  0.2807\n",
      "Epoch: 42/500 | train_loss:  0.2483 | train_loss:  0.2762\n",
      "Epoch: 43/500 | train_loss:  0.2482 | train_loss:  0.2839\n",
      "Epoch: 44/500 | train_loss:  0.2476 | train_loss:  0.2759\n",
      "Epoch: 45/500 | train_loss:  0.2454 | train_loss:  0.2760\n",
      "Epoch: 46/500 | train_loss:  0.2429 | train_loss:  0.2817\n",
      "Epoch: 47/500 | train_loss:  0.2437 | train_loss:  0.2692\n",
      "Epoch: 48/500 | train_loss:  0.2430 | train_loss:  0.2712\n",
      "Epoch: 49/500 | train_loss:  0.2434 | train_loss:  0.2729\n",
      "Epoch: 50/500 | train_loss:  0.2405 | train_loss:  0.2705\n",
      "Epoch: 51/500 | train_loss:  0.2408 | train_loss:  0.2774\n",
      "Epoch: 52/500 | train_loss:  0.2401 | train_loss:  0.2777\n",
      "Epoch: 53/500 | train_loss:  0.2388 | train_loss:  0.2740\n",
      "Epoch: 54/500 | train_loss:  0.2367 | train_loss:  0.2735\n",
      "Epoch: 55/500 | train_loss:  0.2377 | train_loss:  0.2962\n",
      "Epoch: 56/500 | train_loss:  0.2350 | train_loss:  0.2719\n",
      "Epoch: 57/500 | train_loss:  0.2350 | train_loss:  0.2759\n",
      "Epoch: 58/500 | train_loss:  0.2334 | train_loss:  0.2769\n",
      "Epoch: 59/500 | train_loss:  0.2378 | train_loss:  0.2771\n",
      "Epoch: 60/500 | train_loss:  0.2348 | train_loss:  0.2702\n",
      "Epoch: 61/500 | train_loss:  0.2310 | train_loss:  0.2764\n",
      "Epoch: 62/500 | train_loss:  0.2312 | train_loss:  0.2713\n",
      "Epoch: 63/500 | train_loss:  0.2335 | train_loss:  0.2780\n",
      "Epoch: 64/500 | train_loss:  0.2357 | train_loss:  0.2708\n",
      "Epoch: 65/500 | train_loss:  0.2302 | train_loss:  0.2978\n",
      "Epoch: 66/500 | train_loss:  0.2295 | train_loss:  0.2751\n",
      "Epoch: 67/500 | train_loss:  0.2264 | train_loss:  0.2758\n",
      "Epoch: 68/500 | train_loss:  0.2276 | train_loss:  0.2781\n",
      "Epoch: 69/500 | train_loss:  0.2264 | train_loss:  0.2757\n",
      "Epoch: 70/500 | train_loss:  0.2263 | train_loss:  0.2710\n",
      "Epoch: 71/500 | train_loss:  0.2262 | train_loss:  0.2727\n",
      "Epoch: 72/500 | train_loss:  0.2232 | train_loss:  0.2766\n",
      "Epoch: 73/500 | train_loss:  0.2265 | train_loss:  0.2821\n",
      "Epoch: 74/500 | train_loss:  0.2244 | train_loss:  0.2771\n",
      "Epoch: 75/500 | train_loss:  0.2243 | train_loss:  0.2694\n",
      "Epoch: 76/500 | train_loss:  0.2237 | train_loss:  0.2741\n",
      "Epoch: 77/500 | train_loss:  0.2243 | train_loss:  0.2690\n",
      "Epoch: 78/500 | train_loss:  0.2211 | train_loss:  0.2743\n",
      "Epoch: 79/500 | train_loss:  0.2208 | train_loss:  0.2841\n",
      "Epoch: 80/500 | train_loss:  0.2235 | train_loss:  0.2692\n",
      "Epoch: 81/500 | train_loss:  0.2206 | train_loss:  0.2782\n",
      "Epoch: 82/500 | train_loss:  0.2188 | train_loss:  0.2783\n",
      "Epoch: 83/500 | train_loss:  0.2196 | train_loss:  0.2755\n",
      "Epoch: 84/500 | train_loss:  0.2182 | train_loss:  0.2748\n",
      "Epoch: 85/500 | train_loss:  0.2179 | train_loss:  0.2687\n",
      "Epoch: 86/500 | train_loss:  0.2177 | train_loss:  0.2774\n",
      "Epoch: 87/500 | train_loss:  0.2159 | train_loss:  0.2763\n",
      "Epoch: 88/500 | train_loss:  0.2166 | train_loss:  0.2711\n",
      "Epoch: 89/500 | train_loss:  0.2142 | train_loss:  0.2767\n",
      "Epoch: 90/500 | train_loss:  0.2146 | train_loss:  0.2736\n",
      "Epoch: 91/500 | train_loss:  0.2136 | train_loss:  0.2810\n",
      "Epoch: 92/500 | train_loss:  0.2170 | train_loss:  0.2669\n",
      "Epoch: 93/500 | train_loss:  0.2136 | train_loss:  0.2868\n",
      "Epoch: 94/500 | train_loss:  0.2134 | train_loss:  0.2711\n",
      "Epoch: 95/500 | train_loss:  0.2136 | train_loss:  0.2686\n",
      "Epoch: 96/500 | train_loss:  0.2116 | train_loss:  0.2709\n",
      "Epoch: 97/500 | train_loss:  0.2164 | train_loss:  0.2693\n",
      "Epoch: 98/500 | train_loss:  0.2137 | train_loss:  0.2807\n",
      "Epoch: 99/500 | train_loss:  0.2127 | train_loss:  0.2825\n",
      "Epoch: 100/500 | train_loss:  0.2118 | train_loss:  0.2762\n",
      "Epoch: 101/500 | train_loss:  0.2094 | train_loss:  0.2753\n",
      "Epoch: 102/500 | train_loss:  0.2098 | train_loss:  0.2683\n",
      "Epoch: 103/500 | train_loss:  0.2090 | train_loss:  0.2720\n",
      "Epoch: 104/500 | train_loss:  0.2087 | train_loss:  0.2879\n",
      "Epoch: 105/500 | train_loss:  0.2092 | train_loss:  0.2718\n",
      "Epoch: 106/500 | train_loss:  0.2085 | train_loss:  0.2728\n",
      "Epoch: 107/500 | train_loss:  0.2167 | train_loss:  0.2761\n",
      "Epoch: 108/500 | train_loss:  0.2075 | train_loss:  0.2678\n",
      "Epoch: 109/500 | train_loss:  0.2075 | train_loss:  0.2686\n",
      "Epoch: 110/500 | train_loss:  0.2119 | train_loss:  0.2729\n",
      "Epoch: 111/500 | train_loss:  0.2066 | train_loss:  0.2737\n",
      "Epoch: 112/500 | train_loss:  0.2055 | train_loss:  0.2764\n",
      "Epoch: 113/500 | train_loss:  0.2070 | train_loss:  0.2769\n",
      "Epoch: 114/500 | train_loss:  0.2036 | train_loss:  0.2826\n",
      "Epoch: 115/500 | train_loss:  0.2034 | train_loss:  0.2722\n",
      "Epoch: 116/500 | train_loss:  0.2035 | train_loss:  0.2764\n",
      "Epoch: 117/500 | train_loss:  0.2029 | train_loss:  0.2698\n",
      "Epoch: 118/500 | train_loss:  0.2044 | train_loss:  0.2765\n",
      "Epoch: 119/500 | train_loss:  0.2026 | train_loss:  0.2747\n",
      "Epoch: 120/500 | train_loss:  0.2032 | train_loss:  0.2909\n",
      "Epoch: 121/500 | train_loss:  0.2017 | train_loss:  0.2791\n",
      "Epoch: 122/500 | train_loss:  0.2013 | train_loss:  0.2785\n",
      "Epoch: 123/500 | train_loss:  0.2067 | train_loss:  0.2808\n",
      "Epoch: 124/500 | train_loss:  0.2019 | train_loss:  0.2685\n",
      "Epoch: 125/500 | train_loss:  0.2021 | train_loss:  0.2728\n",
      "Epoch: 126/500 | train_loss:  0.2012 | train_loss:  0.2726\n",
      "Epoch: 127/500 | train_loss:  0.2002 | train_loss:  0.2703\n",
      "Epoch: 128/500 | train_loss:  0.2001 | train_loss:  0.2722\n",
      "Epoch: 129/500 | train_loss:  0.2014 | train_loss:  0.2671\n",
      "Epoch: 130/500 | train_loss:  0.1979 | train_loss:  0.2691\n",
      "Epoch: 131/500 | train_loss:  0.1986 | train_loss:  0.2702\n",
      "Epoch: 132/500 | train_loss:  0.1986 | train_loss:  0.2803\n",
      "Epoch: 133/500 | train_loss:  0.1982 | train_loss:  0.2784\n",
      "Epoch: 134/500 | train_loss:  0.1976 | train_loss:  0.2705\n",
      "Epoch: 135/500 | train_loss:  0.1969 | train_loss:  0.2783\n",
      "Epoch: 136/500 | train_loss:  0.1982 | train_loss:  0.2698\n",
      "Epoch: 137/500 | train_loss:  0.1988 | train_loss:  0.2713\n",
      "Epoch: 138/500 | train_loss:  0.1968 | train_loss:  0.2735\n",
      "Epoch: 139/500 | train_loss:  0.1972 | train_loss:  0.2720\n",
      "Epoch: 140/500 | train_loss:  0.1964 | train_loss:  0.2781\n",
      "Epoch: 141/500 | train_loss:  0.1988 | train_loss:  0.2740\n",
      "Epoch: 142/500 | train_loss:  0.1962 | train_loss:  0.2673\n",
      "Epoch: 143/500 | train_loss:  0.1948 | train_loss:  0.2739\n",
      "Epoch: 144/500 | train_loss:  0.1966 | train_loss:  0.2827\n",
      "Epoch: 145/500 | train_loss:  0.1945 | train_loss:  0.2727\n",
      "Epoch: 146/500 | train_loss:  0.1949 | train_loss:  0.2766\n",
      "Epoch: 147/500 | train_loss:  0.2062 | train_loss:  0.2701\n",
      "Epoch: 148/500 | train_loss:  0.1952 | train_loss:  0.2743\n",
      "Epoch: 149/500 | train_loss:  0.1977 | train_loss:  0.2710\n",
      "Epoch: 150/500 | train_loss:  0.1953 | train_loss:  0.2720\n",
      "Epoch: 151/500 | train_loss:  0.1947 | train_loss:  0.2692\n",
      "Epoch: 152/500 | train_loss:  0.1928 | train_loss:  0.2842\n",
      "Epoch: 153/500 | train_loss:  0.1917 | train_loss:  0.2767\n",
      "Epoch: 154/500 | train_loss:  0.1924 | train_loss:  0.2695\n",
      "Epoch: 155/500 | train_loss:  0.1927 | train_loss:  0.2741\n",
      "Epoch: 156/500 | train_loss:  0.1915 | train_loss:  0.2715\n",
      "Epoch: 157/500 | train_loss:  0.1923 | train_loss:  0.2776\n",
      "Epoch: 158/500 | train_loss:  0.1912 | train_loss:  0.2700\n",
      "Epoch: 159/500 | train_loss:  0.1918 | train_loss:  0.2739\n",
      "Epoch: 160/500 | train_loss:  0.1925 | train_loss:  0.2746\n",
      "Epoch: 161/500 | train_loss:  0.1905 | train_loss:  0.2795\n",
      "Epoch: 162/500 | train_loss:  0.1911 | train_loss:  0.2673\n",
      "Epoch: 163/500 | train_loss:  0.1922 | train_loss:  0.2752\n",
      "Epoch: 164/500 | train_loss:  0.1913 | train_loss:  0.2746\n",
      "Epoch: 165/500 | train_loss:  0.1959 | train_loss:  0.2750\n",
      "Epoch: 166/500 | train_loss:  0.2034 | train_loss:  0.2749\n",
      "Epoch: 167/500 | train_loss:  0.1903 | train_loss:  0.2640\n",
      "Epoch: 168/500 | train_loss:  0.1890 | train_loss:  0.2682\n",
      "Epoch: 169/500 | train_loss:  0.1904 | train_loss:  0.2698\n",
      "Epoch: 170/500 | train_loss:  0.1873 | train_loss:  0.2741\n",
      "Epoch: 171/500 | train_loss:  0.1895 | train_loss:  0.2809\n",
      "Epoch: 172/500 | train_loss:  0.1877 | train_loss:  0.2799\n",
      "Epoch: 173/500 | train_loss:  0.1871 | train_loss:  0.2808\n",
      "Epoch: 174/500 | train_loss:  0.1887 | train_loss:  0.2811\n",
      "Epoch: 175/500 | train_loss:  0.1927 | train_loss:  0.2694\n",
      "Epoch: 176/500 | train_loss:  0.1863 | train_loss:  0.2868\n",
      "Epoch: 177/500 | train_loss:  0.1986 | train_loss:  0.2740\n",
      "Epoch: 178/500 | train_loss:  0.1864 | train_loss:  0.2847\n",
      "Epoch: 179/500 | train_loss:  0.1859 | train_loss:  0.2737\n",
      "Epoch: 180/500 | train_loss:  0.1905 | train_loss:  0.2751\n",
      "Epoch: 181/500 | train_loss:  0.1871 | train_loss:  0.2693\n",
      "Epoch: 182/500 | train_loss:  0.1852 | train_loss:  0.2866\n",
      "Epoch: 183/500 | train_loss:  0.1849 | train_loss:  0.2769\n",
      "Epoch: 184/500 | train_loss:  0.1865 | train_loss:  0.2677\n",
      "Epoch: 185/500 | train_loss:  0.1850 | train_loss:  0.2853\n",
      "Epoch: 186/500 | train_loss:  0.1835 | train_loss:  0.2751\n",
      "Epoch: 187/500 | train_loss:  0.1846 | train_loss:  0.2752\n",
      "Epoch: 188/500 | train_loss:  0.1828 | train_loss:  0.2662\n",
      "Epoch: 189/500 | train_loss:  0.1854 | train_loss:  0.2737\n",
      "Epoch: 190/500 | train_loss:  0.1852 | train_loss:  0.2774\n",
      "Epoch: 191/500 | train_loss:  0.1838 | train_loss:  0.2783\n",
      "Epoch: 192/500 | train_loss:  0.1830 | train_loss:  0.2789\n",
      "Epoch: 193/500 | train_loss:  0.1823 | train_loss:  0.2824\n",
      "Epoch: 194/500 | train_loss:  0.1858 | train_loss:  0.2785\n",
      "Epoch: 195/500 | train_loss:  0.1827 | train_loss:  0.2715\n",
      "Epoch: 196/500 | train_loss:  0.1805 | train_loss:  0.2721\n",
      "Epoch: 197/500 | train_loss:  0.1810 | train_loss:  0.2761\n",
      "Epoch: 198/500 | train_loss:  0.1822 | train_loss:  0.2739\n",
      "Epoch: 199/500 | train_loss:  0.1817 | train_loss:  0.2816\n",
      "Epoch: 200/500 | train_loss:  0.1809 | train_loss:  0.2710\n",
      "Epoch: 201/500 | train_loss:  0.1807 | train_loss:  0.2765\n",
      "Epoch: 202/500 | train_loss:  0.1813 | train_loss:  0.2759\n",
      "Epoch: 203/500 | train_loss:  0.1812 | train_loss:  0.2779\n",
      "Epoch: 204/500 | train_loss:  0.1801 | train_loss:  0.2782\n",
      "Epoch: 205/500 | train_loss:  0.1785 | train_loss:  0.2800\n",
      "Epoch: 206/500 | train_loss:  0.1797 | train_loss:  0.2766\n",
      "Epoch: 207/500 | train_loss:  0.1797 | train_loss:  0.2786\n",
      "Epoch: 208/500 | train_loss:  0.1799 | train_loss:  0.2700\n",
      "Epoch: 209/500 | train_loss:  0.1803 | train_loss:  0.2772\n",
      "Epoch: 210/500 | train_loss:  0.1791 | train_loss:  0.2738\n",
      "Epoch: 211/500 | train_loss:  0.1800 | train_loss:  0.2736\n",
      "Epoch: 212/500 | train_loss:  0.1800 | train_loss:  0.2742\n",
      "Epoch: 213/500 | train_loss:  0.1792 | train_loss:  0.2837\n",
      "Epoch: 214/500 | train_loss:  0.1777 | train_loss:  0.2735\n",
      "Epoch: 215/500 | train_loss:  0.1801 | train_loss:  0.2887\n",
      "Epoch: 216/500 | train_loss:  0.1771 | train_loss:  0.2794\n",
      "Epoch: 217/500 | train_loss:  0.1775 | train_loss:  0.2743\n",
      "Epoch: 218/500 | train_loss:  0.1792 | train_loss:  0.2741\n",
      "Epoch: 219/500 | train_loss:  0.1775 | train_loss:  0.2692\n",
      "Epoch: 220/500 | train_loss:  0.1778 | train_loss:  0.2775\n",
      "Epoch: 221/500 | train_loss:  0.1782 | train_loss:  0.2791\n",
      "Epoch: 222/500 | train_loss:  0.1861 | train_loss:  0.2737\n",
      "Epoch: 223/500 | train_loss:  0.1752 | train_loss:  0.2734\n",
      "Epoch: 224/500 | train_loss:  0.1762 | train_loss:  0.2805\n",
      "Epoch: 225/500 | train_loss:  0.1760 | train_loss:  0.2781\n",
      "Epoch: 226/500 | train_loss:  0.1766 | train_loss:  0.2755\n",
      "Epoch: 227/500 | train_loss:  0.1779 | train_loss:  0.2696\n",
      "Epoch: 228/500 | train_loss:  0.1761 | train_loss:  0.2830\n",
      "Epoch: 229/500 | train_loss:  0.1752 | train_loss:  0.2831\n",
      "Epoch: 230/500 | train_loss:  0.1760 | train_loss:  0.2811\n",
      "Epoch: 231/500 | train_loss:  0.1737 | train_loss:  0.2754\n",
      "Epoch: 232/500 | train_loss:  0.1752 | train_loss:  0.2792\n",
      "Epoch: 233/500 | train_loss:  0.1743 | train_loss:  0.2762\n",
      "Epoch: 234/500 | train_loss:  0.1754 | train_loss:  0.2853\n",
      "Epoch: 235/500 | train_loss:  0.1744 | train_loss:  0.2743\n",
      "Epoch: 236/500 | train_loss:  0.1733 | train_loss:  0.2713\n",
      "Epoch: 237/500 | train_loss:  0.1762 | train_loss:  0.2731\n",
      "Epoch: 238/500 | train_loss:  0.1757 | train_loss:  0.2758\n",
      "Epoch: 239/500 | train_loss:  0.1747 | train_loss:  0.2691\n",
      "Epoch: 240/500 | train_loss:  0.1737 | train_loss:  0.2759\n",
      "Epoch: 241/500 | train_loss:  0.1736 | train_loss:  0.2891\n",
      "Epoch: 242/500 | train_loss:  0.1745 | train_loss:  0.2755\n",
      "Epoch: 243/500 | train_loss:  0.1733 | train_loss:  0.2804\n",
      "Epoch: 244/500 | train_loss:  0.1729 | train_loss:  0.2735\n",
      "Epoch: 245/500 | train_loss:  0.1735 | train_loss:  0.2692\n",
      "Epoch: 246/500 | train_loss:  0.1734 | train_loss:  0.2763\n",
      "Epoch: 247/500 | train_loss:  0.1723 | train_loss:  0.2727\n",
      "Epoch: 248/500 | train_loss:  0.1741 | train_loss:  0.2695\n",
      "Epoch: 249/500 | train_loss:  0.1713 | train_loss:  0.2782\n",
      "Epoch: 250/500 | train_loss:  0.1730 | train_loss:  0.2733\n",
      "Epoch: 251/500 | train_loss:  0.1716 | train_loss:  0.2716\n",
      "Epoch: 252/500 | train_loss:  0.1726 | train_loss:  0.2800\n",
      "Epoch: 253/500 | train_loss:  0.1711 | train_loss:  0.2731\n",
      "Epoch: 254/500 | train_loss:  0.1740 | train_loss:  0.2696\n",
      "Epoch: 255/500 | train_loss:  0.1711 | train_loss:  0.2712\n",
      "Epoch: 256/500 | train_loss:  0.1722 | train_loss:  0.2681\n",
      "Epoch: 257/500 | train_loss:  0.1708 | train_loss:  0.2749\n",
      "Epoch: 258/500 | train_loss:  0.1717 | train_loss:  0.2693\n",
      "Epoch: 259/500 | train_loss:  0.1708 | train_loss:  0.2789\n",
      "Epoch: 260/500 | train_loss:  0.1701 | train_loss:  0.2766\n",
      "Epoch: 261/500 | train_loss:  0.1705 | train_loss:  0.2779\n",
      "Epoch: 262/500 | train_loss:  0.1701 | train_loss:  0.2738\n",
      "Epoch: 263/500 | train_loss:  0.1691 | train_loss:  0.2753\n",
      "Epoch: 264/500 | train_loss:  0.1702 | train_loss:  0.2713\n",
      "Epoch: 265/500 | train_loss:  0.1679 | train_loss:  0.2701\n",
      "Epoch: 266/500 | train_loss:  0.1693 | train_loss:  0.2779\n",
      "Epoch: 267/500 | train_loss:  0.1694 | train_loss:  0.2844\n",
      "Epoch: 268/500 | train_loss:  0.1701 | train_loss:  0.2841\n",
      "Epoch: 269/500 | train_loss:  0.1713 | train_loss:  0.2728\n",
      "Epoch: 270/500 | train_loss:  0.1683 | train_loss:  0.2812\n",
      "Epoch: 271/500 | train_loss:  0.1682 | train_loss:  0.2769\n",
      "Epoch: 272/500 | train_loss:  0.1680 | train_loss:  0.2827\n",
      "Epoch: 273/500 | train_loss:  0.1712 | train_loss:  0.2739\n",
      "Epoch: 274/500 | train_loss:  0.1670 | train_loss:  0.2757\n",
      "Epoch: 275/500 | train_loss:  0.1681 | train_loss:  0.2711\n",
      "Epoch: 276/500 | train_loss:  0.1677 | train_loss:  0.2749\n",
      "Epoch: 277/500 | train_loss:  0.1698 | train_loss:  0.2807\n",
      "Epoch: 278/500 | train_loss:  0.1682 | train_loss:  0.2732\n",
      "Epoch: 279/500 | train_loss:  0.1656 | train_loss:  0.2745\n",
      "Epoch: 280/500 | train_loss:  0.1672 | train_loss:  0.2783\n",
      "Epoch: 281/500 | train_loss:  0.1669 | train_loss:  0.2793\n",
      "Epoch: 282/500 | train_loss:  0.1715 | train_loss:  0.2747\n",
      "Epoch: 283/500 | train_loss:  0.1663 | train_loss:  0.2809\n",
      "Epoch: 284/500 | train_loss:  0.1691 | train_loss:  0.2757\n",
      "Epoch: 285/500 | train_loss:  0.1663 | train_loss:  0.2716\n",
      "Epoch: 286/500 | train_loss:  0.1651 | train_loss:  0.2778\n",
      "Epoch: 287/500 | train_loss:  0.1661 | train_loss:  0.2860\n",
      "Epoch: 288/500 | train_loss:  0.1662 | train_loss:  0.2697\n",
      "Epoch: 289/500 | train_loss:  0.1651 | train_loss:  0.2754\n",
      "Epoch: 290/500 | train_loss:  0.1659 | train_loss:  0.2830\n",
      "Epoch: 291/500 | train_loss:  0.1656 | train_loss:  0.2703\n",
      "Epoch: 292/500 | train_loss:  0.1656 | train_loss:  0.2718\n",
      "Epoch: 293/500 | train_loss:  0.1662 | train_loss:  0.2708\n",
      "Epoch: 294/500 | train_loss:  0.1670 | train_loss:  0.2834\n",
      "Epoch: 295/500 | train_loss:  0.1651 | train_loss:  0.2753\n",
      "Epoch: 296/500 | train_loss:  0.1652 | train_loss:  0.2731\n",
      "Epoch: 297/500 | train_loss:  0.1638 | train_loss:  0.2803\n",
      "Epoch: 298/500 | train_loss:  0.1652 | train_loss:  0.2811\n",
      "Epoch: 299/500 | train_loss:  0.1639 | train_loss:  0.2796\n",
      "Epoch: 300/500 | train_loss:  0.1640 | train_loss:  0.2865\n",
      "Epoch: 301/500 | train_loss:  0.1636 | train_loss:  0.2713\n",
      "Epoch: 302/500 | train_loss:  0.1646 | train_loss:  0.2787\n",
      "Epoch: 303/500 | train_loss:  0.1645 | train_loss:  0.2780\n",
      "Epoch: 304/500 | train_loss:  0.1645 | train_loss:  0.2805\n",
      "Epoch: 305/500 | train_loss:  0.1725 | train_loss:  0.2767\n",
      "Epoch: 306/500 | train_loss:  0.1634 | train_loss:  0.2779\n",
      "Epoch: 307/500 | train_loss:  0.1641 | train_loss:  0.2928\n",
      "Epoch: 308/500 | train_loss:  0.1641 | train_loss:  0.2768\n",
      "Epoch: 309/500 | train_loss:  0.1632 | train_loss:  0.2860\n",
      "Epoch: 310/500 | train_loss:  0.1620 | train_loss:  0.2769\n",
      "Epoch: 311/500 | train_loss:  0.1633 | train_loss:  0.2936\n",
      "Epoch: 312/500 | train_loss:  0.1642 | train_loss:  0.2829\n",
      "Epoch: 313/500 | train_loss:  0.1641 | train_loss:  0.2831\n",
      "Epoch: 314/500 | train_loss:  0.1630 | train_loss:  0.2811\n",
      "Epoch: 315/500 | train_loss:  0.1626 | train_loss:  0.2762\n",
      "Epoch: 316/500 | train_loss:  0.1617 | train_loss:  0.2774\n",
      "Epoch: 317/500 | train_loss:  0.1631 | train_loss:  0.2776\n",
      "Epoch: 318/500 | train_loss:  0.1616 | train_loss:  0.2770\n",
      "Epoch: 319/500 | train_loss:  0.1631 | train_loss:  0.2785\n",
      "Epoch: 320/500 | train_loss:  0.1613 | train_loss:  0.2762\n",
      "Epoch: 321/500 | train_loss:  0.1634 | train_loss:  0.2765\n",
      "Epoch: 322/500 | train_loss:  0.1619 | train_loss:  0.2833\n",
      "Epoch: 323/500 | train_loss:  0.1634 | train_loss:  0.2878\n",
      "Epoch: 324/500 | train_loss:  0.1620 | train_loss:  0.2858\n",
      "Epoch: 325/500 | train_loss:  0.1614 | train_loss:  0.2733\n",
      "Epoch: 326/500 | train_loss:  0.1614 | train_loss:  0.2760\n",
      "Epoch: 327/500 | train_loss:  0.1606 | train_loss:  0.2785\n",
      "Epoch: 328/500 | train_loss:  0.1630 | train_loss:  0.2759\n",
      "Epoch: 329/500 | train_loss:  0.1612 | train_loss:  0.2751\n",
      "Epoch: 330/500 | train_loss:  0.1604 | train_loss:  0.2801\n",
      "Epoch: 331/500 | train_loss:  0.1620 | train_loss:  0.2761\n",
      "Epoch: 332/500 | train_loss:  0.1602 | train_loss:  0.2892\n",
      "Epoch: 333/500 | train_loss:  0.1627 | train_loss:  0.2762\n",
      "Epoch: 334/500 | train_loss:  0.1609 | train_loss:  0.2807\n",
      "Epoch: 335/500 | train_loss:  0.1601 | train_loss:  0.2865\n",
      "Epoch: 336/500 | train_loss:  0.1593 | train_loss:  0.2848\n",
      "Epoch: 337/500 | train_loss:  0.1591 | train_loss:  0.2794\n",
      "Epoch: 338/500 | train_loss:  0.1602 | train_loss:  0.2767\n",
      "Epoch: 339/500 | train_loss:  0.1603 | train_loss:  0.2809\n",
      "Epoch: 340/500 | train_loss:  0.1613 | train_loss:  0.2797\n",
      "Epoch: 341/500 | train_loss:  0.1596 | train_loss:  0.2807\n",
      "Epoch: 342/500 | train_loss:  0.1590 | train_loss:  0.2773\n",
      "Epoch: 343/500 | train_loss:  0.1602 | train_loss:  0.2802\n",
      "Epoch: 344/500 | train_loss:  0.1570 | train_loss:  0.2917\n",
      "Epoch: 345/500 | train_loss:  0.1610 | train_loss:  0.2791\n",
      "Epoch: 346/500 | train_loss:  0.1613 | train_loss:  0.2853\n",
      "Epoch: 347/500 | train_loss:  0.1591 | train_loss:  0.2819\n",
      "Epoch: 348/500 | train_loss:  0.1586 | train_loss:  0.2815\n",
      "Epoch: 349/500 | train_loss:  0.1591 | train_loss:  0.2763\n",
      "Epoch: 350/500 | train_loss:  0.1580 | train_loss:  0.2830\n",
      "Epoch: 351/500 | train_loss:  0.1587 | train_loss:  0.2882\n",
      "Epoch: 352/500 | train_loss:  0.1583 | train_loss:  0.2830\n",
      "Epoch: 353/500 | train_loss:  0.1589 | train_loss:  0.2808\n",
      "Epoch: 354/500 | train_loss:  0.1580 | train_loss:  0.2887\n",
      "Epoch: 355/500 | train_loss:  0.1576 | train_loss:  0.2872\n",
      "Epoch: 356/500 | train_loss:  0.1585 | train_loss:  0.2778\n",
      "Epoch: 357/500 | train_loss:  0.1605 | train_loss:  0.2774\n",
      "Epoch: 358/500 | train_loss:  0.1580 | train_loss:  0.2932\n",
      "Epoch: 359/500 | train_loss:  0.1565 | train_loss:  0.2781\n",
      "Epoch: 360/500 | train_loss:  0.1567 | train_loss:  0.2894\n",
      "Epoch: 361/500 | train_loss:  0.1578 | train_loss:  0.2788\n",
      "Epoch: 362/500 | train_loss:  0.1593 | train_loss:  0.2837\n",
      "Epoch: 363/500 | train_loss:  0.1600 | train_loss:  0.2787\n",
      "Epoch: 364/500 | train_loss:  0.1574 | train_loss:  0.2784\n",
      "Epoch: 365/500 | train_loss:  0.1581 | train_loss:  0.2834\n",
      "Epoch: 366/500 | train_loss:  0.1576 | train_loss:  0.2787\n",
      "Epoch: 367/500 | train_loss:  0.1577 | train_loss:  0.2816\n",
      "Epoch: 368/500 | train_loss:  0.1572 | train_loss:  0.2807\n",
      "Epoch: 369/500 | train_loss:  0.1573 | train_loss:  0.2930\n",
      "Epoch: 370/500 | train_loss:  0.1567 | train_loss:  0.2825\n",
      "Epoch: 371/500 | train_loss:  0.1566 | train_loss:  0.2877\n",
      "Epoch: 372/500 | train_loss:  0.1575 | train_loss:  0.2752\n",
      "Epoch: 373/500 | train_loss:  0.1567 | train_loss:  0.2778\n",
      "Epoch: 374/500 | train_loss:  0.1569 | train_loss:  0.2799\n",
      "Epoch: 375/500 | train_loss:  0.1566 | train_loss:  0.2804\n",
      "Epoch: 376/500 | train_loss:  0.1555 | train_loss:  0.2898\n",
      "Epoch: 377/500 | train_loss:  0.1555 | train_loss:  0.2778\n",
      "Epoch: 378/500 | train_loss:  0.1569 | train_loss:  0.2853\n",
      "Epoch: 379/500 | train_loss:  0.1564 | train_loss:  0.2828\n",
      "Epoch: 380/500 | train_loss:  0.1563 | train_loss:  0.2795\n",
      "Epoch: 381/500 | train_loss:  0.1557 | train_loss:  0.2814\n",
      "Epoch: 382/500 | train_loss:  0.1564 | train_loss:  0.2784\n",
      "Epoch: 383/500 | train_loss:  0.1568 | train_loss:  0.2833\n",
      "Epoch: 384/500 | train_loss:  0.1548 | train_loss:  0.2801\n",
      "Epoch: 385/500 | train_loss:  0.1564 | train_loss:  0.2888\n",
      "Epoch: 386/500 | train_loss:  0.1563 | train_loss:  0.2876\n",
      "Epoch: 387/500 | train_loss:  0.1537 | train_loss:  0.2795\n",
      "Epoch: 388/500 | train_loss:  0.1569 | train_loss:  0.2837\n",
      "Epoch: 389/500 | train_loss:  0.1566 | train_loss:  0.2800\n",
      "Epoch: 390/500 | train_loss:  0.1548 | train_loss:  0.2787\n",
      "Epoch: 391/500 | train_loss:  0.1556 | train_loss:  0.2853\n",
      "Epoch: 392/500 | train_loss:  0.1562 | train_loss:  0.2907\n",
      "Epoch: 393/500 | train_loss:  0.1540 | train_loss:  0.2838\n",
      "Epoch: 394/500 | train_loss:  0.1559 | train_loss:  0.2886\n",
      "Epoch: 395/500 | train_loss:  0.1540 | train_loss:  0.2789\n",
      "Epoch: 396/500 | train_loss:  0.1549 | train_loss:  0.2823\n",
      "Epoch: 397/500 | train_loss:  0.1553 | train_loss:  0.2935\n",
      "Epoch: 398/500 | train_loss:  0.1547 | train_loss:  0.2807\n",
      "Epoch: 399/500 | train_loss:  0.1762 | train_loss:  0.2982\n",
      "Epoch: 400/500 | train_loss:  0.1556 | train_loss:  0.2899\n",
      "Epoch: 401/500 | train_loss:  0.1525 | train_loss:  0.2832\n",
      "Epoch: 402/500 | train_loss:  0.1548 | train_loss:  0.2853\n",
      "Epoch: 403/500 | train_loss:  0.1549 | train_loss:  0.2912\n",
      "Epoch: 404/500 | train_loss:  0.1533 | train_loss:  0.2825\n",
      "Epoch: 405/500 | train_loss:  0.1534 | train_loss:  0.2887\n",
      "Epoch: 406/500 | train_loss:  0.1544 | train_loss:  0.2847\n",
      "Epoch: 407/500 | train_loss:  0.1533 | train_loss:  0.2853\n",
      "Epoch: 408/500 | train_loss:  0.1524 | train_loss:  0.2790\n",
      "Epoch: 409/500 | train_loss:  0.1537 | train_loss:  0.2827\n",
      "Epoch: 410/500 | train_loss:  0.1560 | train_loss:  0.2867\n",
      "Epoch: 411/500 | train_loss:  0.1516 | train_loss:  0.2876\n",
      "Epoch: 412/500 | train_loss:  0.1536 | train_loss:  0.2870\n",
      "Epoch: 413/500 | train_loss:  0.1551 | train_loss:  0.2805\n",
      "Epoch: 414/500 | train_loss:  0.1516 | train_loss:  0.2804\n",
      "Epoch: 415/500 | train_loss:  0.1543 | train_loss:  0.2948\n",
      "Epoch: 416/500 | train_loss:  0.1528 | train_loss:  0.2943\n",
      "Epoch: 417/500 | train_loss:  0.1550 | train_loss:  0.2820\n",
      "Epoch: 418/500 | train_loss:  0.1539 | train_loss:  0.2976\n",
      "Epoch: 419/500 | train_loss:  0.1527 | train_loss:  0.2888\n",
      "Epoch: 420/500 | train_loss:  0.1531 | train_loss:  0.2840\n",
      "Epoch: 421/500 | train_loss:  0.1524 | train_loss:  0.2814\n",
      "Epoch: 422/500 | train_loss:  0.1516 | train_loss:  0.2860\n",
      "Epoch: 423/500 | train_loss:  0.1519 | train_loss:  0.2912\n",
      "Epoch: 424/500 | train_loss:  0.1533 | train_loss:  0.2885\n",
      "Epoch: 425/500 | train_loss:  0.1517 | train_loss:  0.2851\n",
      "Epoch: 426/500 | train_loss:  0.1534 | train_loss:  0.2873\n",
      "Epoch: 427/500 | train_loss:  0.1518 | train_loss:  0.3038\n",
      "Epoch: 428/500 | train_loss:  0.1533 | train_loss:  0.2965\n",
      "Epoch: 429/500 | train_loss:  0.1521 | train_loss:  0.2853\n",
      "Epoch: 430/500 | train_loss:  0.1514 | train_loss:  0.2876\n",
      "Epoch: 431/500 | train_loss:  0.1538 | train_loss:  0.2910\n",
      "Epoch: 432/500 | train_loss:  0.1521 | train_loss:  0.2900\n",
      "Epoch: 433/500 | train_loss:  0.1528 | train_loss:  0.2856\n",
      "Epoch: 434/500 | train_loss:  0.1535 | train_loss:  0.2882\n",
      "Epoch: 435/500 | train_loss:  0.1513 | train_loss:  0.2887\n",
      "Epoch: 436/500 | train_loss:  0.1513 | train_loss:  0.2876\n",
      "Epoch: 437/500 | train_loss:  0.1517 | train_loss:  0.2809\n",
      "Epoch: 438/500 | train_loss:  0.1519 | train_loss:  0.2859\n",
      "Epoch: 439/500 | train_loss:  0.1506 | train_loss:  0.2912\n",
      "Epoch: 440/500 | train_loss:  0.1529 | train_loss:  0.2925\n",
      "Epoch: 441/500 | train_loss:  0.1519 | train_loss:  0.2866\n",
      "Epoch: 442/500 | train_loss:  0.1513 | train_loss:  0.2895\n",
      "Epoch: 443/500 | train_loss:  0.1501 | train_loss:  0.2818\n",
      "Epoch: 444/500 | train_loss:  0.1516 | train_loss:  0.2819\n",
      "Epoch: 445/500 | train_loss:  0.1523 | train_loss:  0.2881\n",
      "Epoch: 446/500 | train_loss:  0.1506 | train_loss:  0.2928\n",
      "Epoch: 447/500 | train_loss:  0.1501 | train_loss:  0.2852\n",
      "Epoch: 448/500 | train_loss:  0.1502 | train_loss:  0.2971\n",
      "Epoch: 449/500 | train_loss:  0.1511 | train_loss:  0.2884\n",
      "Epoch: 450/500 | train_loss:  0.1505 | train_loss:  0.2850\n",
      "Epoch: 451/500 | train_loss:  0.1512 | train_loss:  0.2819\n",
      "Epoch: 452/500 | train_loss:  0.1521 | train_loss:  0.2895\n",
      "Epoch: 453/500 | train_loss:  0.1517 | train_loss:  0.2893\n",
      "Epoch: 454/500 | train_loss:  0.1504 | train_loss:  0.2906\n",
      "Epoch: 455/500 | train_loss:  0.1511 | train_loss:  0.2836\n",
      "Epoch: 456/500 | train_loss:  0.1504 | train_loss:  0.2948\n",
      "Epoch: 457/500 | train_loss:  0.1501 | train_loss:  0.2871\n",
      "Epoch: 458/500 | train_loss:  0.1500 | train_loss:  0.2955\n",
      "Epoch: 459/500 | train_loss:  0.1737 | train_loss:  0.2859\n",
      "Epoch: 460/500 | train_loss:  0.1505 | train_loss:  0.2841\n",
      "Epoch: 461/500 | train_loss:  0.1499 | train_loss:  0.2951\n",
      "Epoch: 462/500 | train_loss:  0.1491 | train_loss:  0.2924\n",
      "Epoch: 463/500 | train_loss:  0.1499 | train_loss:  0.2842\n",
      "Epoch: 464/500 | train_loss:  0.1506 | train_loss:  0.2920\n",
      "Epoch: 465/500 | train_loss:  0.1491 | train_loss:  0.2901\n",
      "Epoch: 466/500 | train_loss:  0.1484 | train_loss:  0.2816\n",
      "Epoch: 467/500 | train_loss:  0.1495 | train_loss:  0.2864\n",
      "Epoch: 468/500 | train_loss:  0.1500 | train_loss:  0.2820\n",
      "Epoch: 469/500 | train_loss:  0.1487 | train_loss:  0.2947\n",
      "Epoch: 470/500 | train_loss:  0.1489 | train_loss:  0.2882\n",
      "Epoch: 471/500 | train_loss:  0.1490 | train_loss:  0.2890\n",
      "Epoch: 472/500 | train_loss:  0.1506 | train_loss:  0.2826\n",
      "Epoch: 473/500 | train_loss:  0.1480 | train_loss:  0.2840\n",
      "Epoch: 474/500 | train_loss:  0.1488 | train_loss:  0.2896\n",
      "Epoch: 475/500 | train_loss:  0.1496 | train_loss:  0.2816\n",
      "Epoch: 476/500 | train_loss:  0.1489 | train_loss:  0.2838\n",
      "Epoch: 477/500 | train_loss:  0.1498 | train_loss:  0.2822\n",
      "Epoch: 478/500 | train_loss:  0.1486 | train_loss:  0.2826\n",
      "Epoch: 479/500 | train_loss:  0.1477 | train_loss:  0.2947\n",
      "Epoch: 480/500 | train_loss:  0.1486 | train_loss:  0.2930\n",
      "Epoch: 481/500 | train_loss:  0.1485 | train_loss:  0.2892\n",
      "Epoch: 482/500 | train_loss:  0.1480 | train_loss:  0.2869\n",
      "Epoch: 483/500 | train_loss:  0.1482 | train_loss:  0.2823\n",
      "Epoch: 484/500 | train_loss:  0.1486 | train_loss:  0.3048\n",
      "Epoch: 485/500 | train_loss:  0.1497 | train_loss:  0.2956\n",
      "Epoch: 486/500 | train_loss:  0.1470 | train_loss:  0.2858\n",
      "Epoch: 487/500 | train_loss:  0.1487 | train_loss:  0.2940\n",
      "Epoch: 488/500 | train_loss:  0.1462 | train_loss:  0.2874\n",
      "Epoch: 489/500 | train_loss:  0.1486 | train_loss:  0.2904\n",
      "Epoch: 490/500 | train_loss:  0.1472 | train_loss:  0.3156\n",
      "Epoch: 491/500 | train_loss:  0.1481 | train_loss:  0.2877\n",
      "Epoch: 492/500 | train_loss:  0.1488 | train_loss:  0.2872\n",
      "Epoch: 493/500 | train_loss:  0.1482 | train_loss:  0.2919\n",
      "Epoch: 494/500 | train_loss:  0.1477 | train_loss:  0.2907\n",
      "Epoch: 495/500 | train_loss:  0.1485 | train_loss:  0.2892\n",
      "Epoch: 496/500 | train_loss:  0.1477 | train_loss:  0.2882\n",
      "Epoch: 497/500 | train_loss:  0.1479 | train_loss:  0.2909\n",
      "Epoch: 498/500 | train_loss:  0.1467 | train_loss:  0.2842\n",
      "Epoch: 499/500 | train_loss:  0.1482 | train_loss:  0.2876\n"
     ]
    }
   ],
   "source": [
    "# preparing data\n",
    "batch_size = 32\n",
    "\n",
    "train_tensordataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "test_tensordataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloader\n",
    "train_loader = DataLoader(train_tensordataset, batch_size=batch_size, shuffle= True)\n",
    "test_loader = DataLoader(test_tensordataset, batch_size=batch_size, shuffle= False)\n",
    "\n",
    "# epochs\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "     ## training ----\n",
    "\n",
    "    accumulated_train_losses = 0.0\n",
    "    train_batches = 0\n",
    "\n",
    "    for input, target in train_loader:\n",
    "\n",
    "        # copy to device\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # train mode\n",
    "        california_model.train()\n",
    "\n",
    "        # forward pass\n",
    "        ouput = california_model(input)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(ouput, target)\n",
    "\n",
    "        # accumulate losses\n",
    "        accumulated_train_losses += loss\n",
    "\n",
    "        # accumulate batches\n",
    "        train_batches += 1\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss = accumulated_train_losses / train_batches\n",
    "\n",
    "    ## testing ----\n",
    "\n",
    "    # eval mode\n",
    "    california_model.eval()\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # accumulate losses\n",
    "        accumulated_test_losses = 0.0\n",
    "        test_batches = 0\n",
    "\n",
    "        for input, target in test_loader:\n",
    "\n",
    "            # copy to device\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            output = california_model(input)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            # accumulate losses\n",
    "            accumulated_test_losses += loss\n",
    "\n",
    "            # accumulate batches\n",
    "            test_batches += 1\n",
    "\n",
    "    test_loss = accumulated_test_losses / test_batches\n",
    "\n",
    "\n",
    "    print(f\"Epoch: {epoch}/{epochs} | \"\n",
    "          f\"train_loss: {train_loss.item() : 0.4f} | \"\n",
    "          f\"test_loss: {test_loss.item() : 0.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "# eval mode\n",
    "california_model.eval()\n",
    "with torch.inference_mode():\n",
    "    predictions = california_model(torch.from_numpy(X_test).to(device))\n",
    "\n",
    "predictions = predictions.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  0.45 | True value:  0.48\n",
      "Prediction:  0.69 | True value:  0.46\n",
      "Prediction:  4.95 | True value:  5.00\n",
      "Prediction:  2.33 | True value:  2.19\n"
     ]
    }
   ],
   "source": [
    "# print out top-4 predictions\n",
    "for i in range(0,4):\n",
    "    print(f\"Prediction: {predictions[i].squeeze(): 0.2f} | True value: {y_test[i].squeeze(): 0.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tadac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
