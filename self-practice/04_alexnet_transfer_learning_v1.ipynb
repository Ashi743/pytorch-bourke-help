{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet\n",
    "\n",
    "> https://github.com/pytorch/hub/blob/master/pytorch_vision_alexnet.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# pretrained \n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "\n",
    "# eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet(\n",
    "#   (features): Sequential(\n",
    "#      # the first convolutional layer\n",
    "#     (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "    \n",
    "#     # normalization, pooling layers\n",
    "#     (1): ReLU(inplace=True)\n",
    "#     (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\n",
    "#     # the second convolutional layer\n",
    "#     (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "\n",
    "#     # normalization, pooling layers\n",
    "#     (4): ReLU(inplace=True)\n",
    "#     (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "\n",
    "#     # the third convolutional layer\n",
    "#     (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "#     # normalization layer\n",
    "#     (7): ReLU(inplace=True)\n",
    "\n",
    "#     # the fourth convolutional layer\n",
    "#     (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "#     # normalization layer\n",
    "#     (9): ReLU(inplace=True)\n",
    "\n",
    "#     # the fifth convolutional layer\n",
    "#     (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "#     # normalization, pooling layers\n",
    "#     (11): ReLU(inplace=True)\n",
    "#     (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "#   )\n",
    "\n",
    "#   (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
    "\n",
    "#   (classifier): Sequential(\n",
    "      \n",
    "#     # regularization method\n",
    "#     (0): Dropout(p=0.5, inplace=False)\n",
    "\n",
    "#     # fully_connected layer\n",
    "#     (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
    "\n",
    "#     # normalization layer\n",
    "#     (2): ReLU(inplace=True)\n",
    "\n",
    "#     # regularization method\n",
    "#     (3): Dropout(p=0.5, inplace=False)\n",
    "\n",
    "#      # fully_connected layer\n",
    "#     (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "\n",
    "#     # normalization layer\n",
    "#     (5): ReLU(inplace=True)\n",
    "\n",
    "#     # fully_connected layer\n",
    "#     (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
    "#   )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 55, 55]          23,296\n",
      "              ReLU-2           [-1, 64, 55, 55]               0\n",
      "         MaxPool2d-3           [-1, 64, 27, 27]               0\n",
      "            Conv2d-4          [-1, 192, 27, 27]         307,392\n",
      "              ReLU-5          [-1, 192, 27, 27]               0\n",
      "         MaxPool2d-6          [-1, 192, 13, 13]               0\n",
      "            Conv2d-7          [-1, 384, 13, 13]         663,936\n",
      "              ReLU-8          [-1, 384, 13, 13]               0\n",
      "            Conv2d-9          [-1, 256, 13, 13]         884,992\n",
      "             ReLU-10          [-1, 256, 13, 13]               0\n",
      "           Conv2d-11          [-1, 256, 13, 13]         590,080\n",
      "             ReLU-12          [-1, 256, 13, 13]               0\n",
      "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
      "AdaptiveAvgPool2d-14            [-1, 256, 6, 6]               0\n",
      "          Dropout-15                 [-1, 9216]               0\n",
      "           Linear-16                 [-1, 4096]      37,752,832\n",
      "             ReLU-17                 [-1, 4096]               0\n",
      "          Dropout-18                 [-1, 4096]               0\n",
      "           Linear-19                 [-1, 4096]      16,781,312\n",
      "             ReLU-20                 [-1, 4096]               0\n",
      "           Linear-21                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 61,100,840\n",
      "Trainable params: 61,100,840\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 8.38\n",
      "Params size (MB): 233.08\n",
      "Estimated Total Size (MB): 242.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from torchvision import models\n",
    "\n",
    "alexnet = models.alexnet()\n",
    "\n",
    "# check summary\n",
    "summary(alexnet, batch_size=-1, input_size=(3, 224, 224), device=\"cuda\") # batch size is set to -1 meaning any batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61100840"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils import parameters_to_vector as p2v\n",
    "p2v(alexnet.parameters()).numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download an example image from the pytorch website\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x `H` x `W`), where `H` and `W` are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using `mean` = [0.485, 0.456, 0.406] and `std` = [0.229, 0.224, 0.225]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ouput: torch.Size([1, 1000])\n",
      "tensor([6.8070e-09, 4.5677e-10, 5.7809e-09, 5.2474e-10, 1.4567e-09, 5.0166e-08,\n",
      "        1.0571e-07, 1.3456e-05, 1.1150e-04, 1.7735e-08, 1.3930e-08, 1.9339e-08,\n",
      "        2.8034e-08, 4.8585e-09, 7.7248e-09, 1.3523e-09, 2.0293e-08, 1.0262e-07,\n",
      "        4.3273e-08, 3.1826e-10, 1.2125e-09, 2.6504e-06, 1.1823e-08, 3.5808e-06,\n",
      "        3.5143e-08, 1.6876e-10, 3.1054e-10, 1.1854e-09, 5.7156e-10, 4.7373e-08,\n",
      "        1.3152e-09, 4.3327e-11, 3.1284e-10, 5.4636e-10, 4.0191e-09, 1.8491e-09,\n",
      "        7.4754e-07, 9.7579e-10, 5.9732e-11, 4.2378e-10, 1.2135e-09, 2.1615e-10,\n",
      "        2.6510e-10, 1.4177e-10, 8.5561e-10, 6.3991e-10, 4.6318e-08, 4.0571e-10,\n",
      "        1.2357e-10, 1.4798e-10, 2.4508e-09, 1.4211e-09, 6.8810e-09, 1.8810e-10,\n",
      "        2.2684e-09, 2.5487e-09, 5.6846e-09, 3.4717e-09, 1.7955e-10, 8.1710e-10,\n",
      "        1.6106e-09, 5.5779e-10, 1.8480e-10, 3.3829e-10, 9.8067e-10, 6.3747e-10,\n",
      "        6.4716e-10, 3.7525e-10, 1.4146e-09, 1.4597e-11, 8.2853e-09, 7.6292e-10,\n",
      "        3.6925e-10, 4.5024e-10, 4.6367e-08, 6.3435e-10, 5.2514e-10, 1.5159e-10,\n",
      "        1.1342e-07, 2.9798e-10, 5.7359e-09, 5.4672e-08, 3.2779e-07, 3.2424e-08,\n",
      "        4.4124e-06, 1.5377e-08, 6.2740e-08, 6.1593e-10, 1.3102e-09, 6.2602e-07,\n",
      "        1.1287e-08, 1.4162e-08, 1.9081e-09, 2.6168e-09, 2.9729e-08, 2.7913e-09,\n",
      "        4.3173e-09, 3.3363e-07, 6.9287e-09, 6.0500e-05, 3.5835e-07, 1.1440e-10,\n",
      "        1.6796e-08, 1.5939e-10, 1.3808e-01, 3.4205e-08, 2.2095e-07, 8.2582e-10,\n",
      "        8.9686e-08, 2.2972e-08, 8.7657e-09, 4.7702e-10, 6.6841e-08, 1.1931e-08,\n",
      "        8.5929e-09, 4.9241e-07, 7.6180e-10, 1.1333e-07, 2.4086e-09, 9.5816e-10,\n",
      "        2.5482e-10, 5.6240e-10, 1.2194e-09, 1.1462e-09, 7.8395e-09, 2.0740e-09,\n",
      "        9.8414e-09, 1.7046e-07, 6.2022e-09, 5.4535e-08, 4.8094e-08, 2.1225e-09,\n",
      "        5.1259e-08, 4.3530e-09, 1.8442e-07, 1.4595e-07, 2.3316e-09, 9.4415e-10,\n",
      "        1.2916e-07, 2.8015e-09, 1.8448e-09, 5.5709e-10, 1.4555e-09, 2.2367e-08,\n",
      "        2.3313e-07, 1.6868e-08, 3.5818e-06, 2.8057e-09, 7.2388e-11, 9.5119e-09,\n",
      "        2.6051e-09, 7.5693e-04, 6.3230e-04, 5.7006e-04, 3.1375e-04, 1.1481e-05,\n",
      "        1.6187e-07, 3.2219e-04, 3.3600e-06, 1.1522e-08, 1.3094e-05, 2.5784e-08,\n",
      "        1.6158e-08, 2.0232e-08, 5.3403e-09, 5.7094e-10, 5.9019e-09, 1.6602e-08,\n",
      "        5.2797e-08, 8.9325e-05, 1.6113e-06, 9.1709e-09, 1.2948e-07, 2.2805e-05,\n",
      "        1.2345e-05, 4.7886e-07, 5.2114e-07, 2.6171e-07, 4.1993e-09, 3.7159e-06,\n",
      "        1.0613e-06, 1.3405e-07, 2.2079e-06, 1.8332e-07, 1.8942e-06, 3.8461e-05,\n",
      "        9.2702e-05, 6.6422e-07, 2.0919e-05, 7.0935e-08, 1.5947e-05, 7.4437e-08,\n",
      "        9.5222e-06, 7.7192e-07, 1.3062e-05, 1.0596e-06, 1.9919e-06, 2.7396e-07,\n",
      "        1.9919e-07, 1.6872e-04, 8.6748e-05, 3.6176e-07, 4.8690e-06, 1.5896e-03,\n",
      "        1.5054e-05, 8.3104e-07, 1.1763e-08, 3.0676e-04, 3.7377e-05, 7.0134e-07,\n",
      "        7.3932e-09, 4.4626e-08, 4.0460e-06, 6.1512e-08, 6.0113e-07, 5.9906e-06,\n",
      "        9.0253e-06, 7.3497e-06, 4.6568e-07, 4.4989e-07, 1.8443e-05, 4.6385e-08,\n",
      "        1.3385e-03, 9.3985e-06, 2.8815e-06, 5.7899e-08, 3.4468e-06, 2.4239e-05,\n",
      "        1.9340e-05, 1.3333e-04, 3.8364e-03, 1.2228e-03, 3.2196e-04, 3.2634e-06,\n",
      "        3.7176e-08, 1.3939e-04, 5.7729e-07, 1.2578e-07, 1.8381e-06, 3.7593e-06,\n",
      "        2.5270e-06, 2.3481e-07, 1.4851e-06, 2.0961e-08, 8.3305e-06, 2.6647e-06,\n",
      "        4.3198e-06, 2.2373e-05, 3.7230e-03, 4.6163e-04, 1.1564e-03, 5.9720e-06,\n",
      "        3.4666e-07, 8.4957e-07, 4.7159e-07, 9.1511e-07, 1.5201e-05, 2.9046e-03,\n",
      "        7.2485e-01, 5.9142e-02, 5.5247e-04, 3.8665e-04, 1.6762e-06, 4.4043e-04,\n",
      "        3.8895e-04, 1.5742e-04, 6.5567e-05, 2.0226e-04, 6.9085e-09, 5.3749e-05,\n",
      "        4.9800e-03, 4.2982e-07, 3.0967e-07, 5.6315e-05, 2.1076e-06, 1.3551e-09,\n",
      "        1.6726e-08, 3.3476e-06, 2.1116e-05, 1.2579e-02, 7.0672e-07, 2.8966e-05,\n",
      "        1.5222e-05, 3.7598e-03, 1.4162e-04, 8.6616e-05, 2.7300e-07, 5.0511e-05,\n",
      "        1.4672e-08, 8.7899e-08, 3.4618e-09, 2.1073e-07, 1.8436e-06, 7.1832e-08,\n",
      "        4.3573e-09, 8.9387e-09, 1.7716e-06, 2.6862e-10, 2.7955e-09, 4.0537e-09,\n",
      "        5.6536e-10, 6.1773e-09, 1.2735e-09, 1.8248e-10, 3.4294e-10, 3.1555e-10,\n",
      "        3.2591e-10, 6.3645e-10, 4.6419e-08, 9.8215e-09, 2.2030e-08, 6.1255e-09,\n",
      "        1.4366e-09, 6.8025e-09, 3.8946e-09, 1.4236e-09, 2.1794e-09, 2.6982e-09,\n",
      "        2.0875e-09, 1.5181e-10, 1.1097e-10, 3.2882e-08, 6.3762e-09, 8.0656e-10,\n",
      "        9.2757e-09, 7.1649e-10, 8.6641e-09, 1.7847e-07, 9.2853e-09, 4.1301e-09,\n",
      "        9.4148e-04, 7.5733e-03, 2.3099e-02, 2.7193e-04, 4.3489e-07, 2.8681e-06,\n",
      "        4.6140e-07, 1.1184e-07, 6.7274e-06, 1.1817e-08, 5.9796e-08, 3.8719e-07,\n",
      "        2.2395e-08, 1.7034e-09, 2.7505e-10, 2.2229e-07, 3.7912e-09, 1.1204e-09,\n",
      "        2.5618e-04, 2.2836e-08, 5.0972e-09, 2.7771e-10, 2.0756e-08, 2.0983e-07,\n",
      "        7.1457e-09, 1.1420e-04, 7.6827e-05, 2.6476e-05, 4.4059e-05, 2.4940e-05,\n",
      "        7.1000e-09, 1.3930e-06, 1.3364e-07, 2.2719e-08, 2.1286e-09, 1.5686e-09,\n",
      "        1.0825e-09, 2.1128e-10, 1.1022e-07, 1.7860e-09, 8.9201e-08, 3.2335e-08,\n",
      "        1.6812e-08, 1.4593e-07, 1.3741e-07, 3.2736e-09, 1.3826e-09, 4.3800e-07,\n",
      "        1.1300e-07, 5.5083e-09, 1.8565e-07, 1.2863e-08, 9.5588e-09, 6.1150e-07,\n",
      "        1.7998e-07, 7.7110e-11, 2.4605e-11, 5.9497e-09, 4.4822e-08, 2.5790e-09,\n",
      "        3.0863e-10, 2.7563e-08, 4.4370e-10, 2.6366e-09, 2.2717e-09, 3.8851e-09,\n",
      "        1.5583e-09, 5.3901e-10, 2.6106e-08, 1.2254e-09, 1.0532e-09, 6.9829e-09,\n",
      "        7.4831e-09, 7.8225e-10, 2.5219e-08, 1.4744e-07, 2.0950e-10, 1.1888e-08,\n",
      "        1.2586e-09, 8.4868e-08, 7.4490e-09, 4.1180e-09, 6.7218e-06, 8.2879e-09,\n",
      "        1.3872e-07, 4.9690e-09, 3.6104e-09, 1.6187e-07, 7.6558e-08, 2.7473e-08,\n",
      "        9.9004e-09, 3.4964e-08, 1.1602e-08, 3.6462e-09, 3.9753e-09, 8.0158e-08,\n",
      "        4.2050e-09, 8.2070e-09, 5.1906e-05, 4.9161e-06, 5.2335e-08, 1.7773e-07,\n",
      "        4.0633e-08, 3.7996e-09, 2.7704e-07, 1.6971e-07, 1.6956e-08, 1.7243e-06,\n",
      "        2.1318e-07, 5.1116e-07, 1.7492e-08, 8.6225e-09, 4.5249e-08, 1.5433e-07,\n",
      "        4.4365e-09, 2.2245e-08, 6.1864e-09, 5.0024e-09, 9.0197e-08, 7.9887e-09,\n",
      "        1.8169e-07, 8.7184e-09, 6.5326e-07, 3.4314e-08, 6.8737e-08, 5.4751e-08,\n",
      "        4.5846e-09, 5.7868e-05, 6.9066e-08, 4.5890e-08, 3.1505e-07, 4.3967e-09,\n",
      "        5.8690e-07, 7.9935e-05, 5.4774e-09, 1.4675e-09, 1.4218e-09, 1.5856e-09,\n",
      "        6.6348e-08, 2.0278e-08, 5.6530e-07, 3.4873e-07, 3.5337e-08, 1.9078e-09,\n",
      "        4.8338e-09, 3.6679e-08, 2.0519e-08, 5.7476e-09, 1.1692e-06, 2.1577e-08,\n",
      "        6.9158e-09, 1.5332e-09, 2.7978e-09, 6.1754e-08, 1.8443e-09, 1.0784e-09,\n",
      "        1.1114e-09, 4.9634e-07, 2.0435e-09, 9.7177e-07, 1.2047e-08, 6.1152e-09,\n",
      "        1.2231e-08, 8.8708e-09, 5.1060e-09, 1.8491e-09, 1.1949e-06, 2.3400e-07,\n",
      "        1.7106e-09, 3.8248e-09, 4.1258e-11, 4.6992e-08, 2.6179e-08, 7.1085e-09,\n",
      "        1.2529e-07, 2.5550e-09, 4.3761e-09, 3.3659e-10, 4.6718e-07, 8.0777e-09,\n",
      "        1.4446e-09, 6.8455e-09, 1.9916e-09, 1.4793e-08, 1.0736e-08, 1.2374e-06,\n",
      "        1.1583e-09, 3.4033e-08, 3.3178e-08, 1.6150e-06, 1.5565e-08, 7.8695e-07,\n",
      "        4.8009e-07, 4.3669e-08, 1.3459e-09, 9.5289e-10, 1.2810e-08, 2.0965e-08,\n",
      "        3.8010e-09, 3.5845e-08, 5.0294e-08, 5.5114e-09, 9.2025e-09, 7.1192e-08,\n",
      "        1.4045e-06, 3.1660e-10, 9.8939e-10, 9.4611e-07, 5.4886e-10, 5.6184e-05,\n",
      "        2.5909e-10, 6.7035e-09, 1.0369e-08, 1.3327e-06, 2.7283e-09, 1.8518e-08,\n",
      "        1.6326e-08, 5.9746e-10, 1.1371e-09, 1.7941e-08, 8.1105e-11, 4.9345e-09,\n",
      "        5.6257e-06, 1.9265e-07, 1.1502e-08, 1.3244e-08, 6.7397e-08, 1.2859e-06,\n",
      "        2.1534e-08, 1.2494e-08, 6.1064e-07, 1.1358e-09, 7.8712e-08, 3.1167e-09,\n",
      "        1.8970e-09, 7.2487e-10, 5.0097e-08, 1.0144e-08, 1.2398e-06, 6.6018e-09,\n",
      "        3.8325e-09, 3.2005e-09, 3.2838e-08, 8.3048e-09, 4.0629e-08, 2.9033e-08,\n",
      "        1.1290e-09, 8.3480e-10, 1.6125e-06, 1.2040e-09, 2.9196e-08, 8.5265e-09,\n",
      "        1.1468e-08, 1.8410e-09, 9.5686e-08, 3.0566e-08, 1.8881e-08, 2.8531e-07,\n",
      "        1.6609e-07, 8.3080e-08, 2.5496e-09, 6.4539e-09, 3.9829e-09, 7.0171e-08,\n",
      "        1.9513e-08, 7.6503e-09, 2.6935e-07, 1.3965e-10, 9.6837e-09, 1.8279e-09,\n",
      "        6.5463e-09, 6.7937e-07, 6.5247e-09, 3.6747e-08, 2.5083e-09, 3.8131e-08,\n",
      "        1.9432e-08, 1.6758e-09, 5.6792e-07, 2.4780e-08, 1.0632e-08, 8.1183e-08,\n",
      "        1.1086e-09, 2.3480e-09, 4.7264e-07, 1.7732e-08, 1.8495e-10, 4.2296e-09,\n",
      "        3.0352e-09, 2.5734e-08, 1.9754e-06, 1.6297e-06, 2.1927e-07, 3.7753e-09,\n",
      "        2.7694e-09, 5.3972e-09, 1.1133e-07, 1.4284e-08, 1.9306e-09, 3.4700e-08,\n",
      "        1.2833e-08, 1.4971e-07, 4.1140e-09, 1.9023e-09, 2.3472e-09, 6.6150e-09,\n",
      "        2.7254e-08, 1.2013e-07, 2.4505e-08, 3.4833e-08, 6.8117e-08, 6.8369e-09,\n",
      "        2.7094e-09, 2.8829e-07, 2.4285e-07, 3.8298e-08, 6.5588e-08, 5.4344e-08,\n",
      "        3.1439e-08, 9.1057e-10, 1.0938e-09, 5.9537e-09, 4.7102e-08, 8.0793e-08,\n",
      "        1.2606e-08, 4.7832e-09, 1.0051e-08, 3.2777e-06, 6.0322e-07, 3.2081e-08,\n",
      "        1.0764e-08, 1.2267e-09, 6.4551e-09, 4.4581e-09, 2.6122e-06, 1.4368e-08,\n",
      "        2.3463e-09, 1.9074e-06, 1.3432e-08, 9.4144e-10, 1.0237e-07, 1.9082e-09,\n",
      "        1.3565e-08, 8.5107e-07, 6.1944e-08, 5.3473e-09, 3.3433e-05, 1.7679e-08,\n",
      "        1.0279e-08, 1.2606e-09, 4.5150e-07, 1.4011e-06, 4.2810e-10, 1.3341e-09,\n",
      "        5.5675e-08, 1.8794e-08, 3.8569e-10, 1.2520e-10, 5.9258e-09, 2.0804e-08,\n",
      "        5.3681e-09, 3.0883e-09, 7.2766e-09, 7.2151e-09, 1.8484e-08, 1.3808e-09,\n",
      "        2.5453e-07, 8.2263e-09, 2.0368e-09, 4.2679e-08, 1.0606e-06, 1.3668e-06,\n",
      "        5.5980e-10, 7.8350e-06, 1.9792e-07, 6.1253e-09, 5.7961e-09, 2.0171e-08,\n",
      "        2.9872e-08, 4.7430e-08, 1.0620e-08, 2.5855e-08, 2.2734e-08, 6.9222e-09,\n",
      "        3.3510e-08, 4.0410e-10, 1.3931e-07, 2.4082e-08, 1.1714e-08, 1.7110e-07,\n",
      "        1.1492e-07, 8.0544e-07, 3.9599e-06, 2.5157e-06, 1.9847e-09, 1.1213e-07,\n",
      "        8.0719e-11, 2.7702e-08, 5.4577e-07, 9.7431e-10, 3.1987e-09, 1.5716e-07,\n",
      "        2.3180e-08, 1.2543e-07, 3.1227e-09, 5.6251e-09, 1.5790e-08, 3.2321e-08,\n",
      "        1.8574e-07, 9.2786e-11, 1.0611e-08, 6.2106e-09, 1.0041e-06, 4.0552e-10,\n",
      "        6.8628e-06, 7.7274e-09, 4.4743e-09, 1.9065e-08, 1.8542e-07, 1.6174e-06,\n",
      "        1.8220e-06, 1.8337e-08, 5.1188e-08, 1.0824e-07, 7.9030e-09, 1.0613e-08,\n",
      "        2.7464e-08, 1.7983e-08, 5.1203e-09, 9.9545e-09, 5.0813e-07, 2.1742e-07,\n",
      "        2.4630e-09, 1.6131e-08, 3.2757e-08, 3.2280e-07, 1.7845e-09, 2.4123e-08,\n",
      "        7.0236e-07, 2.8131e-07, 5.6890e-08, 8.9238e-09, 1.1643e-08, 5.6382e-08,\n",
      "        1.3964e-09, 1.2938e-08, 1.7509e-09, 1.1760e-09, 4.9923e-07, 8.8411e-10,\n",
      "        3.7100e-08, 1.5127e-08, 4.0543e-06, 6.3566e-09, 2.9806e-08, 3.9795e-08,\n",
      "        2.7298e-08, 1.8159e-08, 1.8953e-08, 7.0167e-09, 1.1877e-07, 2.7061e-07,\n",
      "        1.0214e-07, 7.1086e-08, 3.3969e-07, 2.5061e-07, 1.0940e-08, 3.6369e-07,\n",
      "        1.5082e-09, 1.7066e-07, 1.6614e-07, 7.8195e-09, 1.0894e-07, 2.1934e-09,\n",
      "        9.0678e-08, 8.3732e-06, 1.2119e-06, 2.9140e-08, 4.3495e-09, 1.2637e-08,\n",
      "        7.0697e-08, 4.5633e-07, 3.3207e-08, 6.0639e-08, 7.8501e-08, 4.3730e-08,\n",
      "        1.1677e-08, 4.5712e-09, 4.7168e-08, 3.9689e-09, 4.2633e-08, 1.2589e-09,\n",
      "        6.3290e-09, 1.1325e-08, 5.5017e-08, 1.7840e-08, 8.7685e-09, 9.6611e-09,\n",
      "        1.7356e-08, 2.4969e-09, 5.9009e-09, 1.8815e-08, 9.2632e-09, 4.1520e-10,\n",
      "        1.8742e-08, 1.6054e-08, 7.6502e-08, 1.3332e-07, 1.1913e-08, 2.0659e-09,\n",
      "        1.4030e-07, 1.4188e-08, 8.8770e-08, 1.8506e-08, 2.3113e-08, 1.2622e-07,\n",
      "        2.3512e-08, 2.8085e-08, 1.1518e-09, 4.9888e-08, 8.9182e-06, 4.1266e-08,\n",
      "        2.4902e-04, 2.1254e-09, 6.6519e-09, 1.9847e-08, 1.7159e-09, 2.8196e-09,\n",
      "        4.2024e-08, 2.4527e-08, 2.8620e-09, 5.8121e-07, 2.0486e-08, 9.3684e-09,\n",
      "        2.8818e-09, 1.5276e-07, 1.4825e-08, 9.2542e-09, 2.1193e-08, 8.4361e-09,\n",
      "        9.1153e-08, 2.5212e-09, 2.6768e-08, 1.6881e-09, 3.2439e-10, 6.0765e-10,\n",
      "        2.7674e-07, 1.1590e-08, 1.2344e-08, 3.6126e-07, 1.0655e-07, 6.4170e-10,\n",
      "        6.2902e-08, 2.6166e-09, 9.7981e-10, 1.4153e-08, 5.4134e-09, 2.1399e-09,\n",
      "        8.1285e-08, 3.6534e-09, 8.1398e-09, 3.0968e-08, 3.8969e-08, 2.6113e-09,\n",
      "        2.5014e-09, 1.5694e-07, 6.6811e-07, 1.8765e-07, 5.3918e-08, 1.4496e-07,\n",
      "        2.9673e-08, 5.9362e-10, 2.6931e-08, 6.4625e-08, 1.5595e-07, 1.3458e-07,\n",
      "        3.5107e-09, 6.4736e-09, 1.2750e-07, 2.0537e-08, 1.0247e-08, 1.7070e-07,\n",
      "        3.1881e-06, 2.0572e-10, 1.0949e-09, 2.0220e-09, 8.9454e-08, 9.4336e-09,\n",
      "        3.3969e-07, 2.3293e-08, 5.4318e-09, 2.5121e-07, 1.0804e-09, 1.4424e-09,\n",
      "        3.7449e-09, 2.0301e-09, 2.7729e-09, 1.5946e-08, 1.2873e-09, 3.3752e-05,\n",
      "        5.5922e-10, 6.4645e-10, 2.2454e-08, 5.8686e-10, 1.9293e-09, 1.0333e-07,\n",
      "        2.1664e-09, 2.8259e-06, 1.2867e-07, 4.4179e-09, 2.2296e-09, 9.0017e-10,\n",
      "        2.0465e-08, 3.4999e-09, 1.3820e-09, 3.8002e-09, 2.2057e-08, 2.1869e-08,\n",
      "        3.1947e-09, 1.2017e-08, 1.2054e-09, 4.0401e-10, 6.5401e-10, 1.0721e-08,\n",
      "        7.3146e-10, 1.5616e-09, 1.7674e-08, 7.1052e-09, 3.6042e-07, 2.1400e-10,\n",
      "        6.3405e-10, 4.8474e-08, 1.8102e-10, 1.9265e-09, 6.2578e-10, 5.9686e-09,\n",
      "        1.3284e-09, 3.3214e-09, 5.5321e-06, 3.2346e-08, 6.1087e-09, 2.4997e-06,\n",
      "        5.9344e-08, 8.2349e-09, 6.3035e-08, 3.1915e-07, 7.1402e-10, 1.0602e-08,\n",
      "        1.8712e-09, 2.3254e-08, 8.4889e-09, 9.4194e-08, 1.0056e-06, 5.2407e-10,\n",
      "        3.8127e-07, 2.9577e-08, 2.7757e-10, 5.8177e-07, 6.6841e-10, 5.9003e-08,\n",
      "        2.9466e-08, 1.8775e-08, 3.1848e-08, 9.6895e-09, 2.7906e-09, 6.4224e-07,\n",
      "        2.1217e-09, 1.3542e-09, 8.6860e-08, 3.6114e-07], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "input_image = Image.open(filename)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.405], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "input_tensor = preprocess(input_image)\n",
    "\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output = model(input_batch)\n",
    "\n",
    "# check shape of output\n",
    "print(f\"Shape of ouput: {output.shape}\") # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "\n",
    "# run a softmax\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "# download ImageNet labels\n",
    "# wget.download(\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samoyed 0.7248545289039612\n",
      "wallaby 0.13807834684848785\n",
      "Pomeranian 0.05914163589477539\n",
      "Angora 0.023098941892385483\n",
      "Arctic fox 0.01257919892668724\n"
     ]
    }
   ],
   "source": [
    "# read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "# show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5) # top `k` largest elements with their indexes\n",
    "for i in range(top5_prob.size(0)): # 5\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` 40% top-1 error` should mean `60% accuracy`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "\n",
    "AlexNet competed in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up. The original paper's primary result was that the depth of the model was essential for its high performance, which was computationally expensive, but made feasible due to the utilization of graphics processing units (GPUs) during training.\n",
    "\n",
    "The 1-crop error rates on the imagenet dataset with the pretrained model are listed below.\n",
    "\n",
    "| Model structure | Top-1 error | Top-5 error |\n",
    "| --------------- | ----------- | ----------- |\n",
    "|  alexnet        | 43.45       | 20.91       |\n",
    "\n",
    "### References\n",
    "\n",
    "1. [One weird trick for parallelizing convolutional neural networks](https://arxiv.org/abs/1404.5997).\n",
    "2. [ImageNet: what is top-1 and top-5 error rate?](https://stats.stackexchange.com/questions/156471/imagenet-what-is-top-1-and-top-5-error-rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tadac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
