{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.12.1+cu102\n",
      "Created date: 2023-06-24 21:25:51.478665\n",
      "Modifed date: 2023-06-25 02:23:16.350854\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "# ignore warning\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Created date\n",
    "print(f\"Created date: 2023-06-24 21:25:51.478665\")\n",
    "\n",
    "# Modified date\n",
    "print(f\"Modifed date: {datetime.now()}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load boston dataset\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "# features\n",
    "X = boston_dataset[\"data\"]\n",
    "\n",
    "# labels\n",
    "y = boston_dataset[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of features\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of labels\n",
    "y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train_set, test_set\n",
    "raw_X_train, raw_X_test, raw_y_train, raw_y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "raw_X_train = raw_X_train.astype(np.float32)\n",
    "raw_X_test = raw_X_test.astype(np.float32)\n",
    "raw_y_train = raw_y_train.astype(np.float32)\n",
    "raw_y_test = raw_y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check train_set\n",
    "raw_X_train.shape, raw_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102, 13), (102,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check test_set\n",
    "raw_X_test.shape, raw_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.6091244e+00, 1.1569307e+01, 1.0985050e+01, 7.1782179e-02,\n",
       "        5.5648381e-01, 6.3158932e+00, 6.8556465e+01, 3.8081961e+00,\n",
       "        9.3564358e+00, 4.0403217e+02, 1.8318344e+01, 3.5627826e+02,\n",
       "        1.2457352e+01], dtype=float32),\n",
       " (13,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean and standard deviation \n",
    "X_mean = np.mean(raw_X_train, axis=0)\n",
    "X_mean, X_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.8640671e+00, 2.3123781e+01, 6.8860784e+00, 2.5812620e-01,\n",
       "        1.1755869e-01, 7.0857310e-01, 2.7960257e+01, 2.1285870e+00,\n",
       "        8.5790796e+00, 1.6596674e+02, 2.2259424e+00, 9.1453209e+01,\n",
       "        7.1015739e+00], dtype=float32),\n",
       " (13,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean and standard deviation \n",
    "X_stddev = np.std(raw_X_train, axis=0)\n",
    "X_stddev, X_stddev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized train_set, test_set\n",
    "normalized_X_train = (raw_X_train - X_mean) / X_stddev\n",
    "normalized_X_test = (raw_X_test - X_mean) / X_stddev\n",
    "\n",
    "# convert to float32\n",
    "X_train = normalized_X_train.astype(np.float32)\n",
    "X_test = normalized_X_test.astype(np.float32)\n",
    "\n",
    "y_train = raw_y_train.reshape((-1, 1))\n",
    "y_test = raw_y_test.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (102, 13))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 1), (102, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to TensorDataset\n",
    "train_tensor_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "test_tensor_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size = 16\n",
    "\n",
    "# train dataloader\n",
    "train_dataloader = DataLoader(dataset=train_tensor_dataset, shuffle= True, batch_size= batch_size)\n",
    "test_dataloader = DataLoader(dataset=test_tensor_dataset, shuffle= False, batch_size= batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonHousingRegressionNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linears = nn.Sequential(\n",
    "            nn.Linear(out_features= 64, in_features=13),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_features= 64, in_features= 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features= 64, out_features= 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linears(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BostonHousingRegressionNeuralNetwork(\n",
       "  (linears): Sequential(\n",
       "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# epochs \n",
    "num_epochs = 500\n",
    "\n",
    "# model\n",
    "boston_housing_model = BostonHousingRegressionNeuralNetwork()\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(params=boston_housing_model.parameters())\n",
    "\n",
    "# init weights\n",
    "for module in boston_housing_model.modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "# copy to device\n",
    "boston_housing_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/500 |  train_loss:  571.233 |  test_loss:  473.980\n",
      "Epoch: 1/500 |  train_loss:  466.017 |  test_loss:  347.964\n",
      "Epoch: 2/500 |  train_loss:  294.703 |  test_loss:  165.105\n",
      "Epoch: 3/500 |  train_loss:  117.762 |  test_loss:  65.340\n",
      "Epoch: 4/500 |  train_loss:  57.193 |  test_loss:  48.453\n",
      "Epoch: 5/500 |  train_loss:  35.789 |  test_loss:  40.377\n",
      "Epoch: 6/500 |  train_loss:  28.728 |  test_loss:  36.868\n",
      "Epoch: 7/500 |  train_loss:  23.234 |  test_loss:  33.954\n",
      "Epoch: 8/500 |  train_loss:  21.976 |  test_loss:  32.379\n",
      "Epoch: 9/500 |  train_loss:  20.065 |  test_loss:  31.065\n",
      "Epoch: 10/500 |  train_loss:  18.950 |  test_loss:  29.783\n",
      "Epoch: 11/500 |  train_loss:  17.413 |  test_loss:  28.483\n",
      "Epoch: 12/500 |  train_loss:  17.070 |  test_loss:  27.501\n",
      "Epoch: 13/500 |  train_loss:  16.063 |  test_loss:  27.178\n",
      "Epoch: 14/500 |  train_loss:  15.811 |  test_loss:  25.900\n",
      "Epoch: 15/500 |  train_loss:  15.721 |  test_loss:  25.566\n",
      "Epoch: 16/500 |  train_loss:  15.460 |  test_loss:  25.457\n",
      "Epoch: 17/500 |  train_loss:  14.231 |  test_loss:  25.156\n",
      "Epoch: 18/500 |  train_loss:  13.712 |  test_loss:  24.103\n",
      "Epoch: 19/500 |  train_loss:  13.393 |  test_loss:  23.705\n",
      "Epoch: 20/500 |  train_loss:  12.763 |  test_loss:  23.178\n",
      "Epoch: 21/500 |  train_loss:  12.766 |  test_loss:  22.910\n",
      "Epoch: 22/500 |  train_loss:  12.055 |  test_loss:  22.216\n",
      "Epoch: 23/500 |  train_loss:  11.842 |  test_loss:  22.024\n",
      "Epoch: 24/500 |  train_loss:  11.650 |  test_loss:  21.911\n",
      "Epoch: 25/500 |  train_loss:  11.684 |  test_loss:  21.166\n",
      "Epoch: 26/500 |  train_loss:  11.235 |  test_loss:  21.672\n",
      "Epoch: 27/500 |  train_loss:  11.074 |  test_loss:  20.776\n",
      "Epoch: 28/500 |  train_loss:  10.769 |  test_loss:  21.421\n",
      "Epoch: 29/500 |  train_loss:  11.071 |  test_loss:  20.380\n",
      "Epoch: 30/500 |  train_loss:  10.744 |  test_loss:  20.667\n",
      "Epoch: 31/500 |  train_loss:  10.433 |  test_loss:  20.758\n",
      "Epoch: 32/500 |  train_loss:  10.180 |  test_loss:  19.979\n",
      "Epoch: 33/500 |  train_loss:  9.960 |  test_loss:  20.212\n",
      "Epoch: 34/500 |  train_loss:  11.402 |  test_loss:  20.164\n",
      "Epoch: 35/500 |  train_loss:  9.979 |  test_loss:  19.243\n",
      "Epoch: 36/500 |  train_loss:  9.702 |  test_loss:  19.634\n",
      "Epoch: 37/500 |  train_loss:  9.394 |  test_loss:  19.424\n",
      "Epoch: 38/500 |  train_loss:  10.062 |  test_loss:  19.410\n",
      "Epoch: 39/500 |  train_loss:  9.512 |  test_loss:  19.127\n",
      "Epoch: 40/500 |  train_loss:  10.251 |  test_loss:  19.031\n",
      "Epoch: 41/500 |  train_loss:  8.946 |  test_loss:  18.759\n",
      "Epoch: 42/500 |  train_loss:  8.889 |  test_loss:  18.912\n",
      "Epoch: 43/500 |  train_loss:  8.748 |  test_loss:  18.850\n",
      "Epoch: 44/500 |  train_loss:  8.630 |  test_loss:  18.579\n",
      "Epoch: 45/500 |  train_loss:  8.700 |  test_loss:  18.934\n",
      "Epoch: 46/500 |  train_loss:  8.346 |  test_loss:  18.863\n",
      "Epoch: 47/500 |  train_loss:  8.573 |  test_loss:  18.382\n",
      "Epoch: 48/500 |  train_loss:  8.305 |  test_loss:  18.644\n",
      "Epoch: 49/500 |  train_loss:  8.151 |  test_loss:  18.449\n",
      "Epoch: 50/500 |  train_loss:  8.315 |  test_loss:  18.105\n",
      "Epoch: 51/500 |  train_loss:  8.311 |  test_loss:  18.116\n",
      "Epoch: 52/500 |  train_loss:  8.255 |  test_loss:  17.888\n",
      "Epoch: 53/500 |  train_loss:  7.770 |  test_loss:  18.305\n",
      "Epoch: 54/500 |  train_loss:  7.788 |  test_loss:  18.237\n",
      "Epoch: 55/500 |  train_loss:  7.547 |  test_loss:  17.575\n",
      "Epoch: 56/500 |  train_loss:  8.237 |  test_loss:  17.362\n",
      "Epoch: 57/500 |  train_loss:  7.522 |  test_loss:  18.275\n",
      "Epoch: 58/500 |  train_loss:  7.822 |  test_loss:  17.563\n",
      "Epoch: 59/500 |  train_loss:  7.411 |  test_loss:  17.983\n",
      "Epoch: 60/500 |  train_loss:  7.291 |  test_loss:  17.895\n",
      "Epoch: 61/500 |  train_loss:  7.319 |  test_loss:  18.295\n",
      "Epoch: 62/500 |  train_loss:  7.119 |  test_loss:  17.267\n",
      "Epoch: 63/500 |  train_loss:  7.222 |  test_loss:  17.267\n",
      "Epoch: 64/500 |  train_loss:  6.888 |  test_loss:  17.652\n",
      "Epoch: 65/500 |  train_loss:  6.881 |  test_loss:  17.662\n",
      "Epoch: 66/500 |  train_loss:  6.706 |  test_loss:  18.017\n",
      "Epoch: 67/500 |  train_loss:  6.795 |  test_loss:  17.524\n",
      "Epoch: 68/500 |  train_loss:  6.607 |  test_loss:  17.694\n",
      "Epoch: 69/500 |  train_loss:  6.337 |  test_loss:  17.446\n",
      "Epoch: 70/500 |  train_loss:  6.498 |  test_loss:  16.884\n",
      "Epoch: 71/500 |  train_loss:  6.647 |  test_loss:  17.044\n",
      "Epoch: 72/500 |  train_loss:  6.617 |  test_loss:  17.118\n",
      "Epoch: 73/500 |  train_loss:  6.184 |  test_loss:  17.672\n",
      "Epoch: 74/500 |  train_loss:  6.157 |  test_loss:  16.521\n",
      "Epoch: 75/500 |  train_loss:  6.092 |  test_loss:  17.284\n",
      "Epoch: 76/500 |  train_loss:  6.035 |  test_loss:  17.212\n",
      "Epoch: 77/500 |  train_loss:  5.992 |  test_loss:  16.969\n",
      "Epoch: 78/500 |  train_loss:  6.181 |  test_loss:  17.004\n",
      "Epoch: 79/500 |  train_loss:  6.193 |  test_loss:  17.281\n",
      "Epoch: 80/500 |  train_loss:  6.142 |  test_loss:  16.589\n",
      "Epoch: 81/500 |  train_loss:  5.642 |  test_loss:  16.663\n",
      "Epoch: 82/500 |  train_loss:  5.883 |  test_loss:  16.532\n",
      "Epoch: 83/500 |  train_loss:  6.276 |  test_loss:  17.048\n",
      "Epoch: 84/500 |  train_loss:  5.694 |  test_loss:  16.368\n",
      "Epoch: 85/500 |  train_loss:  5.710 |  test_loss:  17.333\n",
      "Epoch: 86/500 |  train_loss:  5.569 |  test_loss:  16.797\n",
      "Epoch: 87/500 |  train_loss:  5.425 |  test_loss:  16.596\n",
      "Epoch: 88/500 |  train_loss:  5.296 |  test_loss:  16.761\n",
      "Epoch: 89/500 |  train_loss:  5.277 |  test_loss:  16.626\n",
      "Epoch: 90/500 |  train_loss:  5.171 |  test_loss:  16.568\n",
      "Epoch: 91/500 |  train_loss:  5.233 |  test_loss:  16.345\n",
      "Epoch: 92/500 |  train_loss:  5.779 |  test_loss:  16.394\n",
      "Epoch: 93/500 |  train_loss:  5.070 |  test_loss:  16.120\n",
      "Epoch: 94/500 |  train_loss:  5.222 |  test_loss:  16.707\n",
      "Epoch: 95/500 |  train_loss:  4.897 |  test_loss:  16.635\n",
      "Epoch: 96/500 |  train_loss:  5.418 |  test_loss:  16.103\n",
      "Epoch: 97/500 |  train_loss:  5.157 |  test_loss:  16.802\n",
      "Epoch: 98/500 |  train_loss:  4.691 |  test_loss:  15.987\n",
      "Epoch: 99/500 |  train_loss:  4.988 |  test_loss:  16.500\n",
      "Epoch: 100/500 |  train_loss:  4.713 |  test_loss:  16.137\n",
      "Epoch: 101/500 |  train_loss:  4.623 |  test_loss:  16.557\n",
      "Epoch: 102/500 |  train_loss:  4.762 |  test_loss:  16.470\n",
      "Epoch: 103/500 |  train_loss:  4.471 |  test_loss:  16.595\n",
      "Epoch: 104/500 |  train_loss:  4.418 |  test_loss:  16.064\n",
      "Epoch: 105/500 |  train_loss:  4.289 |  test_loss:  16.360\n",
      "Epoch: 106/500 |  train_loss:  4.303 |  test_loss:  17.311\n",
      "Epoch: 107/500 |  train_loss:  4.507 |  test_loss:  16.080\n",
      "Epoch: 108/500 |  train_loss:  4.414 |  test_loss:  15.970\n",
      "Epoch: 109/500 |  train_loss:  4.212 |  test_loss:  16.409\n",
      "Epoch: 110/500 |  train_loss:  4.417 |  test_loss:  16.298\n",
      "Epoch: 111/500 |  train_loss:  4.415 |  test_loss:  16.088\n",
      "Epoch: 112/500 |  train_loss:  4.141 |  test_loss:  16.086\n",
      "Epoch: 113/500 |  train_loss:  4.158 |  test_loss:  16.481\n",
      "Epoch: 114/500 |  train_loss:  4.187 |  test_loss:  16.202\n",
      "Epoch: 115/500 |  train_loss:  4.190 |  test_loss:  15.759\n",
      "Epoch: 116/500 |  train_loss:  4.007 |  test_loss:  16.773\n",
      "Epoch: 117/500 |  train_loss:  3.952 |  test_loss:  16.282\n",
      "Epoch: 118/500 |  train_loss:  3.791 |  test_loss:  16.121\n",
      "Epoch: 119/500 |  train_loss:  4.043 |  test_loss:  15.823\n",
      "Epoch: 120/500 |  train_loss:  3.710 |  test_loss:  16.441\n",
      "Epoch: 121/500 |  train_loss:  3.710 |  test_loss:  15.707\n",
      "Epoch: 122/500 |  train_loss:  3.706 |  test_loss:  15.575\n",
      "Epoch: 123/500 |  train_loss:  4.139 |  test_loss:  16.292\n",
      "Epoch: 124/500 |  train_loss:  3.927 |  test_loss:  15.765\n",
      "Epoch: 125/500 |  train_loss:  3.655 |  test_loss:  16.560\n",
      "Epoch: 126/500 |  train_loss:  3.907 |  test_loss:  16.010\n",
      "Epoch: 127/500 |  train_loss:  3.671 |  test_loss:  16.062\n",
      "Epoch: 128/500 |  train_loss:  3.472 |  test_loss:  16.399\n",
      "Epoch: 129/500 |  train_loss:  3.673 |  test_loss:  16.019\n",
      "Epoch: 130/500 |  train_loss:  3.477 |  test_loss:  16.299\n",
      "Epoch: 131/500 |  train_loss:  3.513 |  test_loss:  15.492\n",
      "Epoch: 132/500 |  train_loss:  3.440 |  test_loss:  16.168\n",
      "Epoch: 133/500 |  train_loss:  3.546 |  test_loss:  16.454\n",
      "Epoch: 134/500 |  train_loss:  3.424 |  test_loss:  16.191\n",
      "Epoch: 135/500 |  train_loss:  3.478 |  test_loss:  16.701\n",
      "Epoch: 136/500 |  train_loss:  3.397 |  test_loss:  17.323\n",
      "Epoch: 137/500 |  train_loss:  3.345 |  test_loss:  16.250\n",
      "Epoch: 138/500 |  train_loss:  3.723 |  test_loss:  16.107\n",
      "Epoch: 139/500 |  train_loss:  3.283 |  test_loss:  16.062\n",
      "Epoch: 140/500 |  train_loss:  3.111 |  test_loss:  15.483\n",
      "Epoch: 141/500 |  train_loss:  3.185 |  test_loss:  15.718\n",
      "Epoch: 142/500 |  train_loss:  3.251 |  test_loss:  16.086\n",
      "Epoch: 143/500 |  train_loss:  3.063 |  test_loss:  15.905\n",
      "Epoch: 144/500 |  train_loss:  3.061 |  test_loss:  16.097\n",
      "Epoch: 145/500 |  train_loss:  3.059 |  test_loss:  16.188\n",
      "Epoch: 146/500 |  train_loss:  3.001 |  test_loss:  15.847\n",
      "Epoch: 147/500 |  train_loss:  3.148 |  test_loss:  15.702\n",
      "Epoch: 148/500 |  train_loss:  2.910 |  test_loss:  16.323\n",
      "Epoch: 149/500 |  train_loss:  2.918 |  test_loss:  16.096\n",
      "Epoch: 150/500 |  train_loss:  3.074 |  test_loss:  15.733\n",
      "Epoch: 151/500 |  train_loss:  3.279 |  test_loss:  16.477\n",
      "Epoch: 152/500 |  train_loss:  3.143 |  test_loss:  16.466\n",
      "Epoch: 153/500 |  train_loss:  3.320 |  test_loss:  18.061\n",
      "Epoch: 154/500 |  train_loss:  3.235 |  test_loss:  16.977\n",
      "Epoch: 155/500 |  train_loss:  3.371 |  test_loss:  16.274\n",
      "Epoch: 156/500 |  train_loss:  2.761 |  test_loss:  15.576\n",
      "Epoch: 157/500 |  train_loss:  3.178 |  test_loss:  15.873\n",
      "Epoch: 158/500 |  train_loss:  2.753 |  test_loss:  16.649\n",
      "Epoch: 159/500 |  train_loss:  2.818 |  test_loss:  15.969\n",
      "Epoch: 160/500 |  train_loss:  2.664 |  test_loss:  16.191\n",
      "Epoch: 161/500 |  train_loss:  2.536 |  test_loss:  15.964\n",
      "Epoch: 162/500 |  train_loss:  2.742 |  test_loss:  16.532\n",
      "Epoch: 163/500 |  train_loss:  2.673 |  test_loss:  16.027\n",
      "Epoch: 164/500 |  train_loss:  2.468 |  test_loss:  16.025\n",
      "Epoch: 165/500 |  train_loss:  2.674 |  test_loss:  16.495\n",
      "Epoch: 166/500 |  train_loss:  2.587 |  test_loss:  16.040\n",
      "Epoch: 167/500 |  train_loss:  2.585 |  test_loss:  16.096\n",
      "Epoch: 168/500 |  train_loss:  2.436 |  test_loss:  16.568\n",
      "Epoch: 169/500 |  train_loss:  2.788 |  test_loss:  17.432\n",
      "Epoch: 170/500 |  train_loss:  2.911 |  test_loss:  17.162\n",
      "Epoch: 171/500 |  train_loss:  2.692 |  test_loss:  16.870\n",
      "Epoch: 172/500 |  train_loss:  2.627 |  test_loss:  16.652\n",
      "Epoch: 173/500 |  train_loss:  2.387 |  test_loss:  16.777\n",
      "Epoch: 174/500 |  train_loss:  2.477 |  test_loss:  16.866\n",
      "Epoch: 175/500 |  train_loss:  2.440 |  test_loss:  17.020\n",
      "Epoch: 176/500 |  train_loss:  2.407 |  test_loss:  16.779\n",
      "Epoch: 177/500 |  train_loss:  2.453 |  test_loss:  17.586\n",
      "Epoch: 178/500 |  train_loss:  2.555 |  test_loss:  17.011\n",
      "Epoch: 179/500 |  train_loss:  2.460 |  test_loss:  16.726\n",
      "Epoch: 180/500 |  train_loss:  2.349 |  test_loss:  16.382\n",
      "Epoch: 181/500 |  train_loss:  2.350 |  test_loss:  17.002\n",
      "Epoch: 182/500 |  train_loss:  2.394 |  test_loss:  17.216\n",
      "Epoch: 183/500 |  train_loss:  2.284 |  test_loss:  17.387\n",
      "Epoch: 184/500 |  train_loss:  2.536 |  test_loss:  17.116\n",
      "Epoch: 185/500 |  train_loss:  2.250 |  test_loss:  17.003\n",
      "Epoch: 186/500 |  train_loss:  2.420 |  test_loss:  17.388\n",
      "Epoch: 187/500 |  train_loss:  2.323 |  test_loss:  17.178\n",
      "Epoch: 188/500 |  train_loss:  2.493 |  test_loss:  17.036\n",
      "Epoch: 189/500 |  train_loss:  2.493 |  test_loss:  17.153\n",
      "Epoch: 190/500 |  train_loss:  2.304 |  test_loss:  16.741\n",
      "Epoch: 191/500 |  train_loss:  2.347 |  test_loss:  17.667\n",
      "Epoch: 192/500 |  train_loss:  2.279 |  test_loss:  17.404\n",
      "Epoch: 193/500 |  train_loss:  2.154 |  test_loss:  16.733\n",
      "Epoch: 194/500 |  train_loss:  1.971 |  test_loss:  17.508\n",
      "Epoch: 195/500 |  train_loss:  2.085 |  test_loss:  18.307\n",
      "Epoch: 196/500 |  train_loss:  2.123 |  test_loss:  16.452\n",
      "Epoch: 197/500 |  train_loss:  2.103 |  test_loss:  17.251\n",
      "Epoch: 198/500 |  train_loss:  2.012 |  test_loss:  16.835\n",
      "Epoch: 199/500 |  train_loss:  1.990 |  test_loss:  17.454\n",
      "Epoch: 200/500 |  train_loss:  1.979 |  test_loss:  17.684\n",
      "Epoch: 201/500 |  train_loss:  2.252 |  test_loss:  18.108\n",
      "Epoch: 202/500 |  train_loss:  2.025 |  test_loss:  17.968\n",
      "Epoch: 203/500 |  train_loss:  1.924 |  test_loss:  16.920\n",
      "Epoch: 204/500 |  train_loss:  2.067 |  test_loss:  17.239\n",
      "Epoch: 205/500 |  train_loss:  2.090 |  test_loss:  16.782\n",
      "Epoch: 206/500 |  train_loss:  2.162 |  test_loss:  16.881\n",
      "Epoch: 207/500 |  train_loss:  2.037 |  test_loss:  17.709\n",
      "Epoch: 208/500 |  train_loss:  2.166 |  test_loss:  17.771\n",
      "Epoch: 209/500 |  train_loss:  1.929 |  test_loss:  16.862\n",
      "Epoch: 210/500 |  train_loss:  1.928 |  test_loss:  17.406\n",
      "Epoch: 211/500 |  train_loss:  2.048 |  test_loss:  16.489\n",
      "Epoch: 212/500 |  train_loss:  1.953 |  test_loss:  16.883\n",
      "Epoch: 213/500 |  train_loss:  1.848 |  test_loss:  17.577\n",
      "Epoch: 214/500 |  train_loss:  1.863 |  test_loss:  17.690\n",
      "Epoch: 215/500 |  train_loss:  1.792 |  test_loss:  17.077\n",
      "Epoch: 216/500 |  train_loss:  1.777 |  test_loss:  17.499\n",
      "Epoch: 217/500 |  train_loss:  1.822 |  test_loss:  16.930\n",
      "Epoch: 218/500 |  train_loss:  1.691 |  test_loss:  17.041\n",
      "Epoch: 219/500 |  train_loss:  1.748 |  test_loss:  17.327\n",
      "Epoch: 220/500 |  train_loss:  1.660 |  test_loss:  17.169\n",
      "Epoch: 221/500 |  train_loss:  1.694 |  test_loss:  17.442\n",
      "Epoch: 222/500 |  train_loss:  1.689 |  test_loss:  17.473\n",
      "Epoch: 223/500 |  train_loss:  1.623 |  test_loss:  16.936\n",
      "Epoch: 224/500 |  train_loss:  1.749 |  test_loss:  17.334\n",
      "Epoch: 225/500 |  train_loss:  1.658 |  test_loss:  16.960\n",
      "Epoch: 226/500 |  train_loss:  1.841 |  test_loss:  17.066\n",
      "Epoch: 227/500 |  train_loss:  1.776 |  test_loss:  17.134\n",
      "Epoch: 228/500 |  train_loss:  1.824 |  test_loss:  17.168\n",
      "Epoch: 229/500 |  train_loss:  1.730 |  test_loss:  17.624\n",
      "Epoch: 230/500 |  train_loss:  1.651 |  test_loss:  17.741\n",
      "Epoch: 231/500 |  train_loss:  1.620 |  test_loss:  18.338\n",
      "Epoch: 232/500 |  train_loss:  1.631 |  test_loss:  17.614\n",
      "Epoch: 233/500 |  train_loss:  1.610 |  test_loss:  18.022\n",
      "Epoch: 234/500 |  train_loss:  1.838 |  test_loss:  16.760\n",
      "Epoch: 235/500 |  train_loss:  1.607 |  test_loss:  17.466\n",
      "Epoch: 236/500 |  train_loss:  1.510 |  test_loss:  16.856\n",
      "Epoch: 237/500 |  train_loss:  1.503 |  test_loss:  17.057\n",
      "Epoch: 238/500 |  train_loss:  1.514 |  test_loss:  17.119\n",
      "Epoch: 239/500 |  train_loss:  1.496 |  test_loss:  17.990\n",
      "Epoch: 240/500 |  train_loss:  1.581 |  test_loss:  17.746\n",
      "Epoch: 241/500 |  train_loss:  1.437 |  test_loss:  17.369\n",
      "Epoch: 242/500 |  train_loss:  1.526 |  test_loss:  17.218\n",
      "Epoch: 243/500 |  train_loss:  1.489 |  test_loss:  18.242\n",
      "Epoch: 244/500 |  train_loss:  1.534 |  test_loss:  19.483\n",
      "Epoch: 245/500 |  train_loss:  1.624 |  test_loss:  18.635\n",
      "Epoch: 246/500 |  train_loss:  1.587 |  test_loss:  16.501\n",
      "Epoch: 247/500 |  train_loss:  1.722 |  test_loss:  16.378\n",
      "Epoch: 248/500 |  train_loss:  1.641 |  test_loss:  16.726\n",
      "Epoch: 249/500 |  train_loss:  1.551 |  test_loss:  16.531\n",
      "Epoch: 250/500 |  train_loss:  1.460 |  test_loss:  17.746\n",
      "Epoch: 251/500 |  train_loss:  1.488 |  test_loss:  16.841\n",
      "Epoch: 252/500 |  train_loss:  1.619 |  test_loss:  17.437\n",
      "Epoch: 253/500 |  train_loss:  1.362 |  test_loss:  17.656\n",
      "Epoch: 254/500 |  train_loss:  1.402 |  test_loss:  16.799\n",
      "Epoch: 255/500 |  train_loss:  1.705 |  test_loss:  16.795\n",
      "Epoch: 256/500 |  train_loss:  1.352 |  test_loss:  16.772\n",
      "Epoch: 257/500 |  train_loss:  1.441 |  test_loss:  17.046\n",
      "Epoch: 258/500 |  train_loss:  1.570 |  test_loss:  17.148\n",
      "Epoch: 259/500 |  train_loss:  1.543 |  test_loss:  17.397\n",
      "Epoch: 260/500 |  train_loss:  1.443 |  test_loss:  17.254\n",
      "Epoch: 261/500 |  train_loss:  1.354 |  test_loss:  17.862\n",
      "Epoch: 262/500 |  train_loss:  1.629 |  test_loss:  17.666\n",
      "Epoch: 263/500 |  train_loss:  1.584 |  test_loss:  17.871\n",
      "Epoch: 264/500 |  train_loss:  1.392 |  test_loss:  16.537\n",
      "Epoch: 265/500 |  train_loss:  1.344 |  test_loss:  17.072\n",
      "Epoch: 266/500 |  train_loss:  1.281 |  test_loss:  16.932\n",
      "Epoch: 267/500 |  train_loss:  1.326 |  test_loss:  17.357\n",
      "Epoch: 268/500 |  train_loss:  1.346 |  test_loss:  17.350\n",
      "Epoch: 269/500 |  train_loss:  1.305 |  test_loss:  17.482\n",
      "Epoch: 270/500 |  train_loss:  1.325 |  test_loss:  17.848\n",
      "Epoch: 271/500 |  train_loss:  1.386 |  test_loss:  17.599\n",
      "Epoch: 272/500 |  train_loss:  1.788 |  test_loss:  17.318\n",
      "Epoch: 273/500 |  train_loss:  1.408 |  test_loss:  17.418\n",
      "Epoch: 274/500 |  train_loss:  1.286 |  test_loss:  17.768\n",
      "Epoch: 275/500 |  train_loss:  1.273 |  test_loss:  18.677\n",
      "Epoch: 276/500 |  train_loss:  1.290 |  test_loss:  17.531\n",
      "Epoch: 277/500 |  train_loss:  1.214 |  test_loss:  17.096\n",
      "Epoch: 278/500 |  train_loss:  1.287 |  test_loss:  17.764\n",
      "Epoch: 279/500 |  train_loss:  1.407 |  test_loss:  18.929\n",
      "Epoch: 280/500 |  train_loss:  1.521 |  test_loss:  16.902\n",
      "Epoch: 281/500 |  train_loss:  1.323 |  test_loss:  17.779\n",
      "Epoch: 282/500 |  train_loss:  1.282 |  test_loss:  16.683\n",
      "Epoch: 283/500 |  train_loss:  1.428 |  test_loss:  17.356\n",
      "Epoch: 284/500 |  train_loss:  1.361 |  test_loss:  17.640\n",
      "Epoch: 285/500 |  train_loss:  1.207 |  test_loss:  17.053\n",
      "Epoch: 286/500 |  train_loss:  1.175 |  test_loss:  17.299\n",
      "Epoch: 287/500 |  train_loss:  1.167 |  test_loss:  17.510\n",
      "Epoch: 288/500 |  train_loss:  1.225 |  test_loss:  18.491\n",
      "Epoch: 289/500 |  train_loss:  1.314 |  test_loss:  17.436\n",
      "Epoch: 290/500 |  train_loss:  1.143 |  test_loss:  17.368\n",
      "Epoch: 291/500 |  train_loss:  1.358 |  test_loss:  17.632\n",
      "Epoch: 292/500 |  train_loss:  1.203 |  test_loss:  18.729\n",
      "Epoch: 293/500 |  train_loss:  1.142 |  test_loss:  16.881\n",
      "Epoch: 294/500 |  train_loss:  1.128 |  test_loss:  18.349\n",
      "Epoch: 295/500 |  train_loss:  1.135 |  test_loss:  17.431\n",
      "Epoch: 296/500 |  train_loss:  1.176 |  test_loss:  17.301\n",
      "Epoch: 297/500 |  train_loss:  1.087 |  test_loss:  18.641\n",
      "Epoch: 298/500 |  train_loss:  1.143 |  test_loss:  18.318\n",
      "Epoch: 299/500 |  train_loss:  1.114 |  test_loss:  18.474\n",
      "Epoch: 300/500 |  train_loss:  1.059 |  test_loss:  17.023\n",
      "Epoch: 301/500 |  train_loss:  1.153 |  test_loss:  17.951\n",
      "Epoch: 302/500 |  train_loss:  1.256 |  test_loss:  17.703\n",
      "Epoch: 303/500 |  train_loss:  1.256 |  test_loss:  19.083\n",
      "Epoch: 304/500 |  train_loss:  1.226 |  test_loss:  18.225\n",
      "Epoch: 305/500 |  train_loss:  1.183 |  test_loss:  17.607\n",
      "Epoch: 306/500 |  train_loss:  1.088 |  test_loss:  18.641\n",
      "Epoch: 307/500 |  train_loss:  1.093 |  test_loss:  17.992\n",
      "Epoch: 308/500 |  train_loss:  1.157 |  test_loss:  17.405\n",
      "Epoch: 309/500 |  train_loss:  1.331 |  test_loss:  17.760\n",
      "Epoch: 310/500 |  train_loss:  1.187 |  test_loss:  17.697\n",
      "Epoch: 311/500 |  train_loss:  1.087 |  test_loss:  18.100\n",
      "Epoch: 312/500 |  train_loss:  1.003 |  test_loss:  18.586\n",
      "Epoch: 313/500 |  train_loss:  1.064 |  test_loss:  17.946\n",
      "Epoch: 314/500 |  train_loss:  1.012 |  test_loss:  17.930\n",
      "Epoch: 315/500 |  train_loss:  0.972 |  test_loss:  18.095\n",
      "Epoch: 316/500 |  train_loss:  1.048 |  test_loss:  17.537\n",
      "Epoch: 317/500 |  train_loss:  1.100 |  test_loss:  17.781\n",
      "Epoch: 318/500 |  train_loss:  0.945 |  test_loss:  18.376\n",
      "Epoch: 319/500 |  train_loss:  0.994 |  test_loss:  17.667\n",
      "Epoch: 320/500 |  train_loss:  1.047 |  test_loss:  17.504\n",
      "Epoch: 321/500 |  train_loss:  1.048 |  test_loss:  17.805\n",
      "Epoch: 322/500 |  train_loss:  0.971 |  test_loss:  17.237\n",
      "Epoch: 323/500 |  train_loss:  0.979 |  test_loss:  17.332\n",
      "Epoch: 324/500 |  train_loss:  0.912 |  test_loss:  17.652\n",
      "Epoch: 325/500 |  train_loss:  0.955 |  test_loss:  19.597\n",
      "Epoch: 326/500 |  train_loss:  0.945 |  test_loss:  18.403\n",
      "Epoch: 327/500 |  train_loss:  0.889 |  test_loss:  18.940\n",
      "Epoch: 328/500 |  train_loss:  1.018 |  test_loss:  18.187\n",
      "Epoch: 329/500 |  train_loss:  0.941 |  test_loss:  17.684\n",
      "Epoch: 330/500 |  train_loss:  0.907 |  test_loss:  18.362\n",
      "Epoch: 331/500 |  train_loss:  0.986 |  test_loss:  18.009\n",
      "Epoch: 332/500 |  train_loss:  0.927 |  test_loss:  17.474\n",
      "Epoch: 333/500 |  train_loss:  0.918 |  test_loss:  17.152\n",
      "Epoch: 334/500 |  train_loss:  0.907 |  test_loss:  18.115\n",
      "Epoch: 335/500 |  train_loss:  0.956 |  test_loss:  17.768\n",
      "Epoch: 336/500 |  train_loss:  0.958 |  test_loss:  18.298\n",
      "Epoch: 337/500 |  train_loss:  1.044 |  test_loss:  17.163\n",
      "Epoch: 338/500 |  train_loss:  1.047 |  test_loss:  19.091\n",
      "Epoch: 339/500 |  train_loss:  0.943 |  test_loss:  18.265\n",
      "Epoch: 340/500 |  train_loss:  0.865 |  test_loss:  18.140\n",
      "Epoch: 341/500 |  train_loss:  0.971 |  test_loss:  18.777\n",
      "Epoch: 342/500 |  train_loss:  0.836 |  test_loss:  18.064\n",
      "Epoch: 343/500 |  train_loss:  0.953 |  test_loss:  17.576\n",
      "Epoch: 344/500 |  train_loss:  0.957 |  test_loss:  18.046\n",
      "Epoch: 345/500 |  train_loss:  0.882 |  test_loss:  17.994\n",
      "Epoch: 346/500 |  train_loss:  0.917 |  test_loss:  18.700\n",
      "Epoch: 347/500 |  train_loss:  0.879 |  test_loss:  16.910\n",
      "Epoch: 348/500 |  train_loss:  1.107 |  test_loss:  18.022\n",
      "Epoch: 349/500 |  train_loss:  0.944 |  test_loss:  17.606\n",
      "Epoch: 350/500 |  train_loss:  0.919 |  test_loss:  18.540\n",
      "Epoch: 351/500 |  train_loss:  0.847 |  test_loss:  18.305\n",
      "Epoch: 352/500 |  train_loss:  0.778 |  test_loss:  17.954\n",
      "Epoch: 353/500 |  train_loss:  0.835 |  test_loss:  17.845\n",
      "Epoch: 354/500 |  train_loss:  0.847 |  test_loss:  19.034\n",
      "Epoch: 355/500 |  train_loss:  0.819 |  test_loss:  17.522\n",
      "Epoch: 356/500 |  train_loss:  0.806 |  test_loss:  17.724\n",
      "Epoch: 357/500 |  train_loss:  0.773 |  test_loss:  18.262\n",
      "Epoch: 358/500 |  train_loss:  0.842 |  test_loss:  18.321\n",
      "Epoch: 359/500 |  train_loss:  0.756 |  test_loss:  18.721\n",
      "Epoch: 360/500 |  train_loss:  0.800 |  test_loss:  18.209\n",
      "Epoch: 361/500 |  train_loss:  0.818 |  test_loss:  18.214\n",
      "Epoch: 362/500 |  train_loss:  0.845 |  test_loss:  18.331\n",
      "Epoch: 363/500 |  train_loss:  0.783 |  test_loss:  18.512\n",
      "Epoch: 364/500 |  train_loss:  0.848 |  test_loss:  18.014\n",
      "Epoch: 365/500 |  train_loss:  0.831 |  test_loss:  18.237\n",
      "Epoch: 366/500 |  train_loss:  0.761 |  test_loss:  17.841\n",
      "Epoch: 367/500 |  train_loss:  0.751 |  test_loss:  18.706\n",
      "Epoch: 368/500 |  train_loss:  0.839 |  test_loss:  18.497\n",
      "Epoch: 369/500 |  train_loss:  0.931 |  test_loss:  17.638\n",
      "Epoch: 370/500 |  train_loss:  0.944 |  test_loss:  18.646\n",
      "Epoch: 371/500 |  train_loss:  0.882 |  test_loss:  19.038\n",
      "Epoch: 372/500 |  train_loss:  0.745 |  test_loss:  18.383\n",
      "Epoch: 373/500 |  train_loss:  0.777 |  test_loss:  19.075\n",
      "Epoch: 374/500 |  train_loss:  0.745 |  test_loss:  18.678\n",
      "Epoch: 375/500 |  train_loss:  0.737 |  test_loss:  18.304\n",
      "Epoch: 376/500 |  train_loss:  0.778 |  test_loss:  18.957\n",
      "Epoch: 377/500 |  train_loss:  0.693 |  test_loss:  18.121\n",
      "Epoch: 378/500 |  train_loss:  0.718 |  test_loss:  18.343\n",
      "Epoch: 379/500 |  train_loss:  0.855 |  test_loss:  18.682\n",
      "Epoch: 380/500 |  train_loss:  0.820 |  test_loss:  19.654\n",
      "Epoch: 381/500 |  train_loss:  0.827 |  test_loss:  17.902\n",
      "Epoch: 382/500 |  train_loss:  0.809 |  test_loss:  17.933\n",
      "Epoch: 383/500 |  train_loss:  0.826 |  test_loss:  19.340\n",
      "Epoch: 384/500 |  train_loss:  0.810 |  test_loss:  18.541\n",
      "Epoch: 385/500 |  train_loss:  0.844 |  test_loss:  17.847\n",
      "Epoch: 386/500 |  train_loss:  0.820 |  test_loss:  18.548\n",
      "Epoch: 387/500 |  train_loss:  0.843 |  test_loss:  18.220\n",
      "Epoch: 388/500 |  train_loss:  0.724 |  test_loss:  18.666\n",
      "Epoch: 389/500 |  train_loss:  0.915 |  test_loss:  18.498\n",
      "Epoch: 390/500 |  train_loss:  0.789 |  test_loss:  19.063\n",
      "Epoch: 391/500 |  train_loss:  0.802 |  test_loss:  18.655\n",
      "Epoch: 392/500 |  train_loss:  0.694 |  test_loss:  19.107\n",
      "Epoch: 393/500 |  train_loss:  0.740 |  test_loss:  18.525\n",
      "Epoch: 394/500 |  train_loss:  0.872 |  test_loss:  18.277\n",
      "Epoch: 395/500 |  train_loss:  0.919 |  test_loss:  18.570\n",
      "Epoch: 396/500 |  train_loss:  0.755 |  test_loss:  18.472\n",
      "Epoch: 397/500 |  train_loss:  0.762 |  test_loss:  18.587\n",
      "Epoch: 398/500 |  train_loss:  0.760 |  test_loss:  19.019\n",
      "Epoch: 399/500 |  train_loss:  0.786 |  test_loss:  18.665\n",
      "Epoch: 400/500 |  train_loss:  0.691 |  test_loss:  19.231\n",
      "Epoch: 401/500 |  train_loss:  0.661 |  test_loss:  18.223\n",
      "Epoch: 402/500 |  train_loss:  0.759 |  test_loss:  18.813\n",
      "Epoch: 403/500 |  train_loss:  0.630 |  test_loss:  19.003\n",
      "Epoch: 404/500 |  train_loss:  0.648 |  test_loss:  18.581\n",
      "Epoch: 405/500 |  train_loss:  0.614 |  test_loss:  19.025\n",
      "Epoch: 406/500 |  train_loss:  0.581 |  test_loss:  18.820\n",
      "Epoch: 407/500 |  train_loss:  0.720 |  test_loss:  18.621\n",
      "Epoch: 408/500 |  train_loss:  0.816 |  test_loss:  19.133\n",
      "Epoch: 409/500 |  train_loss:  0.670 |  test_loss:  18.691\n",
      "Epoch: 410/500 |  train_loss:  0.662 |  test_loss:  19.287\n",
      "Epoch: 411/500 |  train_loss:  0.678 |  test_loss:  18.658\n",
      "Epoch: 412/500 |  train_loss:  0.653 |  test_loss:  18.974\n",
      "Epoch: 413/500 |  train_loss:  0.610 |  test_loss:  18.615\n",
      "Epoch: 414/500 |  train_loss:  0.615 |  test_loss:  18.969\n",
      "Epoch: 415/500 |  train_loss:  0.577 |  test_loss:  19.661\n",
      "Epoch: 416/500 |  train_loss:  0.771 |  test_loss:  18.430\n",
      "Epoch: 417/500 |  train_loss:  0.769 |  test_loss:  18.821\n",
      "Epoch: 418/500 |  train_loss:  0.657 |  test_loss:  19.219\n",
      "Epoch: 419/500 |  train_loss:  0.651 |  test_loss:  19.343\n",
      "Epoch: 420/500 |  train_loss:  0.578 |  test_loss:  19.042\n",
      "Epoch: 421/500 |  train_loss:  0.591 |  test_loss:  18.698\n",
      "Epoch: 422/500 |  train_loss:  0.540 |  test_loss:  18.866\n",
      "Epoch: 423/500 |  train_loss:  0.586 |  test_loss:  19.967\n",
      "Epoch: 424/500 |  train_loss:  0.659 |  test_loss:  19.069\n",
      "Epoch: 425/500 |  train_loss:  0.790 |  test_loss:  19.768\n",
      "Epoch: 426/500 |  train_loss:  0.686 |  test_loss:  17.989\n",
      "Epoch: 427/500 |  train_loss:  0.575 |  test_loss:  19.130\n",
      "Epoch: 428/500 |  train_loss:  0.582 |  test_loss:  19.419\n",
      "Epoch: 429/500 |  train_loss:  0.629 |  test_loss:  18.970\n",
      "Epoch: 430/500 |  train_loss:  0.630 |  test_loss:  19.084\n",
      "Epoch: 431/500 |  train_loss:  0.563 |  test_loss:  18.814\n",
      "Epoch: 432/500 |  train_loss:  0.540 |  test_loss:  18.573\n",
      "Epoch: 433/500 |  train_loss:  0.588 |  test_loss:  19.035\n",
      "Epoch: 434/500 |  train_loss:  0.551 |  test_loss:  18.249\n",
      "Epoch: 435/500 |  train_loss:  0.572 |  test_loss:  19.068\n",
      "Epoch: 436/500 |  train_loss:  0.569 |  test_loss:  19.475\n",
      "Epoch: 437/500 |  train_loss:  0.668 |  test_loss:  18.907\n",
      "Epoch: 438/500 |  train_loss:  0.674 |  test_loss:  20.437\n",
      "Epoch: 439/500 |  train_loss:  0.731 |  test_loss:  18.737\n",
      "Epoch: 440/500 |  train_loss:  0.507 |  test_loss:  17.894\n",
      "Epoch: 441/500 |  train_loss:  0.572 |  test_loss:  18.793\n",
      "Epoch: 442/500 |  train_loss:  0.577 |  test_loss:  19.360\n",
      "Epoch: 443/500 |  train_loss:  0.677 |  test_loss:  19.189\n",
      "Epoch: 444/500 |  train_loss:  0.711 |  test_loss:  18.841\n",
      "Epoch: 445/500 |  train_loss:  0.544 |  test_loss:  19.794\n",
      "Epoch: 446/500 |  train_loss:  0.565 |  test_loss:  18.775\n",
      "Epoch: 447/500 |  train_loss:  0.520 |  test_loss:  19.556\n",
      "Epoch: 448/500 |  train_loss:  0.525 |  test_loss:  18.780\n",
      "Epoch: 449/500 |  train_loss:  0.653 |  test_loss:  18.906\n",
      "Epoch: 450/500 |  train_loss:  0.653 |  test_loss:  19.251\n",
      "Epoch: 451/500 |  train_loss:  0.611 |  test_loss:  19.093\n",
      "Epoch: 452/500 |  train_loss:  0.637 |  test_loss:  18.390\n",
      "Epoch: 453/500 |  train_loss:  0.715 |  test_loss:  19.184\n",
      "Epoch: 454/500 |  train_loss:  0.677 |  test_loss:  19.417\n",
      "Epoch: 455/500 |  train_loss:  0.555 |  test_loss:  18.709\n",
      "Epoch: 456/500 |  train_loss:  0.492 |  test_loss:  18.480\n",
      "Epoch: 457/500 |  train_loss:  0.461 |  test_loss:  18.845\n",
      "Epoch: 458/500 |  train_loss:  0.499 |  test_loss:  19.252\n",
      "Epoch: 459/500 |  train_loss:  0.567 |  test_loss:  19.284\n",
      "Epoch: 460/500 |  train_loss:  0.493 |  test_loss:  19.143\n",
      "Epoch: 461/500 |  train_loss:  0.519 |  test_loss:  19.347\n",
      "Epoch: 462/500 |  train_loss:  0.638 |  test_loss:  19.508\n",
      "Epoch: 463/500 |  train_loss:  0.611 |  test_loss:  18.865\n",
      "Epoch: 464/500 |  train_loss:  0.546 |  test_loss:  19.734\n",
      "Epoch: 465/500 |  train_loss:  0.580 |  test_loss:  19.046\n",
      "Epoch: 466/500 |  train_loss:  0.549 |  test_loss:  18.948\n",
      "Epoch: 467/500 |  train_loss:  0.519 |  test_loss:  19.431\n",
      "Epoch: 468/500 |  train_loss:  0.497 |  test_loss:  19.380\n",
      "Epoch: 469/500 |  train_loss:  0.720 |  test_loss:  19.906\n",
      "Epoch: 470/500 |  train_loss:  0.568 |  test_loss:  19.590\n",
      "Epoch: 471/500 |  train_loss:  0.570 |  test_loss:  18.079\n",
      "Epoch: 472/500 |  train_loss:  0.625 |  test_loss:  19.752\n",
      "Epoch: 473/500 |  train_loss:  0.566 |  test_loss:  19.545\n",
      "Epoch: 474/500 |  train_loss:  0.528 |  test_loss:  19.600\n",
      "Epoch: 475/500 |  train_loss:  0.497 |  test_loss:  19.911\n",
      "Epoch: 476/500 |  train_loss:  0.507 |  test_loss:  19.602\n",
      "Epoch: 477/500 |  train_loss:  0.502 |  test_loss:  19.183\n",
      "Epoch: 478/500 |  train_loss:  0.579 |  test_loss:  19.524\n",
      "Epoch: 479/500 |  train_loss:  0.518 |  test_loss:  18.701\n",
      "Epoch: 480/500 |  train_loss:  0.523 |  test_loss:  18.993\n",
      "Epoch: 481/500 |  train_loss:  0.415 |  test_loss:  19.024\n",
      "Epoch: 482/500 |  train_loss:  0.418 |  test_loss:  19.245\n",
      "Epoch: 483/500 |  train_loss:  0.479 |  test_loss:  19.581\n",
      "Epoch: 484/500 |  train_loss:  0.514 |  test_loss:  19.324\n",
      "Epoch: 485/500 |  train_loss:  0.580 |  test_loss:  19.903\n",
      "Epoch: 486/500 |  train_loss:  0.488 |  test_loss:  19.524\n",
      "Epoch: 487/500 |  train_loss:  0.474 |  test_loss:  19.564\n",
      "Epoch: 488/500 |  train_loss:  0.430 |  test_loss:  19.290\n",
      "Epoch: 489/500 |  train_loss:  0.468 |  test_loss:  19.910\n",
      "Epoch: 490/500 |  train_loss:  0.645 |  test_loss:  20.100\n",
      "Epoch: 491/500 |  train_loss:  0.669 |  test_loss:  20.369\n",
      "Epoch: 492/500 |  train_loss:  0.644 |  test_loss:  19.164\n",
      "Epoch: 493/500 |  train_loss:  0.693 |  test_loss:  19.182\n",
      "Epoch: 494/500 |  train_loss:  0.496 |  test_loss:  19.808\n",
      "Epoch: 495/500 |  train_loss:  0.447 |  test_loss:  19.477\n",
      "Epoch: 496/500 |  train_loss:  0.465 |  test_loss:  19.609\n",
      "Epoch: 497/500 |  train_loss:  0.454 |  test_loss:  19.474\n",
      "Epoch: 498/500 |  train_loss:  0.440 |  test_loss:  18.697\n",
      "Epoch: 499/500 |  train_loss:  0.518 |  test_loss:  19.894\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    accumulated_train_loss = 0.0\n",
    "    accumulated_train_batches = 0\n",
    "\n",
    "    # training\n",
    "    for input, target in train_dataloader:\n",
    "\n",
    "        # copy data to device\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # train mode\n",
    "        boston_housing_model.train()\n",
    "\n",
    "        # forward pass\n",
    "        output = boston_housing_model(input)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # accumulate losses \n",
    "        accumulated_train_loss += loss.item()\n",
    "\n",
    "        # accumulate batches\n",
    "        accumulated_train_batches += 1\n",
    "\n",
    "        # set gradients of all optimized parameters to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # update gradients\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    train_loss = accumulated_train_loss / accumulated_train_batches\n",
    "\n",
    "    # eval mode\n",
    "    boston_housing_model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # testing\n",
    "        accumulated_test_loss = 0.0\n",
    "        accumulated_test_batches = 0\n",
    "        \n",
    "        for input, target in test_dataloader:\n",
    "\n",
    "            # copy data to device\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            output = boston_housing_model(input)\n",
    "\n",
    "            # calculate loss\n",
    "            test_loss = loss_fn(output, target)\n",
    "\n",
    "            # accumulate losses \n",
    "            accumulated_test_loss += test_loss.item()\n",
    "\n",
    "            # accumulate batches\n",
    "            accumulated_test_batches += 1\n",
    "\n",
    "        \n",
    "        test_loss = accumulated_test_loss/ accumulated_test_batches\n",
    "\n",
    "        print(f\"Epoch: {epoch}/500 | \"\n",
    "            f\" train_loss: {train_loss: 0.3f} | \"\n",
    "            f\" test_loss: { test_loss: 0.3f}\")\n",
    "\n",
    "    \n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([102, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "# eval mode\n",
    "boston_housing_model.eval()\n",
    "with torch.inference_mode():\n",
    "    predictions = boston_housing_model(torch.from_numpy(X_test).to(device))\n",
    "\n",
    "predictions = predictions.cpu()\n",
    "predictions.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predicted value:  28.44 | True value:  23.60\n",
      " Predicted value:  34.19 | True value:  32.40\n",
      " Predicted value:  12.10 | True value:  13.60\n",
      " Predicted value:  21.37 | True value:  22.80\n"
     ]
    }
   ],
   "source": [
    "# print out \n",
    "for i in range(0, 4):\n",
    "    print(f\" Predicted value: {predictions[i].squeeze(): 0.2f} | True value: {y_test[i].squeeze(): 0.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tadac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
