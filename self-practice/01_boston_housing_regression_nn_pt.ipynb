{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem** \n",
    "\n",
    "Use a neural network to solve a regression problem, using the Boston housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.12.1+cu102\n",
      "Device: cuda\n",
      "Created date: 2023-06-23 00:30:42.960612\n",
      "Modified date: 2023-06-25 01:53:13.795503\n"
     ]
    }
   ],
   "source": [
    "# import libraties\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ignore FutureWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# check torch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# check GPU or CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# created date\n",
    "print(f\"Created date: 2023-06-23 00:30:42.960612\")\n",
    "\n",
    "# modified date\n",
    "print(f\"Modified date: {datetime.now()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://pytorch.org/docs/stable/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and standardize the data.\n",
    "boston_housing = load_boston()\n",
    "data = boston_housing.get('data')\n",
    "target = boston_housing.get('target')\n",
    "\n",
    "raw_x_train, raw_x_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to same precision as model.\n",
    "raw_x_train = raw_x_train.astype(np.float32)\n",
    "raw_x_test = raw_x_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.reshape(y_train, (-1, 1))\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.reshape(y_test, (-1, 1))\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.mean(raw_x_train, axis=0)\n",
    "x_stddev = np.std(raw_x_train, axis=0)\n",
    "x_train = (raw_x_train - x_mean) / x_stddev\n",
    "x_test = (raw_x_test - x_mean) / x_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data, testing data into data loader\n",
    "batch_size = 16\n",
    "\n",
    "# create Dataset objects\n",
    "train_dataset = TensorDataset(torch.from_numpy(x_train),\n",
    "                         torch.from_numpy(y_train))\n",
    "test_dataset = TensorDataset(torch.from_numpy(x_test),\n",
    "                        torch.from_numpy(y_test))\n",
    "\n",
    "# create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle= True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle= False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonRegressionNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features=13, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BostonRegressionNeuralNetwork(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# model\n",
    "boston_housing_model = BostonRegressionNeuralNetwork()\n",
    "\n",
    "# init weights\n",
    "for module in boston_housing_model.modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "# copy to device\n",
    "boston_housing_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 loss: 571.2335 - mae: 21.3626 - val_loss: 473.9796 - val_mae: 17.8465\n",
      "Epoch 2/500 loss: 466.0174 - mae: 18.8653 - val_loss: 347.9645 - val_mae: 14.7854\n",
      "Epoch 3/500 loss: 294.7034 - mae: 14.2650 - val_loss: 165.1048 - val_mae: 9.4841\n",
      "Epoch 4/500 loss: 117.7620 - mae: 8.4803 - val_loss: 65.3399 - val_mae: 4.9051\n",
      "Epoch 5/500 loss: 57.1925 - mae: 5.7693 - val_loss: 48.4534 - val_mae: 3.8058\n",
      "Epoch 6/500 loss: 35.7888 - mae: 4.4097 - val_loss: 40.3774 - val_mae: 3.2068\n",
      "Epoch 7/500 loss: 28.7282 - mae: 3.7386 - val_loss: 36.8683 - val_mae: 2.9374\n",
      "Epoch 8/500 loss: 23.2344 - mae: 3.4219 - val_loss: 33.9543 - val_mae: 2.7850\n",
      "Epoch 9/500 loss: 21.9759 - mae: 3.2746 - val_loss: 32.3792 - val_mae: 2.6551\n",
      "Epoch 10/500 loss: 20.0651 - mae: 3.1229 - val_loss: 31.0648 - val_mae: 2.6031\n",
      "Epoch 11/500 loss: 18.9504 - mae: 3.0288 - val_loss: 29.7827 - val_mae: 2.5005\n",
      "Epoch 12/500 loss: 17.4127 - mae: 2.9402 - val_loss: 28.4827 - val_mae: 2.4407\n",
      "Epoch 13/500 loss: 17.0702 - mae: 2.8914 - val_loss: 27.5010 - val_mae: 2.3582\n",
      "Epoch 14/500 loss: 16.0626 - mae: 2.8024 - val_loss: 27.1783 - val_mae: 2.3349\n",
      "Epoch 15/500 loss: 15.8113 - mae: 2.7450 - val_loss: 25.8998 - val_mae: 2.3199\n",
      "Epoch 16/500 loss: 15.7209 - mae: 2.6940 - val_loss: 25.5657 - val_mae: 2.2297\n",
      "Epoch 17/500 loss: 15.4600 - mae: 2.6819 - val_loss: 25.4574 - val_mae: 2.2176\n",
      "Epoch 18/500 loss: 14.2310 - mae: 2.6095 - val_loss: 25.1564 - val_mae: 2.1918\n",
      "Epoch 19/500 loss: 13.7116 - mae: 2.5612 - val_loss: 24.1031 - val_mae: 2.1933\n",
      "Epoch 20/500 loss: 13.3925 - mae: 2.5304 - val_loss: 23.7050 - val_mae: 2.2072\n",
      "Epoch 21/500 loss: 12.7629 - mae: 2.4574 - val_loss: 23.1779 - val_mae: 2.1805\n",
      "Epoch 22/500 loss: 12.7662 - mae: 2.4662 - val_loss: 22.9103 - val_mae: 2.1656\n",
      "Epoch 23/500 loss: 12.0555 - mae: 2.4007 - val_loss: 22.2164 - val_mae: 2.1634\n",
      "Epoch 24/500 loss: 11.8419 - mae: 2.3831 - val_loss: 22.0240 - val_mae: 2.1456\n",
      "Epoch 25/500 loss: 11.6505 - mae: 2.3592 - val_loss: 21.9108 - val_mae: 2.1646\n",
      "Epoch 26/500 loss: 11.6841 - mae: 2.3396 - val_loss: 21.1655 - val_mae: 2.1476\n",
      "Epoch 27/500 loss: 11.2354 - mae: 2.3227 - val_loss: 21.6716 - val_mae: 2.1419\n",
      "Epoch 28/500 loss: 11.0743 - mae: 2.2936 - val_loss: 20.7761 - val_mae: 2.1307\n",
      "Epoch 29/500 loss: 10.7693 - mae: 2.2651 - val_loss: 21.4212 - val_mae: 2.0891\n",
      "Epoch 30/500 loss: 11.0707 - mae: 2.2520 - val_loss: 20.3797 - val_mae: 2.1409\n",
      "Epoch 31/500 loss: 10.7438 - mae: 2.2372 - val_loss: 20.6675 - val_mae: 2.1270\n",
      "Epoch 32/500 loss: 10.4334 - mae: 2.2476 - val_loss: 20.7583 - val_mae: 2.1315\n",
      "Epoch 33/500 loss: 10.1803 - mae: 2.2169 - val_loss: 19.9791 - val_mae: 2.0888\n",
      "Epoch 34/500 loss: 9.9603 - mae: 2.2160 - val_loss: 20.2120 - val_mae: 2.1562\n",
      "Epoch 35/500 loss: 11.4016 - mae: 2.1923 - val_loss: 20.1643 - val_mae: 2.0932\n",
      "Epoch 36/500 loss: 9.9788 - mae: 2.1974 - val_loss: 19.2425 - val_mae: 2.0722\n",
      "Epoch 37/500 loss: 9.7017 - mae: 2.1592 - val_loss: 19.6337 - val_mae: 2.0232\n",
      "Epoch 38/500 loss: 9.3944 - mae: 2.1491 - val_loss: 19.4239 - val_mae: 2.0931\n",
      "Epoch 39/500 loss: 10.0620 - mae: 2.1493 - val_loss: 19.4098 - val_mae: 2.0879\n",
      "Epoch 40/500 loss: 9.5118 - mae: 2.1367 - val_loss: 19.1269 - val_mae: 2.0329\n",
      "Epoch 41/500 loss: 10.2512 - mae: 2.0897 - val_loss: 19.0308 - val_mae: 2.0829\n",
      "Epoch 42/500 loss: 8.9460 - mae: 2.0917 - val_loss: 18.7593 - val_mae: 2.0214\n",
      "Epoch 43/500 loss: 8.8891 - mae: 2.0748 - val_loss: 18.9125 - val_mae: 2.0489\n",
      "Epoch 44/500 loss: 8.7484 - mae: 2.0708 - val_loss: 18.8500 - val_mae: 2.0367\n",
      "Epoch 45/500 loss: 8.6305 - mae: 2.0733 - val_loss: 18.5785 - val_mae: 2.0552\n",
      "Epoch 46/500 loss: 8.7002 - mae: 2.0513 - val_loss: 18.9339 - val_mae: 2.0205\n",
      "Epoch 47/500 loss: 8.3459 - mae: 2.0298 - val_loss: 18.8631 - val_mae: 2.0836\n",
      "Epoch 48/500 loss: 8.5730 - mae: 2.0645 - val_loss: 18.3817 - val_mae: 2.0263\n",
      "Epoch 49/500 loss: 8.3054 - mae: 1.9942 - val_loss: 18.6444 - val_mae: 2.0025\n",
      "Epoch 50/500 loss: 8.1513 - mae: 2.0144 - val_loss: 18.4486 - val_mae: 2.0536\n",
      "Epoch 51/500 loss: 8.3153 - mae: 2.0075 - val_loss: 18.1047 - val_mae: 2.0002\n",
      "Epoch 52/500 loss: 8.3107 - mae: 2.0191 - val_loss: 18.1162 - val_mae: 2.0321\n",
      "Epoch 53/500 loss: 8.2548 - mae: 1.9846 - val_loss: 17.8879 - val_mae: 2.0181\n",
      "Epoch 54/500 loss: 7.7701 - mae: 1.9617 - val_loss: 18.3047 - val_mae: 2.0161\n",
      "Epoch 55/500 loss: 7.7875 - mae: 1.9585 - val_loss: 18.2365 - val_mae: 1.9997\n",
      "Epoch 56/500 loss: 7.5465 - mae: 1.9228 - val_loss: 17.5747 - val_mae: 2.0393\n",
      "Epoch 57/500 loss: 8.2365 - mae: 1.9988 - val_loss: 17.3617 - val_mae: 2.0866\n",
      "Epoch 58/500 loss: 7.5221 - mae: 1.9155 - val_loss: 18.2747 - val_mae: 1.9756\n",
      "Epoch 59/500 loss: 7.8221 - mae: 1.9266 - val_loss: 17.5633 - val_mae: 2.0271\n",
      "Epoch 60/500 loss: 7.4111 - mae: 1.9402 - val_loss: 17.9826 - val_mae: 2.0017\n",
      "Epoch 61/500 loss: 7.2906 - mae: 1.8921 - val_loss: 17.8949 - val_mae: 1.9909\n",
      "Epoch 62/500 loss: 7.3187 - mae: 1.9229 - val_loss: 18.2946 - val_mae: 1.9846\n",
      "Epoch 63/500 loss: 7.1186 - mae: 1.8878 - val_loss: 17.2674 - val_mae: 1.9965\n",
      "Epoch 64/500 loss: 7.2224 - mae: 1.8822 - val_loss: 17.2666 - val_mae: 2.0264\n",
      "Epoch 65/500 loss: 6.8883 - mae: 1.8677 - val_loss: 17.6521 - val_mae: 1.9534\n",
      "Epoch 66/500 loss: 6.8813 - mae: 1.8575 - val_loss: 17.6624 - val_mae: 2.0218\n",
      "Epoch 67/500 loss: 6.7056 - mae: 1.8408 - val_loss: 18.0174 - val_mae: 1.9679\n",
      "Epoch 68/500 loss: 6.7951 - mae: 1.8368 - val_loss: 17.5242 - val_mae: 1.9996\n",
      "Epoch 69/500 loss: 6.6066 - mae: 1.8474 - val_loss: 17.6938 - val_mae: 1.9911\n",
      "Epoch 70/500 loss: 6.3374 - mae: 1.7938 - val_loss: 17.4463 - val_mae: 1.9940\n",
      "Epoch 71/500 loss: 6.4979 - mae: 1.8083 - val_loss: 16.8837 - val_mae: 1.9820\n",
      "Epoch 72/500 loss: 6.6472 - mae: 1.7917 - val_loss: 17.0439 - val_mae: 1.9399\n",
      "Epoch 73/500 loss: 6.6170 - mae: 1.8115 - val_loss: 17.1180 - val_mae: 1.9468\n",
      "Epoch 74/500 loss: 6.1837 - mae: 1.7961 - val_loss: 17.6719 - val_mae: 1.9637\n",
      "Epoch 75/500 loss: 6.1570 - mae: 1.7754 - val_loss: 16.5209 - val_mae: 1.9838\n",
      "Epoch 76/500 loss: 6.0920 - mae: 1.8058 - val_loss: 17.2840 - val_mae: 2.0075\n",
      "Epoch 77/500 loss: 6.0348 - mae: 1.7556 - val_loss: 17.2121 - val_mae: 1.9415\n",
      "Epoch 78/500 loss: 5.9917 - mae: 1.7434 - val_loss: 16.9689 - val_mae: 1.9528\n",
      "Epoch 79/500 loss: 6.1814 - mae: 1.7417 - val_loss: 17.0040 - val_mae: 2.0001\n",
      "Epoch 80/500 loss: 6.1929 - mae: 1.7672 - val_loss: 17.2811 - val_mae: 1.9507\n",
      "Epoch 81/500 loss: 6.1423 - mae: 1.7751 - val_loss: 16.5891 - val_mae: 1.9481\n",
      "Epoch 82/500 loss: 5.6418 - mae: 1.7125 - val_loss: 16.6625 - val_mae: 1.9469\n",
      "Epoch 83/500 loss: 5.8826 - mae: 1.7100 - val_loss: 16.5316 - val_mae: 1.9665\n",
      "Epoch 84/500 loss: 6.2757 - mae: 1.7453 - val_loss: 17.0476 - val_mae: 1.9797\n",
      "Epoch 85/500 loss: 5.6943 - mae: 1.7116 - val_loss: 16.3678 - val_mae: 1.9680\n",
      "Epoch 86/500 loss: 5.7101 - mae: 1.6978 - val_loss: 17.3326 - val_mae: 1.9592\n",
      "Epoch 87/500 loss: 5.5690 - mae: 1.7039 - val_loss: 16.7966 - val_mae: 1.9128\n",
      "Epoch 88/500 loss: 5.4251 - mae: 1.6600 - val_loss: 16.5956 - val_mae: 1.9560\n",
      "Epoch 89/500 loss: 5.2961 - mae: 1.6511 - val_loss: 16.7609 - val_mae: 1.9279\n",
      "Epoch 90/500 loss: 5.2769 - mae: 1.6460 - val_loss: 16.6264 - val_mae: 1.9269\n",
      "Epoch 91/500 loss: 5.1706 - mae: 1.6319 - val_loss: 16.5675 - val_mae: 1.9800\n",
      "Epoch 92/500 loss: 5.2334 - mae: 1.6425 - val_loss: 16.3447 - val_mae: 1.9741\n",
      "Epoch 93/500 loss: 5.7788 - mae: 1.6287 - val_loss: 16.3937 - val_mae: 1.9384\n",
      "Epoch 94/500 loss: 5.0703 - mae: 1.6318 - val_loss: 16.1204 - val_mae: 1.9605\n",
      "Epoch 95/500 loss: 5.2216 - mae: 1.6066 - val_loss: 16.7075 - val_mae: 1.9248\n",
      "Epoch 96/500 loss: 4.8974 - mae: 1.5755 - val_loss: 16.6348 - val_mae: 1.9346\n",
      "Epoch 97/500 loss: 5.4184 - mae: 1.5913 - val_loss: 16.1032 - val_mae: 1.9597\n",
      "Epoch 98/500 loss: 5.1575 - mae: 1.6198 - val_loss: 16.8025 - val_mae: 1.9037\n",
      "Epoch 99/500 loss: 4.6912 - mae: 1.5538 - val_loss: 15.9870 - val_mae: 1.9369\n",
      "Epoch 100/500 loss: 4.9881 - mae: 1.5516 - val_loss: 16.5001 - val_mae: 1.9333\n",
      "Epoch 101/500 loss: 4.7131 - mae: 1.5566 - val_loss: 16.1372 - val_mae: 1.9494\n",
      "Epoch 102/500 loss: 4.6226 - mae: 1.5391 - val_loss: 16.5574 - val_mae: 1.9207\n",
      "Epoch 103/500 loss: 4.7616 - mae: 1.5448 - val_loss: 16.4700 - val_mae: 1.9035\n",
      "Epoch 104/500 loss: 4.4714 - mae: 1.5175 - val_loss: 16.5952 - val_mae: 1.9080\n",
      "Epoch 105/500 loss: 4.4184 - mae: 1.5029 - val_loss: 16.0643 - val_mae: 1.9191\n",
      "Epoch 106/500 loss: 4.2890 - mae: 1.4941 - val_loss: 16.3597 - val_mae: 1.8978\n",
      "Epoch 107/500 loss: 4.3028 - mae: 1.4925 - val_loss: 17.3106 - val_mae: 1.9291\n",
      "Epoch 108/500 loss: 4.5072 - mae: 1.5509 - val_loss: 16.0802 - val_mae: 1.9595\n",
      "Epoch 109/500 loss: 4.4140 - mae: 1.4962 - val_loss: 15.9703 - val_mae: 1.9363\n",
      "Epoch 110/500 loss: 4.2124 - mae: 1.4617 - val_loss: 16.4087 - val_mae: 1.9113\n",
      "Epoch 111/500 loss: 4.4173 - mae: 1.5063 - val_loss: 16.2982 - val_mae: 1.9730\n",
      "Epoch 112/500 loss: 4.4151 - mae: 1.5420 - val_loss: 16.0879 - val_mae: 1.9474\n",
      "Epoch 113/500 loss: 4.1411 - mae: 1.4381 - val_loss: 16.0861 - val_mae: 1.9874\n",
      "Epoch 114/500 loss: 4.1578 - mae: 1.4437 - val_loss: 16.4806 - val_mae: 1.8969\n",
      "Epoch 115/500 loss: 4.1868 - mae: 1.4328 - val_loss: 16.2016 - val_mae: 1.9297\n",
      "Epoch 116/500 loss: 4.1901 - mae: 1.4732 - val_loss: 15.7588 - val_mae: 1.9833\n",
      "Epoch 117/500 loss: 4.0070 - mae: 1.4337 - val_loss: 16.7730 - val_mae: 2.0488\n",
      "Epoch 118/500 loss: 3.9523 - mae: 1.4251 - val_loss: 16.2823 - val_mae: 1.8899\n",
      "Epoch 119/500 loss: 3.7909 - mae: 1.3757 - val_loss: 16.1212 - val_mae: 1.9243\n",
      "Epoch 120/500 loss: 4.0428 - mae: 1.4196 - val_loss: 15.8234 - val_mae: 1.9089\n",
      "Epoch 121/500 loss: 3.7105 - mae: 1.3906 - val_loss: 16.4407 - val_mae: 1.8685\n",
      "Epoch 122/500 loss: 3.7101 - mae: 1.3797 - val_loss: 15.7069 - val_mae: 1.9162\n",
      "Epoch 123/500 loss: 3.7058 - mae: 1.3761 - val_loss: 15.5749 - val_mae: 1.9352\n",
      "Epoch 124/500 loss: 4.1390 - mae: 1.3912 - val_loss: 16.2921 - val_mae: 1.8981\n",
      "Epoch 125/500 loss: 3.9265 - mae: 1.4355 - val_loss: 15.7645 - val_mae: 1.8773\n",
      "Epoch 126/500 loss: 3.6550 - mae: 1.3539 - val_loss: 16.5598 - val_mae: 1.9500\n",
      "Epoch 127/500 loss: 3.9066 - mae: 1.4233 - val_loss: 16.0095 - val_mae: 1.9394\n",
      "Epoch 128/500 loss: 3.6713 - mae: 1.3786 - val_loss: 16.0625 - val_mae: 1.8765\n",
      "Epoch 129/500 loss: 3.4716 - mae: 1.3260 - val_loss: 16.3987 - val_mae: 1.8660\n",
      "Epoch 130/500 loss: 3.6734 - mae: 1.3088 - val_loss: 16.0186 - val_mae: 1.9539\n",
      "Epoch 131/500 loss: 3.4768 - mae: 1.3402 - val_loss: 16.2988 - val_mae: 1.9346\n",
      "Epoch 132/500 loss: 3.5131 - mae: 1.3283 - val_loss: 15.4918 - val_mae: 1.9238\n",
      "Epoch 133/500 loss: 3.4396 - mae: 1.2958 - val_loss: 16.1677 - val_mae: 1.8967\n",
      "Epoch 134/500 loss: 3.5464 - mae: 1.3592 - val_loss: 16.4543 - val_mae: 1.9010\n",
      "Epoch 135/500 loss: 3.4240 - mae: 1.3225 - val_loss: 16.1908 - val_mae: 1.9112\n",
      "Epoch 136/500 loss: 3.4777 - mae: 1.3378 - val_loss: 16.7011 - val_mae: 1.9914\n",
      "Epoch 137/500 loss: 3.3972 - mae: 1.3313 - val_loss: 17.3230 - val_mae: 1.9714\n",
      "Epoch 138/500 loss: 3.3454 - mae: 1.2642 - val_loss: 16.2504 - val_mae: 2.0118\n",
      "Epoch 139/500 loss: 3.7233 - mae: 1.2972 - val_loss: 16.1071 - val_mae: 1.8666\n",
      "Epoch 140/500 loss: 3.2827 - mae: 1.3113 - val_loss: 16.0622 - val_mae: 1.8476\n",
      "Epoch 141/500 loss: 3.1105 - mae: 1.2486 - val_loss: 15.4826 - val_mae: 1.9286\n",
      "Epoch 142/500 loss: 3.1852 - mae: 1.2524 - val_loss: 15.7177 - val_mae: 1.8954\n",
      "Epoch 143/500 loss: 3.2511 - mae: 1.2628 - val_loss: 16.0857 - val_mae: 1.8569\n",
      "Epoch 144/500 loss: 3.0629 - mae: 1.2243 - val_loss: 15.9054 - val_mae: 1.8567\n",
      "Epoch 145/500 loss: 3.0611 - mae: 1.2474 - val_loss: 16.0967 - val_mae: 1.8741\n",
      "Epoch 146/500 loss: 3.0594 - mae: 1.2301 - val_loss: 16.1877 - val_mae: 1.8949\n",
      "Epoch 147/500 loss: 3.0008 - mae: 1.2087 - val_loss: 15.8474 - val_mae: 1.9326\n",
      "Epoch 148/500 loss: 3.1481 - mae: 1.2666 - val_loss: 15.7023 - val_mae: 1.8826\n",
      "Epoch 149/500 loss: 2.9100 - mae: 1.2267 - val_loss: 16.3226 - val_mae: 1.8988\n",
      "Epoch 150/500 loss: 2.9180 - mae: 1.2043 - val_loss: 16.0964 - val_mae: 1.9162\n",
      "Epoch 151/500 loss: 3.0745 - mae: 1.2125 - val_loss: 15.7328 - val_mae: 1.8991\n",
      "Epoch 152/500 loss: 3.2789 - mae: 1.2394 - val_loss: 16.4770 - val_mae: 1.9417\n",
      "Epoch 153/500 loss: 3.1431 - mae: 1.2220 - val_loss: 16.4658 - val_mae: 1.9592\n",
      "Epoch 154/500 loss: 3.3202 - mae: 1.2315 - val_loss: 18.0613 - val_mae: 2.1399\n",
      "Epoch 155/500 loss: 3.2354 - mae: 1.2800 - val_loss: 16.9766 - val_mae: 1.8473\n",
      "Epoch 156/500 loss: 3.3705 - mae: 1.2470 - val_loss: 16.2738 - val_mae: 1.8550\n",
      "Epoch 157/500 loss: 2.7612 - mae: 1.1860 - val_loss: 15.5763 - val_mae: 1.9177\n",
      "Epoch 158/500 loss: 3.1780 - mae: 1.2064 - val_loss: 15.8733 - val_mae: 1.9260\n",
      "Epoch 159/500 loss: 2.7530 - mae: 1.1971 - val_loss: 16.6486 - val_mae: 1.8636\n",
      "Epoch 160/500 loss: 2.8179 - mae: 1.1876 - val_loss: 15.9691 - val_mae: 1.9272\n",
      "Epoch 161/500 loss: 2.6636 - mae: 1.1448 - val_loss: 16.1913 - val_mae: 1.8801\n",
      "Epoch 162/500 loss: 2.5362 - mae: 1.1427 - val_loss: 15.9642 - val_mae: 1.8957\n",
      "Epoch 163/500 loss: 2.7419 - mae: 1.1643 - val_loss: 16.5318 - val_mae: 1.8810\n",
      "Epoch 164/500 loss: 2.6731 - mae: 1.1596 - val_loss: 16.0268 - val_mae: 1.8841\n",
      "Epoch 165/500 loss: 2.4681 - mae: 1.0931 - val_loss: 16.0248 - val_mae: 1.9765\n",
      "Epoch 166/500 loss: 2.6735 - mae: 1.1552 - val_loss: 16.4946 - val_mae: 1.8917\n",
      "Epoch 167/500 loss: 2.5865 - mae: 1.1139 - val_loss: 16.0402 - val_mae: 1.9023\n",
      "Epoch 168/500 loss: 2.5855 - mae: 1.1468 - val_loss: 16.0956 - val_mae: 1.9033\n",
      "Epoch 169/500 loss: 2.4363 - mae: 1.1166 - val_loss: 16.5682 - val_mae: 1.8939\n",
      "Epoch 170/500 loss: 2.7883 - mae: 1.1359 - val_loss: 17.4317 - val_mae: 1.9432\n",
      "Epoch 171/500 loss: 2.9114 - mae: 1.2098 - val_loss: 17.1624 - val_mae: 1.9061\n",
      "Epoch 172/500 loss: 2.6916 - mae: 1.1490 - val_loss: 16.8703 - val_mae: 2.0236\n",
      "Epoch 173/500 loss: 2.6268 - mae: 1.0899 - val_loss: 16.6515 - val_mae: 1.9648\n",
      "Epoch 174/500 loss: 2.3872 - mae: 1.1037 - val_loss: 16.7769 - val_mae: 1.8883\n",
      "Epoch 175/500 loss: 2.4772 - mae: 1.1018 - val_loss: 16.8661 - val_mae: 1.9476\n",
      "Epoch 176/500 loss: 2.4397 - mae: 1.0708 - val_loss: 17.0205 - val_mae: 1.8782\n",
      "Epoch 177/500 loss: 2.4070 - mae: 1.0964 - val_loss: 16.7785 - val_mae: 1.9183\n",
      "Epoch 178/500 loss: 2.4534 - mae: 1.1013 - val_loss: 17.5856 - val_mae: 2.0988\n",
      "Epoch 179/500 loss: 2.5552 - mae: 1.1114 - val_loss: 17.0107 - val_mae: 1.9923\n",
      "Epoch 180/500 loss: 2.4599 - mae: 1.1294 - val_loss: 16.7264 - val_mae: 1.9511\n",
      "Epoch 181/500 loss: 2.3494 - mae: 1.0803 - val_loss: 16.3817 - val_mae: 1.9460\n",
      "Epoch 182/500 loss: 2.3503 - mae: 1.0653 - val_loss: 17.0022 - val_mae: 1.8906\n",
      "Epoch 183/500 loss: 2.3937 - mae: 1.0819 - val_loss: 17.2164 - val_mae: 1.9060\n",
      "Epoch 184/500 loss: 2.2842 - mae: 1.0357 - val_loss: 17.3872 - val_mae: 1.9666\n",
      "Epoch 185/500 loss: 2.5358 - mae: 1.1109 - val_loss: 17.1164 - val_mae: 1.8892\n",
      "Epoch 186/500 loss: 2.2503 - mae: 1.0588 - val_loss: 17.0031 - val_mae: 1.9191\n",
      "Epoch 187/500 loss: 2.4203 - mae: 1.0332 - val_loss: 17.3875 - val_mae: 1.9303\n",
      "Epoch 188/500 loss: 2.3230 - mae: 1.0802 - val_loss: 17.1784 - val_mae: 1.9450\n",
      "Epoch 189/500 loss: 2.4934 - mae: 1.1396 - val_loss: 17.0357 - val_mae: 1.9106\n",
      "Epoch 190/500 loss: 2.4925 - mae: 1.0254 - val_loss: 17.1531 - val_mae: 1.9408\n",
      "Epoch 191/500 loss: 2.3042 - mae: 1.0359 - val_loss: 16.7407 - val_mae: 1.9960\n",
      "Epoch 192/500 loss: 2.3472 - mae: 1.0962 - val_loss: 17.6672 - val_mae: 2.2047\n",
      "Epoch 193/500 loss: 2.2789 - mae: 1.0717 - val_loss: 17.4044 - val_mae: 2.0043\n",
      "Epoch 194/500 loss: 2.1536 - mae: 1.0333 - val_loss: 16.7329 - val_mae: 1.9144\n",
      "Epoch 195/500 loss: 1.9711 - mae: 0.9964 - val_loss: 17.5084 - val_mae: 1.9123\n",
      "Epoch 196/500 loss: 2.0852 - mae: 1.0404 - val_loss: 18.3068 - val_mae: 1.9057\n",
      "Epoch 197/500 loss: 2.1227 - mae: 0.9829 - val_loss: 16.4523 - val_mae: 1.9591\n",
      "Epoch 198/500 loss: 2.1026 - mae: 1.0129 - val_loss: 17.2513 - val_mae: 1.9670\n",
      "Epoch 199/500 loss: 2.0117 - mae: 1.0167 - val_loss: 16.8347 - val_mae: 1.9521\n",
      "Epoch 200/500 loss: 1.9901 - mae: 0.9960 - val_loss: 17.4536 - val_mae: 2.0012\n",
      "Epoch 201/500 loss: 1.9789 - mae: 0.9886 - val_loss: 17.6839 - val_mae: 2.0427\n",
      "Epoch 202/500 loss: 2.2524 - mae: 1.0489 - val_loss: 18.1076 - val_mae: 1.9465\n",
      "Epoch 203/500 loss: 2.0247 - mae: 0.9991 - val_loss: 17.9680 - val_mae: 2.0533\n",
      "Epoch 204/500 loss: 1.9243 - mae: 0.9839 - val_loss: 16.9204 - val_mae: 1.9218\n",
      "Epoch 205/500 loss: 2.0671 - mae: 0.9472 - val_loss: 17.2390 - val_mae: 1.9545\n",
      "Epoch 206/500 loss: 2.0902 - mae: 1.0097 - val_loss: 16.7823 - val_mae: 1.9753\n",
      "Epoch 207/500 loss: 2.1616 - mae: 1.0501 - val_loss: 16.8814 - val_mae: 2.0071\n",
      "Epoch 208/500 loss: 2.0367 - mae: 0.9656 - val_loss: 17.7086 - val_mae: 2.0044\n",
      "Epoch 209/500 loss: 2.1659 - mae: 1.0252 - val_loss: 17.7711 - val_mae: 1.9549\n",
      "Epoch 210/500 loss: 1.9288 - mae: 0.9764 - val_loss: 16.8615 - val_mae: 1.9982\n",
      "Epoch 211/500 loss: 1.9285 - mae: 0.9701 - val_loss: 17.4056 - val_mae: 1.9360\n",
      "Epoch 212/500 loss: 2.0482 - mae: 1.0199 - val_loss: 16.4885 - val_mae: 1.9353\n",
      "Epoch 213/500 loss: 1.9525 - mae: 0.9831 - val_loss: 16.8834 - val_mae: 1.9841\n",
      "Epoch 214/500 loss: 1.8481 - mae: 0.9594 - val_loss: 17.5773 - val_mae: 2.0011\n",
      "Epoch 215/500 loss: 1.8633 - mae: 0.9549 - val_loss: 17.6899 - val_mae: 1.9984\n",
      "Epoch 216/500 loss: 1.7920 - mae: 0.9444 - val_loss: 17.0767 - val_mae: 2.0845\n",
      "Epoch 217/500 loss: 1.7771 - mae: 0.9482 - val_loss: 17.4994 - val_mae: 1.9604\n",
      "Epoch 218/500 loss: 1.8222 - mae: 0.9461 - val_loss: 16.9303 - val_mae: 1.9818\n",
      "Epoch 219/500 loss: 1.6906 - mae: 0.9078 - val_loss: 17.0412 - val_mae: 2.0026\n",
      "Epoch 220/500 loss: 1.7479 - mae: 0.9278 - val_loss: 17.3265 - val_mae: 1.9910\n",
      "Epoch 221/500 loss: 1.6602 - mae: 0.8888 - val_loss: 17.1693 - val_mae: 1.9602\n",
      "Epoch 222/500 loss: 1.6942 - mae: 0.8985 - val_loss: 17.4415 - val_mae: 2.0083\n",
      "Epoch 223/500 loss: 1.6886 - mae: 0.9159 - val_loss: 17.4730 - val_mae: 1.9590\n",
      "Epoch 224/500 loss: 1.6231 - mae: 0.8863 - val_loss: 16.9355 - val_mae: 2.0431\n",
      "Epoch 225/500 loss: 1.7495 - mae: 0.9236 - val_loss: 17.3336 - val_mae: 1.9826\n",
      "Epoch 226/500 loss: 1.6584 - mae: 0.8766 - val_loss: 16.9603 - val_mae: 2.0387\n",
      "Epoch 227/500 loss: 1.8411 - mae: 0.9431 - val_loss: 17.0663 - val_mae: 1.9174\n",
      "Epoch 228/500 loss: 1.7756 - mae: 0.9034 - val_loss: 17.1337 - val_mae: 2.0904\n",
      "Epoch 229/500 loss: 1.8242 - mae: 0.9622 - val_loss: 17.1683 - val_mae: 2.0656\n",
      "Epoch 230/500 loss: 1.7295 - mae: 0.9448 - val_loss: 17.6245 - val_mae: 2.0052\n",
      "Epoch 231/500 loss: 1.6511 - mae: 0.9034 - val_loss: 17.7415 - val_mae: 2.1280\n",
      "Epoch 232/500 loss: 1.6198 - mae: 0.9089 - val_loss: 18.3376 - val_mae: 1.9822\n",
      "Epoch 233/500 loss: 1.6311 - mae: 0.8980 - val_loss: 17.6138 - val_mae: 2.0508\n",
      "Epoch 234/500 loss: 1.6101 - mae: 0.8859 - val_loss: 18.0222 - val_mae: 2.2011\n",
      "Epoch 235/500 loss: 1.8382 - mae: 0.9602 - val_loss: 16.7604 - val_mae: 1.9801\n",
      "Epoch 236/500 loss: 1.6075 - mae: 0.8736 - val_loss: 17.4656 - val_mae: 2.0591\n",
      "Epoch 237/500 loss: 1.5104 - mae: 0.8694 - val_loss: 16.8557 - val_mae: 2.0152\n",
      "Epoch 238/500 loss: 1.5027 - mae: 0.8604 - val_loss: 17.0573 - val_mae: 2.0073\n",
      "Epoch 239/500 loss: 1.5141 - mae: 0.8503 - val_loss: 17.1191 - val_mae: 2.0245\n",
      "Epoch 240/500 loss: 1.4961 - mae: 0.8420 - val_loss: 17.9903 - val_mae: 2.0605\n",
      "Epoch 241/500 loss: 1.5809 - mae: 0.8886 - val_loss: 17.7461 - val_mae: 2.0655\n",
      "Epoch 242/500 loss: 1.4372 - mae: 0.8343 - val_loss: 17.3690 - val_mae: 2.0061\n",
      "Epoch 243/500 loss: 1.5264 - mae: 0.8592 - val_loss: 17.2176 - val_mae: 2.0570\n",
      "Epoch 244/500 loss: 1.4888 - mae: 0.8540 - val_loss: 18.2420 - val_mae: 2.0812\n",
      "Epoch 245/500 loss: 1.5339 - mae: 0.8643 - val_loss: 19.4831 - val_mae: 2.1412\n",
      "Epoch 246/500 loss: 1.6245 - mae: 0.9055 - val_loss: 18.6350 - val_mae: 2.0490\n",
      "Epoch 247/500 loss: 1.5866 - mae: 0.8613 - val_loss: 16.5010 - val_mae: 2.0530\n",
      "Epoch 248/500 loss: 1.7220 - mae: 0.8807 - val_loss: 16.3784 - val_mae: 1.9723\n",
      "Epoch 249/500 loss: 1.6413 - mae: 0.9055 - val_loss: 16.7259 - val_mae: 1.9635\n",
      "Epoch 250/500 loss: 1.5514 - mae: 0.8753 - val_loss: 16.5306 - val_mae: 2.0378\n",
      "Epoch 251/500 loss: 1.4602 - mae: 0.8353 - val_loss: 17.7456 - val_mae: 2.0608\n",
      "Epoch 252/500 loss: 1.4884 - mae: 0.8499 - val_loss: 16.8414 - val_mae: 1.9056\n",
      "Epoch 253/500 loss: 1.6191 - mae: 0.8690 - val_loss: 17.4367 - val_mae: 2.0224\n",
      "Epoch 254/500 loss: 1.3625 - mae: 0.8175 - val_loss: 17.6558 - val_mae: 1.9814\n",
      "Epoch 255/500 loss: 1.4021 - mae: 0.8250 - val_loss: 16.7990 - val_mae: 2.0155\n",
      "Epoch 256/500 loss: 1.7048 - mae: 0.8617 - val_loss: 16.7950 - val_mae: 2.0043\n",
      "Epoch 257/500 loss: 1.3516 - mae: 0.7994 - val_loss: 16.7716 - val_mae: 1.9909\n",
      "Epoch 258/500 loss: 1.4414 - mae: 0.8484 - val_loss: 17.0463 - val_mae: 2.0611\n",
      "Epoch 259/500 loss: 1.5695 - mae: 0.8894 - val_loss: 17.1475 - val_mae: 2.0080\n",
      "Epoch 260/500 loss: 1.5431 - mae: 0.8489 - val_loss: 17.3972 - val_mae: 2.0039\n",
      "Epoch 261/500 loss: 1.4434 - mae: 0.8312 - val_loss: 17.2542 - val_mae: 2.0417\n",
      "Epoch 262/500 loss: 1.3543 - mae: 0.8185 - val_loss: 17.8616 - val_mae: 2.0366\n",
      "Epoch 263/500 loss: 1.6289 - mae: 0.8746 - val_loss: 17.6663 - val_mae: 2.1028\n",
      "Epoch 264/500 loss: 1.5837 - mae: 0.9165 - val_loss: 17.8706 - val_mae: 2.0358\n",
      "Epoch 265/500 loss: 1.3916 - mae: 0.8314 - val_loss: 16.5368 - val_mae: 2.0109\n",
      "Epoch 266/500 loss: 1.3438 - mae: 0.8029 - val_loss: 17.0717 - val_mae: 2.0428\n",
      "Epoch 267/500 loss: 1.2811 - mae: 0.7932 - val_loss: 16.9319 - val_mae: 2.0961\n",
      "Epoch 268/500 loss: 1.3261 - mae: 0.7983 - val_loss: 17.3566 - val_mae: 2.0021\n",
      "Epoch 269/500 loss: 1.3464 - mae: 0.8042 - val_loss: 17.3503 - val_mae: 2.0319\n",
      "Epoch 270/500 loss: 1.3055 - mae: 0.7609 - val_loss: 17.4821 - val_mae: 2.0395\n",
      "Epoch 271/500 loss: 1.3247 - mae: 0.7971 - val_loss: 17.8476 - val_mae: 2.0549\n",
      "Epoch 272/500 loss: 1.3859 - mae: 0.7841 - val_loss: 17.5991 - val_mae: 2.0590\n",
      "Epoch 273/500 loss: 1.7881 - mae: 0.9261 - val_loss: 17.3176 - val_mae: 2.0122\n",
      "Epoch 274/500 loss: 1.4080 - mae: 0.8036 - val_loss: 17.4181 - val_mae: 2.0955\n",
      "Epoch 275/500 loss: 1.2855 - mae: 0.7823 - val_loss: 17.7679 - val_mae: 2.0532\n",
      "Epoch 276/500 loss: 1.2726 - mae: 0.7608 - val_loss: 18.6769 - val_mae: 2.0467\n",
      "Epoch 277/500 loss: 1.2902 - mae: 0.7809 - val_loss: 17.5313 - val_mae: 2.0474\n",
      "Epoch 278/500 loss: 1.2137 - mae: 0.7468 - val_loss: 17.0965 - val_mae: 2.0450\n",
      "Epoch 279/500 loss: 1.2873 - mae: 0.7841 - val_loss: 17.7641 - val_mae: 2.0820\n",
      "Epoch 280/500 loss: 1.4066 - mae: 0.8369 - val_loss: 18.9295 - val_mae: 2.0238\n",
      "Epoch 281/500 loss: 1.5207 - mae: 0.9053 - val_loss: 16.9024 - val_mae: 1.9841\n",
      "Epoch 282/500 loss: 1.3231 - mae: 0.8075 - val_loss: 17.7788 - val_mae: 2.1336\n",
      "Epoch 283/500 loss: 1.2823 - mae: 0.7722 - val_loss: 16.6828 - val_mae: 2.0310\n",
      "Epoch 284/500 loss: 1.4280 - mae: 0.7885 - val_loss: 17.3565 - val_mae: 2.0327\n",
      "Epoch 285/500 loss: 1.3608 - mae: 0.8245 - val_loss: 17.6397 - val_mae: 1.9643\n",
      "Epoch 286/500 loss: 1.2073 - mae: 0.7553 - val_loss: 17.0528 - val_mae: 2.0313\n",
      "Epoch 287/500 loss: 1.1750 - mae: 0.7527 - val_loss: 17.2991 - val_mae: 2.0653\n",
      "Epoch 288/500 loss: 1.1670 - mae: 0.7722 - val_loss: 17.5101 - val_mae: 1.9995\n",
      "Epoch 289/500 loss: 1.2254 - mae: 0.7816 - val_loss: 18.4914 - val_mae: 2.2309\n",
      "Epoch 290/500 loss: 1.3141 - mae: 0.8246 - val_loss: 17.4364 - val_mae: 2.0294\n",
      "Epoch 291/500 loss: 1.1426 - mae: 0.7538 - val_loss: 17.3683 - val_mae: 2.0808\n",
      "Epoch 292/500 loss: 1.3578 - mae: 0.8574 - val_loss: 17.6324 - val_mae: 2.0224\n",
      "Epoch 293/500 loss: 1.2028 - mae: 0.7657 - val_loss: 18.7286 - val_mae: 2.1017\n",
      "Epoch 294/500 loss: 1.1422 - mae: 0.7503 - val_loss: 16.8812 - val_mae: 2.0277\n",
      "Epoch 295/500 loss: 1.1281 - mae: 0.7488 - val_loss: 18.3493 - val_mae: 2.0949\n",
      "Epoch 296/500 loss: 1.1353 - mae: 0.7573 - val_loss: 17.4311 - val_mae: 2.0627\n",
      "Epoch 297/500 loss: 1.1761 - mae: 0.7365 - val_loss: 17.3010 - val_mae: 2.0418\n",
      "Epoch 298/500 loss: 1.0867 - mae: 0.7176 - val_loss: 18.6409 - val_mae: 2.0654\n",
      "Epoch 299/500 loss: 1.1434 - mae: 0.7370 - val_loss: 18.3178 - val_mae: 2.0775\n",
      "Epoch 300/500 loss: 1.1138 - mae: 0.7464 - val_loss: 18.4735 - val_mae: 2.0675\n",
      "Epoch 301/500 loss: 1.0594 - mae: 0.7150 - val_loss: 17.0233 - val_mae: 2.0546\n",
      "Epoch 302/500 loss: 1.1527 - mae: 0.7492 - val_loss: 17.9510 - val_mae: 2.0818\n",
      "Epoch 303/500 loss: 1.2557 - mae: 0.7854 - val_loss: 17.7026 - val_mae: 2.0459\n",
      "Epoch 304/500 loss: 1.2559 - mae: 0.7482 - val_loss: 19.0832 - val_mae: 2.1924\n",
      "Epoch 305/500 loss: 1.2262 - mae: 0.7946 - val_loss: 18.2248 - val_mae: 2.2008\n",
      "Epoch 306/500 loss: 1.1834 - mae: 0.7564 - val_loss: 17.6075 - val_mae: 2.0537\n",
      "Epoch 307/500 loss: 1.0883 - mae: 0.7219 - val_loss: 18.6407 - val_mae: 2.0793\n",
      "Epoch 308/500 loss: 1.0932 - mae: 0.7216 - val_loss: 17.9917 - val_mae: 2.0778\n",
      "Epoch 309/500 loss: 1.1569 - mae: 0.7281 - val_loss: 17.4052 - val_mae: 2.0965\n",
      "Epoch 310/500 loss: 1.3312 - mae: 0.7829 - val_loss: 17.7597 - val_mae: 2.0469\n",
      "Epoch 311/500 loss: 1.1870 - mae: 0.7424 - val_loss: 17.6974 - val_mae: 2.0692\n",
      "Epoch 312/500 loss: 1.0866 - mae: 0.7187 - val_loss: 18.0998 - val_mae: 2.1181\n",
      "Epoch 313/500 loss: 1.0034 - mae: 0.6865 - val_loss: 18.5863 - val_mae: 2.1696\n",
      "Epoch 314/500 loss: 1.0644 - mae: 0.7178 - val_loss: 17.9465 - val_mae: 2.0527\n",
      "Epoch 315/500 loss: 1.0118 - mae: 0.6804 - val_loss: 17.9298 - val_mae: 2.0894\n",
      "Epoch 316/500 loss: 0.9716 - mae: 0.6521 - val_loss: 18.0952 - val_mae: 2.0826\n",
      "Epoch 317/500 loss: 1.0484 - mae: 0.6985 - val_loss: 17.5371 - val_mae: 2.1114\n",
      "Epoch 318/500 loss: 1.1002 - mae: 0.7010 - val_loss: 17.7811 - val_mae: 2.1111\n",
      "Epoch 319/500 loss: 0.9449 - mae: 0.6804 - val_loss: 18.3756 - val_mae: 2.0797\n",
      "Epoch 320/500 loss: 0.9940 - mae: 0.6890 - val_loss: 17.6670 - val_mae: 2.1011\n",
      "Epoch 321/500 loss: 1.0473 - mae: 0.6821 - val_loss: 17.5040 - val_mae: 2.1098\n",
      "Epoch 322/500 loss: 1.0479 - mae: 0.7194 - val_loss: 17.8049 - val_mae: 2.1632\n",
      "Epoch 323/500 loss: 0.9708 - mae: 0.6806 - val_loss: 17.2373 - val_mae: 2.0407\n",
      "Epoch 324/500 loss: 0.9790 - mae: 0.6884 - val_loss: 17.3323 - val_mae: 2.1127\n",
      "Epoch 325/500 loss: 0.9118 - mae: 0.6589 - val_loss: 17.6517 - val_mae: 2.1025\n",
      "Epoch 326/500 loss: 0.9550 - mae: 0.6749 - val_loss: 19.5972 - val_mae: 2.1323\n",
      "Epoch 327/500 loss: 0.9454 - mae: 0.6857 - val_loss: 18.4033 - val_mae: 2.0948\n",
      "Epoch 328/500 loss: 0.8895 - mae: 0.6523 - val_loss: 18.9396 - val_mae: 2.2032\n",
      "Epoch 329/500 loss: 1.0177 - mae: 0.7053 - val_loss: 18.1866 - val_mae: 2.0623\n",
      "Epoch 330/500 loss: 0.9408 - mae: 0.6630 - val_loss: 17.6844 - val_mae: 2.1697\n",
      "Epoch 331/500 loss: 0.9075 - mae: 0.6721 - val_loss: 18.3616 - val_mae: 2.1468\n",
      "Epoch 332/500 loss: 0.9863 - mae: 0.6868 - val_loss: 18.0086 - val_mae: 2.0710\n",
      "Epoch 333/500 loss: 0.9275 - mae: 0.6738 - val_loss: 17.4736 - val_mae: 2.1988\n",
      "Epoch 334/500 loss: 0.9180 - mae: 0.6674 - val_loss: 17.1519 - val_mae: 2.0762\n",
      "Epoch 335/500 loss: 0.9071 - mae: 0.6333 - val_loss: 18.1149 - val_mae: 2.1294\n",
      "Epoch 336/500 loss: 0.9557 - mae: 0.6687 - val_loss: 17.7684 - val_mae: 2.0984\n",
      "Epoch 337/500 loss: 0.9576 - mae: 0.6512 - val_loss: 18.2977 - val_mae: 2.1087\n",
      "Epoch 338/500 loss: 1.0440 - mae: 0.6613 - val_loss: 17.1628 - val_mae: 2.1235\n",
      "Epoch 339/500 loss: 1.0468 - mae: 0.7226 - val_loss: 19.0914 - val_mae: 2.1999\n",
      "Epoch 340/500 loss: 0.9426 - mae: 0.6853 - val_loss: 18.2649 - val_mae: 2.0654\n",
      "Epoch 341/500 loss: 0.8653 - mae: 0.6424 - val_loss: 18.1399 - val_mae: 2.1239\n",
      "Epoch 342/500 loss: 0.9714 - mae: 0.6950 - val_loss: 18.7774 - val_mae: 2.1696\n",
      "Epoch 343/500 loss: 0.8364 - mae: 0.6238 - val_loss: 18.0638 - val_mae: 2.1117\n",
      "Epoch 344/500 loss: 0.9535 - mae: 0.6676 - val_loss: 17.5757 - val_mae: 2.1117\n",
      "Epoch 345/500 loss: 0.9571 - mae: 0.6851 - val_loss: 18.0462 - val_mae: 2.1288\n",
      "Epoch 346/500 loss: 0.8824 - mae: 0.6256 - val_loss: 17.9941 - val_mae: 2.1249\n",
      "Epoch 347/500 loss: 0.9175 - mae: 0.6642 - val_loss: 18.7001 - val_mae: 2.1223\n",
      "Epoch 348/500 loss: 0.8790 - mae: 0.6242 - val_loss: 16.9101 - val_mae: 2.1468\n",
      "Epoch 349/500 loss: 1.1075 - mae: 0.7481 - val_loss: 18.0220 - val_mae: 2.1089\n",
      "Epoch 350/500 loss: 0.9440 - mae: 0.6717 - val_loss: 17.6061 - val_mae: 2.1384\n",
      "Epoch 351/500 loss: 0.9190 - mae: 0.6787 - val_loss: 18.5404 - val_mae: 2.1701\n",
      "Epoch 352/500 loss: 0.8468 - mae: 0.6299 - val_loss: 18.3050 - val_mae: 2.1370\n",
      "Epoch 353/500 loss: 0.7784 - mae: 0.6076 - val_loss: 17.9535 - val_mae: 2.1524\n",
      "Epoch 354/500 loss: 0.8346 - mae: 0.6111 - val_loss: 17.8450 - val_mae: 2.0946\n",
      "Epoch 355/500 loss: 0.8473 - mae: 0.6197 - val_loss: 19.0337 - val_mae: 2.1499\n",
      "Epoch 356/500 loss: 0.8188 - mae: 0.6481 - val_loss: 17.5219 - val_mae: 2.1621\n",
      "Epoch 357/500 loss: 0.8064 - mae: 0.6157 - val_loss: 17.7237 - val_mae: 2.1289\n",
      "Epoch 358/500 loss: 0.7730 - mae: 0.6072 - val_loss: 18.2621 - val_mae: 2.1164\n",
      "Epoch 359/500 loss: 0.8419 - mae: 0.6333 - val_loss: 18.3211 - val_mae: 2.1174\n",
      "Epoch 360/500 loss: 0.7562 - mae: 0.6020 - val_loss: 18.7214 - val_mae: 2.1478\n",
      "Epoch 361/500 loss: 0.7997 - mae: 0.6105 - val_loss: 18.2094 - val_mae: 2.1155\n",
      "Epoch 362/500 loss: 0.8180 - mae: 0.6110 - val_loss: 18.2139 - val_mae: 2.1256\n",
      "Epoch 363/500 loss: 0.8454 - mae: 0.6275 - val_loss: 18.3314 - val_mae: 2.1260\n",
      "Epoch 364/500 loss: 0.7833 - mae: 0.5955 - val_loss: 18.5116 - val_mae: 2.1567\n",
      "Epoch 365/500 loss: 0.8482 - mae: 0.6331 - val_loss: 18.0136 - val_mae: 2.1444\n",
      "Epoch 366/500 loss: 0.8313 - mae: 0.6404 - val_loss: 18.2367 - val_mae: 2.1510\n",
      "Epoch 367/500 loss: 0.7606 - mae: 0.6031 - val_loss: 17.8408 - val_mae: 2.1463\n",
      "Epoch 368/500 loss: 0.7513 - mae: 0.5932 - val_loss: 18.7057 - val_mae: 2.1491\n",
      "Epoch 369/500 loss: 0.8386 - mae: 0.6001 - val_loss: 18.4973 - val_mae: 2.2265\n",
      "Epoch 370/500 loss: 0.9315 - mae: 0.6647 - val_loss: 17.6375 - val_mae: 2.1286\n",
      "Epoch 371/500 loss: 0.9439 - mae: 0.7004 - val_loss: 18.6465 - val_mae: 2.1391\n",
      "Epoch 372/500 loss: 0.8821 - mae: 0.6551 - val_loss: 19.0379 - val_mae: 2.2179\n",
      "Epoch 373/500 loss: 0.7448 - mae: 0.5892 - val_loss: 18.3826 - val_mae: 2.1552\n",
      "Epoch 374/500 loss: 0.7772 - mae: 0.6085 - val_loss: 19.0753 - val_mae: 2.1730\n",
      "Epoch 375/500 loss: 0.7452 - mae: 0.5973 - val_loss: 18.6783 - val_mae: 2.1546\n",
      "Epoch 376/500 loss: 0.7371 - mae: 0.5841 - val_loss: 18.3038 - val_mae: 2.2145\n",
      "Epoch 377/500 loss: 0.7784 - mae: 0.6302 - val_loss: 18.9573 - val_mae: 2.1599\n",
      "Epoch 378/500 loss: 0.6931 - mae: 0.5752 - val_loss: 18.1213 - val_mae: 2.1834\n",
      "Epoch 379/500 loss: 0.7182 - mae: 0.5797 - val_loss: 18.3427 - val_mae: 2.1989\n",
      "Epoch 380/500 loss: 0.8547 - mae: 0.6696 - val_loss: 18.6819 - val_mae: 2.1923\n",
      "Epoch 381/500 loss: 0.8202 - mae: 0.5930 - val_loss: 19.6541 - val_mae: 2.2110\n",
      "Epoch 382/500 loss: 0.8269 - mae: 0.6354 - val_loss: 17.9015 - val_mae: 2.1832\n",
      "Epoch 383/500 loss: 0.8088 - mae: 0.5893 - val_loss: 17.9327 - val_mae: 2.1654\n",
      "Epoch 384/500 loss: 0.8261 - mae: 0.6590 - val_loss: 19.3402 - val_mae: 2.2240\n",
      "Epoch 385/500 loss: 0.8102 - mae: 0.6446 - val_loss: 18.5405 - val_mae: 2.1059\n",
      "Epoch 386/500 loss: 0.8440 - mae: 0.6012 - val_loss: 17.8473 - val_mae: 2.1649\n",
      "Epoch 387/500 loss: 0.8203 - mae: 0.6458 - val_loss: 18.5485 - val_mae: 2.1836\n",
      "Epoch 388/500 loss: 0.8428 - mae: 0.6396 - val_loss: 18.2205 - val_mae: 2.1136\n",
      "Epoch 389/500 loss: 0.7237 - mae: 0.5889 - val_loss: 18.6656 - val_mae: 2.1966\n",
      "Epoch 390/500 loss: 0.9147 - mae: 0.6742 - val_loss: 18.4982 - val_mae: 2.1996\n",
      "Epoch 391/500 loss: 0.7891 - mae: 0.6232 - val_loss: 19.0628 - val_mae: 2.1869\n",
      "Epoch 392/500 loss: 0.8020 - mae: 0.5930 - val_loss: 18.6550 - val_mae: 2.1278\n",
      "Epoch 393/500 loss: 0.6940 - mae: 0.5660 - val_loss: 19.1070 - val_mae: 2.1805\n",
      "Epoch 394/500 loss: 0.7397 - mae: 0.5935 - val_loss: 18.5248 - val_mae: 2.1810\n",
      "Epoch 395/500 loss: 0.8723 - mae: 0.6266 - val_loss: 18.2769 - val_mae: 2.1349\n",
      "Epoch 396/500 loss: 0.9193 - mae: 0.6925 - val_loss: 18.5699 - val_mae: 2.1948\n",
      "Epoch 397/500 loss: 0.7552 - mae: 0.6119 - val_loss: 18.4721 - val_mae: 2.2539\n",
      "Epoch 398/500 loss: 0.7624 - mae: 0.6213 - val_loss: 18.5870 - val_mae: 2.1487\n",
      "Epoch 399/500 loss: 0.7596 - mae: 0.5805 - val_loss: 19.0188 - val_mae: 2.1604\n",
      "Epoch 400/500 loss: 0.7859 - mae: 0.6234 - val_loss: 18.6654 - val_mae: 2.1530\n",
      "Epoch 401/500 loss: 0.6907 - mae: 0.5744 - val_loss: 19.2313 - val_mae: 2.1869\n",
      "Epoch 402/500 loss: 0.6613 - mae: 0.5338 - val_loss: 18.2229 - val_mae: 2.1530\n",
      "Epoch 403/500 loss: 0.7594 - mae: 0.6356 - val_loss: 18.8127 - val_mae: 2.1563\n",
      "Epoch 404/500 loss: 0.6299 - mae: 0.5576 - val_loss: 19.0027 - val_mae: 2.1813\n",
      "Epoch 405/500 loss: 0.6481 - mae: 0.5586 - val_loss: 18.5810 - val_mae: 2.1594\n",
      "Epoch 406/500 loss: 0.6141 - mae: 0.5459 - val_loss: 19.0246 - val_mae: 2.1768\n",
      "Epoch 407/500 loss: 0.5813 - mae: 0.5048 - val_loss: 18.8199 - val_mae: 2.1784\n",
      "Epoch 408/500 loss: 0.7204 - mae: 0.5229 - val_loss: 18.6214 - val_mae: 2.2358\n",
      "Epoch 409/500 loss: 0.8159 - mae: 0.6500 - val_loss: 19.1332 - val_mae: 2.1665\n",
      "Epoch 410/500 loss: 0.6695 - mae: 0.5838 - val_loss: 18.6911 - val_mae: 2.1281\n",
      "Epoch 411/500 loss: 0.6623 - mae: 0.5745 - val_loss: 19.2870 - val_mae: 2.1408\n",
      "Epoch 412/500 loss: 0.6783 - mae: 0.5699 - val_loss: 18.6579 - val_mae: 2.1981\n",
      "Epoch 413/500 loss: 0.6530 - mae: 0.5486 - val_loss: 18.9735 - val_mae: 2.1944\n",
      "Epoch 414/500 loss: 0.6104 - mae: 0.5421 - val_loss: 18.6153 - val_mae: 2.1440\n",
      "Epoch 415/500 loss: 0.6149 - mae: 0.5424 - val_loss: 18.9693 - val_mae: 2.1876\n",
      "Epoch 416/500 loss: 0.5770 - mae: 0.5190 - val_loss: 19.6605 - val_mae: 2.1934\n",
      "Epoch 417/500 loss: 0.7709 - mae: 0.6008 - val_loss: 18.4300 - val_mae: 2.1518\n",
      "Epoch 418/500 loss: 0.7692 - mae: 0.6014 - val_loss: 18.8206 - val_mae: 2.2208\n",
      "Epoch 419/500 loss: 0.6566 - mae: 0.5787 - val_loss: 19.2186 - val_mae: 2.2114\n",
      "Epoch 420/500 loss: 0.6514 - mae: 0.5602 - val_loss: 19.3432 - val_mae: 2.1881\n",
      "Epoch 421/500 loss: 0.5783 - mae: 0.5242 - val_loss: 19.0417 - val_mae: 2.2395\n",
      "Epoch 422/500 loss: 0.5906 - mae: 0.5237 - val_loss: 18.6983 - val_mae: 2.1776\n",
      "Epoch 423/500 loss: 0.5401 - mae: 0.4868 - val_loss: 18.8656 - val_mae: 2.2790\n",
      "Epoch 424/500 loss: 0.5855 - mae: 0.5345 - val_loss: 19.9667 - val_mae: 2.2052\n",
      "Epoch 425/500 loss: 0.6586 - mae: 0.5757 - val_loss: 19.0688 - val_mae: 2.2088\n",
      "Epoch 426/500 loss: 0.7895 - mae: 0.6317 - val_loss: 19.7680 - val_mae: 2.2278\n",
      "Epoch 427/500 loss: 0.6856 - mae: 0.5997 - val_loss: 17.9890 - val_mae: 2.1570\n",
      "Epoch 428/500 loss: 0.5745 - mae: 0.5293 - val_loss: 19.1304 - val_mae: 2.2417\n",
      "Epoch 429/500 loss: 0.5817 - mae: 0.5494 - val_loss: 19.4185 - val_mae: 2.1906\n",
      "Epoch 430/500 loss: 0.6289 - mae: 0.5692 - val_loss: 18.9703 - val_mae: 2.2544\n",
      "Epoch 431/500 loss: 0.6299 - mae: 0.5474 - val_loss: 19.0839 - val_mae: 2.2297\n",
      "Epoch 432/500 loss: 0.5633 - mae: 0.5120 - val_loss: 18.8139 - val_mae: 2.2249\n",
      "Epoch 433/500 loss: 0.5399 - mae: 0.5150 - val_loss: 18.5731 - val_mae: 2.2199\n",
      "Epoch 434/500 loss: 0.5880 - mae: 0.5328 - val_loss: 19.0351 - val_mae: 2.2216\n",
      "Epoch 435/500 loss: 0.5510 - mae: 0.5148 - val_loss: 18.2487 - val_mae: 2.2101\n",
      "Epoch 436/500 loss: 0.5722 - mae: 0.5241 - val_loss: 19.0677 - val_mae: 2.1851\n",
      "Epoch 437/500 loss: 0.5687 - mae: 0.5201 - val_loss: 19.4750 - val_mae: 2.2414\n",
      "Epoch 438/500 loss: 0.6683 - mae: 0.5808 - val_loss: 18.9066 - val_mae: 2.1878\n",
      "Epoch 439/500 loss: 0.6735 - mae: 0.5735 - val_loss: 20.4366 - val_mae: 2.3288\n",
      "Epoch 440/500 loss: 0.7308 - mae: 0.6096 - val_loss: 18.7370 - val_mae: 2.1705\n",
      "Epoch 441/500 loss: 0.5067 - mae: 0.4865 - val_loss: 17.8942 - val_mae: 2.1696\n",
      "Epoch 442/500 loss: 0.5722 - mae: 0.5366 - val_loss: 18.7931 - val_mae: 2.1792\n",
      "Epoch 443/500 loss: 0.5766 - mae: 0.5362 - val_loss: 19.3603 - val_mae: 2.2286\n",
      "Epoch 444/500 loss: 0.6771 - mae: 0.5349 - val_loss: 19.1887 - val_mae: 2.2734\n",
      "Epoch 445/500 loss: 0.7110 - mae: 0.6008 - val_loss: 18.8413 - val_mae: 2.2704\n",
      "Epoch 446/500 loss: 0.5439 - mae: 0.5007 - val_loss: 19.7937 - val_mae: 2.2362\n",
      "Epoch 447/500 loss: 0.5646 - mae: 0.5198 - val_loss: 18.7746 - val_mae: 2.1732\n",
      "Epoch 448/500 loss: 0.5200 - mae: 0.4929 - val_loss: 19.5561 - val_mae: 2.2375\n",
      "Epoch 449/500 loss: 0.5246 - mae: 0.5064 - val_loss: 18.7804 - val_mae: 2.2903\n",
      "Epoch 450/500 loss: 0.6531 - mae: 0.5818 - val_loss: 18.9062 - val_mae: 2.2482\n",
      "Epoch 451/500 loss: 0.6531 - mae: 0.5944 - val_loss: 19.2511 - val_mae: 2.2391\n",
      "Epoch 452/500 loss: 0.6107 - mae: 0.5492 - val_loss: 19.0928 - val_mae: 2.3484\n",
      "Epoch 453/500 loss: 0.6370 - mae: 0.5746 - val_loss: 18.3901 - val_mae: 2.2055\n",
      "Epoch 454/500 loss: 0.7151 - mae: 0.6102 - val_loss: 19.1845 - val_mae: 2.2263\n",
      "Epoch 455/500 loss: 0.6774 - mae: 0.5806 - val_loss: 19.4166 - val_mae: 2.2618\n",
      "Epoch 456/500 loss: 0.5553 - mae: 0.5014 - val_loss: 18.7086 - val_mae: 2.1785\n",
      "Epoch 457/500 loss: 0.4916 - mae: 0.4734 - val_loss: 18.4797 - val_mae: 2.2414\n",
      "Epoch 458/500 loss: 0.4613 - mae: 0.4589 - val_loss: 18.8450 - val_mae: 2.2318\n",
      "Epoch 459/500 loss: 0.4993 - mae: 0.4851 - val_loss: 19.2520 - val_mae: 2.2768\n",
      "Epoch 460/500 loss: 0.5668 - mae: 0.5428 - val_loss: 19.2836 - val_mae: 2.2844\n",
      "Epoch 461/500 loss: 0.4925 - mae: 0.4923 - val_loss: 19.1434 - val_mae: 2.2269\n",
      "Epoch 462/500 loss: 0.5195 - mae: 0.5028 - val_loss: 19.3472 - val_mae: 2.2282\n",
      "Epoch 463/500 loss: 0.6382 - mae: 0.5377 - val_loss: 19.5075 - val_mae: 2.2723\n",
      "Epoch 464/500 loss: 0.6115 - mae: 0.5346 - val_loss: 18.8648 - val_mae: 2.2651\n",
      "Epoch 465/500 loss: 0.5459 - mae: 0.4928 - val_loss: 19.7340 - val_mae: 2.2660\n",
      "Epoch 466/500 loss: 0.5798 - mae: 0.5387 - val_loss: 19.0460 - val_mae: 2.2435\n",
      "Epoch 467/500 loss: 0.5488 - mae: 0.5257 - val_loss: 18.9477 - val_mae: 2.2053\n",
      "Epoch 468/500 loss: 0.5185 - mae: 0.4925 - val_loss: 19.4307 - val_mae: 2.2377\n",
      "Epoch 469/500 loss: 0.4973 - mae: 0.4804 - val_loss: 19.3805 - val_mae: 2.2305\n",
      "Epoch 470/500 loss: 0.7201 - mae: 0.6145 - val_loss: 19.9055 - val_mae: 2.2491\n",
      "Epoch 471/500 loss: 0.5682 - mae: 0.5196 - val_loss: 19.5898 - val_mae: 2.2398\n",
      "Epoch 472/500 loss: 0.5698 - mae: 0.5390 - val_loss: 18.0788 - val_mae: 2.2497\n",
      "Epoch 473/500 loss: 0.6252 - mae: 0.5782 - val_loss: 19.7523 - val_mae: 2.2631\n",
      "Epoch 474/500 loss: 0.5657 - mae: 0.5225 - val_loss: 19.5447 - val_mae: 2.2350\n",
      "Epoch 475/500 loss: 0.5277 - mae: 0.5074 - val_loss: 19.6000 - val_mae: 2.3086\n",
      "Epoch 476/500 loss: 0.4968 - mae: 0.4838 - val_loss: 19.9105 - val_mae: 2.3024\n",
      "Epoch 477/500 loss: 0.5074 - mae: 0.4714 - val_loss: 19.6019 - val_mae: 2.2867\n",
      "Epoch 478/500 loss: 0.5020 - mae: 0.4755 - val_loss: 19.1827 - val_mae: 2.2826\n",
      "Epoch 479/500 loss: 0.5792 - mae: 0.5488 - val_loss: 19.5239 - val_mae: 2.2207\n",
      "Epoch 480/500 loss: 0.5176 - mae: 0.5224 - val_loss: 18.7013 - val_mae: 2.2242\n",
      "Epoch 481/500 loss: 0.5230 - mae: 0.5142 - val_loss: 18.9932 - val_mae: 2.2374\n",
      "Epoch 482/500 loss: 0.4149 - mae: 0.4437 - val_loss: 19.0242 - val_mae: 2.2565\n",
      "Epoch 483/500 loss: 0.4182 - mae: 0.4228 - val_loss: 19.2448 - val_mae: 2.2956\n",
      "Epoch 484/500 loss: 0.4790 - mae: 0.4710 - val_loss: 19.5810 - val_mae: 2.2676\n",
      "Epoch 485/500 loss: 0.5136 - mae: 0.4852 - val_loss: 19.3241 - val_mae: 2.2347\n",
      "Epoch 486/500 loss: 0.5805 - mae: 0.5520 - val_loss: 19.9028 - val_mae: 2.3083\n",
      "Epoch 487/500 loss: 0.4877 - mae: 0.4760 - val_loss: 19.5237 - val_mae: 2.2652\n",
      "Epoch 488/500 loss: 0.4745 - mae: 0.4761 - val_loss: 19.5636 - val_mae: 2.2789\n",
      "Epoch 489/500 loss: 0.4298 - mae: 0.4388 - val_loss: 19.2902 - val_mae: 2.2509\n",
      "Epoch 490/500 loss: 0.4680 - mae: 0.4661 - val_loss: 19.9105 - val_mae: 2.2731\n",
      "Epoch 491/500 loss: 0.6449 - mae: 0.5682 - val_loss: 20.1001 - val_mae: 2.2977\n",
      "Epoch 492/500 loss: 0.6692 - mae: 0.5765 - val_loss: 20.3694 - val_mae: 2.3300\n",
      "Epoch 493/500 loss: 0.6444 - mae: 0.5534 - val_loss: 19.1643 - val_mae: 2.2700\n",
      "Epoch 494/500 loss: 0.6932 - mae: 0.5814 - val_loss: 19.1815 - val_mae: 2.2545\n",
      "Epoch 495/500 loss: 0.4963 - mae: 0.4895 - val_loss: 19.8076 - val_mae: 2.2380\n",
      "Epoch 496/500 loss: 0.4474 - mae: 0.4533 - val_loss: 19.4775 - val_mae: 2.2570\n",
      "Epoch 497/500 loss: 0.4647 - mae: 0.4726 - val_loss: 19.6086 - val_mae: 2.2356\n",
      "Epoch 498/500 loss: 0.4544 - mae: 0.4641 - val_loss: 19.4739 - val_mae: 2.2425\n",
      "Epoch 499/500 loss: 0.4399 - mae: 0.4501 - val_loss: 18.6974 - val_mae: 2.1851\n",
      "Epoch 500/500 loss: 0.5182 - mae: 0.5186 - val_loss: 19.8942 - val_mae: 2.2246\n"
     ]
    }
   ],
   "source": [
    "# num epochs\n",
    "epochs = 500\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(params=boston_housing_model.parameters())\n",
    "\n",
    "# start training\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # accummulate losses of all batches from training\n",
    "    accummulated_trained_loss = 0.0\n",
    "    accummulated_trained_batches = 0\n",
    "    accummulated_trained_absoluate_error = 0.0\n",
    "\n",
    "    for train_inputs, train_labels in train_loader:\n",
    "\n",
    "        train_inputs = train_inputs.to(device)\n",
    "        train_labels = train_labels.to(device)\n",
    "\n",
    "        # train mode\n",
    "        boston_housing_model.train()\n",
    "\n",
    "        # forward pass \n",
    "        y_preds_train = boston_housing_model(train_inputs)\n",
    "\n",
    "        # calculate loss between y_preds_train with labels\n",
    "        loss = loss_fn(train_labels, y_preds_train)\n",
    "\n",
    "        # reset learned parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward propagration\n",
    "        loss.backward()\n",
    "\n",
    "        # update paramenters\n",
    "        optimizer.step()  \n",
    "\n",
    "        # accummulate loss\n",
    "        accummulated_trained_loss += loss.item()\n",
    "\n",
    "        # accummulate batches\n",
    "        accummulated_trained_batches += 1\n",
    "\n",
    "        # train absolute error\n",
    "        accummulated_trained_absoluate_error += (train_labels - y_preds_train.data).abs().sum().item()\n",
    "\n",
    "    train_loss =  accummulated_trained_loss / accummulated_trained_batches\n",
    "    train_mae = accummulated_trained_absoluate_error / (accummulated_trained_batches * batch_size)\n",
    "\n",
    "    # eval mode\n",
    "    boston_housing_model.eval()\n",
    "\n",
    "    # inference only\n",
    "    with torch.inference_mode():\n",
    "                \n",
    "        # accummulate losses of all batches from training\n",
    "        accummulated_tested_loss = 0.0\n",
    "        accummulated_tested_batches = 0\n",
    "        accummulated_tested_absoluate_error = 0.0\n",
    "\n",
    "        for test_inputs, test_labels in test_loader:    \n",
    "\n",
    "            test_inputs = test_inputs.to(device)\n",
    "            test_labels = test_labels.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            y_preds_test = boston_housing_model(test_inputs)\n",
    "\n",
    "            # calculate loss between y_preds_test with test_labels\n",
    "            loss_test = loss_fn(test_labels, y_preds_test)\n",
    "\n",
    "            # accummulate losses of batches from testing\n",
    "            accummulated_tested_loss += loss_test.item()\n",
    "            accummulated_tested_batches += 1\n",
    "            accummulated_tested_absoluate_error += (test_labels - y_preds_test.data).abs().sum().item()\n",
    "        \n",
    "        test_loss = accummulated_tested_loss / accummulated_tested_batches\n",
    "        test_mae = accummulated_tested_absoluate_error / (accummulated_tested_batches * batch_size)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs} loss: {train_loss:.4f} - mae: {train_mae:0.4f} - val_loss: {test_loss:.4f} - val_mae: {test_mae:0.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate \n",
    "boston_housing_model.eval() # eval mode\n",
    "with torch.inference_mode(): # inference only\n",
    "    y_preds_test = boston_housing_model(torch.from_numpy(x_test).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y:  28.44 | label:  23.60\n",
      "predicted y:  34.19 | label:  32.40\n",
      "predicted y:  12.10 | label:  13.60\n",
      "predicted y:  21.37 | label:  22.80\n"
     ]
    }
   ],
   "source": [
    "# check the diffence between predictions and labels\n",
    "for i in range(0,4):\n",
    "    print(f\"predicted y: {y_preds_test[i].squeeze(): 0.2f} | label: {y_test[i].squeeze(): 0.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tadac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
