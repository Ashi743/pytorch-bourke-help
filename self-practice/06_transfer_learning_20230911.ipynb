{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general libs\n",
    "import os \n",
    "import gdown\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb\n",
    "import nvidia_smi\n",
    "import socket\n",
    "\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "# tensorflow libs\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "\n",
    "# current time\n",
    "now_str = datetime.now().strftime(\"%a, %d %b %Y %H:%M:%S\")\n",
    "date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "datetime_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "time_str = datetime.now().strftime(\"%H%M%S\")\n",
    "\n",
    "# model \n",
    "model_name = \"efficientnet_b0\"\n",
    "\n",
    "# current dir\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# batch size \n",
    "batch_size = 256\n",
    "\n",
    "# using wandb\n",
    "is_wandb = True\n",
    "\n",
    "# current dir name\n",
    "current_dir_name = os.path.basename(current_dir)\n",
    "\n",
    "# project name\n",
    "project_name = f\"{current_dir_name}/06_transfer_learning_20230911/{date_str}/{model_name}_bs_{batch_size}_{time_str}\"\n",
    "\n",
    "# file name\n",
    "file_name = f\"{model_name}_bs_{batch_size}_{datetime_str}\"\n",
    "\n",
    "# data directory\n",
    "DATA_DIR = Path(\"../../data/\")\n",
    "if not DATA_DIR.is_dir():\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# log directory\n",
    "LOG_DIR = Path(f\"../../logs/{project_name}\")\n",
    "if not LOG_DIR.is_dir():\n",
    "    LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "else: \n",
    "    # remove folders\n",
    "    os.system(f\"rm -rf {LOG_DIR}\")\n",
    "    LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_FILE_PATH = LOG_DIR.joinpath(f\"{file_name}.log\")\n",
    "FE_HISTORY_CURVES_FILE_PATH = LOG_DIR.joinpath(f\"fe_history_curve_{file_name}.png\")\n",
    "FT_HISTORY_CURVES_FILE_PATH = LOG_DIR.joinpath(f\"ft_history_curve_{file_name}.png\")\n",
    "FE_PREDICTION_FILE_PATH = LOG_DIR.joinpath(f\"fe_prediction_{file_name}.png\")\n",
    "FT_PREDICTION_FILE_PATH = LOG_DIR.joinpath(f\"ft_prediction_{file_name}.png\")\n",
    "\n",
    "# model directory\n",
    "MODEL_DIR = Path(f\"../../models/{project_name}\")\n",
    "if not MODEL_DIR.is_dir():\n",
    "    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "else: \n",
    "    # remove folders\n",
    "    os.system(f\"rm -rf {MODEL_DIR}\")\n",
    "    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_FEATURE_EXTRACTION_DIR = MODEL_DIR.joinpath(f\"fe_{file_name}/checkpoint.ckpt\")\n",
    "if not MODEL_FEATURE_EXTRACTION_DIR.is_dir():\n",
    "    MODEL_FEATURE_EXTRACTION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_FINE_TUNING_DIR = MODEL_DIR.joinpath(f\"ft_{file_name}/checkpoint.ckpt\")\n",
    "if not MODEL_FINE_TUNING_DIR.is_dir():\n",
    "    MODEL_FINE_TUNING_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%a, %d %b %Y %H:%M:%S\",\n",
    "    format=\"[%(asctime)s.%(msecs)03d] %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(filename=LOG_FILE_PATH, mode=\"w\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# created date\n",
    "logging.info(f\"Created date: Mon, 11 Sep 2023 13:23:45\")\n",
    "\n",
    "# modified date\n",
    "logging.info(f\"Modified date: {now_str}\")\n",
    "\n",
    "logging.info(f\"Model name: {model_name}\")\n",
    "# logging.info(f\"Current dir: {current_dir}\")\n",
    "logging.info(f\"Current dir name: {current_dir_name}\")\n",
    "logging.info(f\"Project name: {project_name}\")\n",
    "logging.info(f\"Model feature extraction dir: {MODEL_FEATURE_EXTRACTION_DIR}\")\n",
    "logging.info(f\"Model fine tuning dir: {MODEL_FINE_TUNING_DIR}\")\n",
    "\n",
    "# tensorflow version\n",
    "logging.info(f\"TensorFlow version: {tf.__version__}\")\n",
    "logging.info(f\"Keras version: {tf.keras.__version__}\")\n",
    "\n",
    "# physical devices\n",
    "gpus = len(tf.config.list_physical_devices(\"GPU\"))\n",
    "logging.info(f\"GPUs: {gpus}\")\n",
    "if gpus > 0:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(tf.config.list_physical_devices(\"GPU\"), \"GPU\")\n",
    "    except Exception as error:\n",
    "        logging.error(f\"Caught this error during setting available devices: {error}\")\n",
    "\n",
    "# cpus\n",
    "cpus = os.cpu_count()\n",
    "logging.info(f\"CPUs: {cpus}\")\n",
    "\n",
    "# dataset name\n",
    "dataset_name = \"food10\"\n",
    "logging.info(f\"Dataset name: {dataset_name}\")\n",
    "\n",
    "# batch_size\n",
    "logging.info(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# wandb\n",
    "logging.info(f\"Use wandb: {is_wandb}\")\n",
    "\n",
    "## seed\n",
    "# default seed\n",
    "default_seed = 42\n",
    "logging.info(f\"Default seed: {default_seed}\")\n",
    "\n",
    "random.seed(default_seed)\n",
    "np.random.seed(default_seed)\n",
    "\n",
    "tf.random.set_seed(default_seed)\n",
    "tf.keras.utils.set_random_seed(default_seed)\n",
    "# tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# when running on the CuDNN backend, two further options must be set\n",
    "# os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# set a fixed value for the hash seed\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(default_seed)\n",
    "\n",
    "## dataset downloading\n",
    "# 10 percents\n",
    "DATASET_FOOD10_10_PERCENT_URL = \"https://drive.google.com/uc?id=17Yw1PGyDGpwV77Ds6VDBQ7Iv9Q2rQR6P\"\n",
    "DATASET_FOOD10_10_PERCENT_ZIPFILE_NAME = \"10_food_classes_10_percent.zip\"\n",
    "DATASET_FOOD10_10_PERCENT_ZIPFILE_PATH = DATA_DIR.joinpath(DATASET_FOOD10_10_PERCENT_ZIPFILE_NAME)\n",
    "DATASET_FOOD10_10_PERCENT_FOLDER_NAME = \"10_food_classes_10_percent\"\n",
    "DATASET_FOOD10_10_PERCENT_FOLDER_PATH = DATA_DIR.joinpath(DATASET_FOOD10_10_PERCENT_FOLDER_NAME)\n",
    "\n",
    "# full data \n",
    "DATASET_FOOD10_FULL_URL = \"https://drive.google.com/uc?id=1h9Zvm0UKeGMk8hXSdfCJ5E1hIgxvQHN6\"\n",
    "DATASET_FOOD10_FULL_ZIPFILE_NAME = \"food10.zip\"\n",
    "DATASET_FOOD10_FULL_ZIPFILE_PATH = DATA_DIR.joinpath(DATASET_FOOD10_FULL_ZIPFILE_NAME)\n",
    "DATASET_FOOD10_FULL_FOLDER_NAME =  \"food10\"\n",
    "DATASET_FOOD10_FULL_FOLDER_PATH =  DATA_DIR.joinpath(DATASET_FOOD10_FULL_FOLDER_NAME)\n",
    "\n",
    "# download 10 percents dataset\n",
    "if not DATASET_FOOD10_10_PERCENT_ZIPFILE_PATH.is_file():\n",
    "    logging.info(f\"The {DATASET_FOOD10_10_PERCENT_ZIPFILE_NAME} is downloading...\")\n",
    "    try:\n",
    "        gdown.download(url=DATASET_FOOD10_10_PERCENT_URL, output=str(DATASET_FOOD10_10_PERCENT_ZIPFILE_PATH))\n",
    "        logging.info(f\"The {DATASET_FOOD10_10_PERCENT_ZIPFILE_NAME} is downloaded successfully.\")\n",
    "    except Exception as error:\n",
    "        logging.info(f\"Caught this error during downloading {DATASET_FOOD10_10_PERCENT_ZIPFILE_NAME}\")\n",
    "else:\n",
    "    logging.info(f\"The {DATASET_FOOD10_10_PERCENT_ZIPFILE_NAME} already exists.\")\n",
    "\n",
    "# download full dataset\n",
    "if not DATASET_FOOD10_FULL_ZIPFILE_PATH.is_file():\n",
    "    logging.info(f\"The {DATASET_FOOD10_FULL_ZIPFILE_NAME} is downloading...\")\n",
    "    try:\n",
    "        gdown.download(url=DATASET_FOOD10_FULL_URL, output=str(DATASET_FOOD10_FULL_ZIPFILE_PATH))\n",
    "        logging.info(f\"The {DATASET_FOOD10_FULL_ZIPFILE_NAME} is downloaded successfully.\")\n",
    "    except Exception as error:\n",
    "        logging.info(f\"Caught this error during downloading {DATASET_FOOD10_FULL_ZIPFILE_NAME}\")\n",
    "else:\n",
    "    logging.info(f\"The {DATASET_FOOD10_FULL_ZIPFILE_NAME} already exists.\")\n",
    "\n",
    "\n",
    "# extract 10 percents dataset\n",
    "if not DATASET_FOOD10_10_PERCENT_FOLDER_PATH.is_dir():\n",
    "    logging.info(f\"The {DATASET_FOOD10_10_PERCENT_ZIPFILE_NAME} is extracting...\")\n",
    "    try:\n",
    "        gdown.extractall(path= str(DATASET_FOOD10_10_PERCENT_ZIPFILE_PATH), to=str(DATA_DIR))\n",
    "        logging.info(f\"The {DATASET_FOOD10_10_PERCENT_ZIPFILE_NAME} is extracted successfully.\")\n",
    "    except Exception as error:\n",
    "        logging.info(f\"Caught this error during extracting {DATASET_FOOD10_10_PERCENT_ZIPFILE_NAME}\")\n",
    "else:\n",
    "    logging.info(f\"The {DATASET_FOOD10_10_PERCENT_FOLDER_PATH} already exists.\")\n",
    "\n",
    "# extract full dataset\n",
    "if not DATASET_FOOD10_FULL_FOLDER_PATH.is_dir():\n",
    "    logging.info(f\"The {DATASET_FOOD10_FULL_ZIPFILE_NAME} is extracting...\")\n",
    "    try:\n",
    "        gdown.extractall(path= str(DATASET_FOOD10_FULL_ZIPFILE_PATH), to=str(DATA_DIR))\n",
    "        logging.info(f\"The {DATASET_FOOD10_FULL_ZIPFILE_NAME} is extracted successfully.\")\n",
    "    except Exception as error:\n",
    "        logging.info(f\"Caught this error during extracting {DATASET_FOOD10_FULL_ZIPFILE_NAME}\")\n",
    "else:\n",
    "    logging.info(f\"The {DATASET_FOOD10_FULL_FOLDER_PATH} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_through_data(data_dir: Path):\n",
    "    total_images = 0\n",
    "    if data_dir.is_dir():\n",
    "        for filepaths, dirnames, filenames in os.walk(str(data_dir)):\n",
    "            if len(filenames) > 0:\n",
    "                total_images += len(filenames)\n",
    "        logging.info(f\"There are {len(filenames)} images in the {data_dir}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"There is no given directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preparing\n",
    "\n",
    "# 10 percents training set\n",
    "train_10_percents_dir = DATASET_FOOD10_10_PERCENT_FOLDER_PATH.joinpath(\"train\")\n",
    "walk_through_data(data_dir=train_10_percents_dir)\n",
    "\n",
    "# 10 percents testing set\n",
    "test_10_percents_dir = DATASET_FOOD10_10_PERCENT_FOLDER_PATH.joinpath(\"test\")\n",
    "walk_through_data(data_dir=test_10_percents_dir)\n",
    "\n",
    "# full data training set\n",
    "train_full_data_dir = DATASET_FOOD10_FULL_FOLDER_PATH.joinpath(\"train\")\n",
    "walk_through_data(data_dir=train_full_data_dir)\n",
    "\n",
    "# full data testing set\n",
    "test_full_data_dir = DATASET_FOOD10_FULL_FOLDER_PATH.joinpath(\"test\")\n",
    "walk_through_data(data_dir=test_full_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names\n",
    "class_names = sorted(os.listdir(train_10_percents_dir))\n",
    "logging.info(f\"Class names: {class_names}\")\n",
    "logging.info(f\"len(class_names): {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "# data augmentation\n",
    "data_augmentation = keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2, 0.2),\n",
    "])\n",
    "\n",
    "# train 10 percent dataset\n",
    "train_10_percent_dataset = tf.keras.preprocessing.image_dataset_from_directory(directory=train_10_percents_dir, label_mode=\"int\", batch_size=batch_size, image_size=(224, 224), shuffle=True, seed=default_seed)\n",
    "\n",
    "# test 10 percent dataset\n",
    "test_10_percent_dataset = tf.keras.preprocessing.image_dataset_from_directory(directory=test_10_percents_dir, label_mode=\"int\", batch_size=batch_size, image_size=(224, 224), shuffle=False, seed=default_seed)\n",
    "\n",
    "# train full data dataset\n",
    "train_full_data_dataset = tf.keras.preprocessing.image_dataset_from_directory(directory=train_full_data_dir, label_mode=\"int\", batch_size=batch_size, image_size=(224, 224), shuffle=True, seed=default_seed)\n",
    "\n",
    "# test full data dataset\n",
    "test_full_data_dataset = tf.keras.preprocessing.image_dataset_from_directory(directory=test_full_data_dir, label_mode=\"int\", batch_size=batch_size, image_size=(224, 224), shuffle=False, seed=default_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model: tf.keras.Model = None):\n",
    "\n",
    "    if model is None:\n",
    "\n",
    "        logging.info(\"Feature extraction mode\")\n",
    "        # inputs\n",
    "        inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "        # data augmentation\n",
    "        x = data_augmentation(inputs)\n",
    "\n",
    "        # base model\n",
    "        base_model  = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "\n",
    "        # freezing all layers\n",
    "        base_model.trainable = False\n",
    "\n",
    "        x = base_model(x, training=False)\n",
    "\n",
    "        # global average pooling\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        dropout_rate = 0.2\n",
    "        logging.info(f\"Dropout rate: {dropout_rate}\")\n",
    "\n",
    "        # dropout\n",
    "        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "\n",
    "        # outputs\n",
    "        outputs = tf.keras.layers.Dense(units=len(class_names), activation=\"softmax\")(x)\n",
    "\n",
    "        # model\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "    else:\n",
    "        logging.info(\"Fine-tuning mode\")\n",
    "        model.trainable = True\n",
    "        for layer in model.layers[:-10]:\n",
    "            layer.trainable = False        \n",
    "\n",
    "    # summary\n",
    "    logging.info(model.summary(print_fn=logging.info))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_plot(feature_extraction_history: dict):\n",
    "\n",
    "    logging.info(\"Display history curves from feature extraction.\")\n",
    "\n",
    "    accuracy = feature_extraction_history[\"accuracy\"]\n",
    "    loss = feature_extraction_history[\"loss\"]\n",
    "    val_accuracy = feature_extraction_history[\"val_accuracy\"]\n",
    "    val_loss = feature_extraction_history[\"val_loss\"]\n",
    "\n",
    "    epochs = list(range(len(accuracy)))\n",
    "\n",
    "    # figure\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.suptitle(f\"Transfer Learning - Feature Extraction : {now_str}\")\n",
    "\n",
    "    # accuracy \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, accuracy, c=\"r\", label=\"Training dataset\")\n",
    "    plt.plot(epochs, val_accuracy, c=\"g\", label=\"Testing dataset\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # loss \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, c=\"r\", label=\"Training dataset\")\n",
    "    plt.plot(epochs, val_loss, c=\"g\", label=\"Testing dataset\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # save history\n",
    "    plt.savefig(FE_HISTORY_CURVES_FILE_PATH, bbox_inches=\"tight\")\n",
    "\n",
    "    logging.info(f\"Save history curves from feature extraction in this path: {FE_HISTORY_CURVES_FILE_PATH}\")\n",
    "\n",
    "    # display \n",
    "    # plt.show()\n",
    "\n",
    "def display_transfer_learning_plot(feature_extraction_history: dict, fine_tuning_history: dict, initial_epoch: int = 0):\n",
    "\n",
    "    logging.info(\"Display history curves from feature extraction and fine-tuning.\")\n",
    "\n",
    "    accuracy = feature_extraction_history[\"accuracy\"]\n",
    "    loss = feature_extraction_history[\"loss\"]\n",
    "    val_accuracy = feature_extraction_history[\"val_accuracy\"]\n",
    "    val_loss = feature_extraction_history[\"val_loss\"]\n",
    "\n",
    "    start_fine_tuning_epoch = initial_epoch\n",
    "    logging.info(f\"Start fine-tuning from initial_epoch: {initial_epoch}.\")\n",
    "    \n",
    "    accuracy += fine_tuning_history[\"accuracy\"]\n",
    "    loss += fine_tuning_history[\"loss\"]\n",
    "    val_accuracy += fine_tuning_history[\"val_accuracy\"]\n",
    "    val_loss += fine_tuning_history[\"val_loss\"]\n",
    "\n",
    "    epochs = list(range(len(accuracy)))\n",
    "    \n",
    "    # figure\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.suptitle(f\"Transfer Learning - Fine-tuning : {now_str}\")\n",
    "\n",
    "\n",
    "    # accuracy \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, accuracy, c=\"r\", label=\"Training dataset\")\n",
    "    plt.plot(epochs, val_accuracy, c=\"g\", label=\"Testing dataset\")\n",
    "    plt.plot([start_fine_tuning_epoch, start_fine_tuning_epoch], plt.ylim(), c=\"b\", label=\"Start fine-tuning\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # loss \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, c=\"r\", label=\"Training dataset\")\n",
    "    plt.plot(epochs, val_loss, c=\"g\", label=\"Testing dataset\")\n",
    "    plt.plot([start_fine_tuning_epoch, start_fine_tuning_epoch], plt.ylim(), c=\"b\", label=\"Start fine-tuning\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # save history\n",
    "    plt.savefig(FT_HISTORY_CURVES_FILE_PATH, bbox_inches=\"tight\")\n",
    "\n",
    "    logging.info(f\"Save history curves from from feature extraction and fine-tuning in this path:{FT_HISTORY_CURVES_FILE_PATH}\")\n",
    "    # display \n",
    "    # plt.show()\n",
    "\n",
    "def get_gpu_util():\n",
    "\n",
    "    logging.info(\"GPU status\")\n",
    "    nvidia_smi.nvmlInit()\n",
    "    deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
    "    host_name = socket.gethostname()\n",
    "    now = datetime.now().strftime(\"%a, %d %b %Y %H:%M:%S\")\n",
    "\n",
    "    for i in range(deviceCount):\n",
    "        \n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
    "\n",
    "        util = nvidia_smi.nvmlDeviceGetUtilizationRates(handle)\n",
    "\n",
    "        device_name = nvidia_smi.nvmlDeviceGetName(handle).decode('utf-8')\n",
    "        \n",
    "        memInfo = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "    \n",
    "        mem_total = int(memInfo.total / 1024 / 1024)\n",
    "\n",
    "        mem_used =  int(memInfo.used / 1024 / 1024)\n",
    "\n",
    "        mem_free =  int(mem_total - mem_used)\n",
    "        \n",
    "        gpu_util =  int(util.gpu/100.0)\n",
    "        \n",
    "        gpu_mem =   int(util.memory/100.0)\n",
    "        \n",
    "        logging.info(f\"{host_name} - [{i}] {device_name} | {mem_used} / {mem_total} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet  = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "\n",
    "# summary\n",
    "efficientnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epochs: int, wandb: Any, is_wandb: bool = True):\n",
    "        super().__init__()\n",
    "        self.epochs = epochs\n",
    "        self.wandb = wandb\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs = None):\n",
    "        self.start_timer = timer() \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        self.end_timer = timer()\n",
    "        duration = self.end_timer - self.start_timer\n",
    "\n",
    "        accuracy = logs.get(\"accuracy\")\n",
    "        loss = logs.get(\"loss\")\n",
    "        val_accuracy = logs.get(\"val_accuracy\")\n",
    "        val_loss = logs.get(\"val_loss\")\n",
    "\n",
    "        logging.info(f\"Epoch: {epoch + 1} / {self.epochs} | \"\n",
    "                     f\"{duration:.2f}s | \"\n",
    "                     f\"accuracy: {accuracy:.2f}| \"\n",
    "                     f\"loss: {loss: .3f} | \"\n",
    "                     f\"val_accuracy: {val_accuracy: .2f} | \"\n",
    "                     f\"val_loss: {val_loss:.3f}\"\n",
    "                     )\n",
    "\n",
    "        # history dict\n",
    "        history_dict = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"loss\": loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "        }\n",
    "\n",
    "        if is_wandb:\n",
    "            self.wandb.log(history_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction with freezing all layers except top layers\n",
    "feature_extraction_model = create_model()\n",
    "\n",
    "# epochs\n",
    "feature_extraction_epochs = 10\n",
    "logging.info(f\"Feature extraction (epochs): {feature_extraction_epochs}\")\n",
    "\n",
    "# loss function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "logging.info(f\"Feature extraction (loss_fn): SparseCategoricalCrossentropy\")\n",
    "\n",
    "# optimizer\n",
    "feature_extraction_lr = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=feature_extraction_lr)\n",
    "logging.info(f\"Feature extraction (optimizer): Adam\")\n",
    "logging.info(f\"Feature extraction (lr): {feature_extraction_lr}\")\n",
    "\n",
    "if is_wandb:\n",
    "    # init wandb\n",
    "    wandb.init(project=\"tensorflow-deep-learning\", name=f\"tadac/{project_name}_feature_extraction\", config={\n",
    "        \"batch_size\": batch_size,\n",
    "        \"architecture\": model_name,\n",
    "        \"learing_rate\": feature_extraction_lr,\n",
    "        \"dataset\": \"10_food_classes_10_percent\",\n",
    "        \"epochs\": feature_extraction_epochs,\n",
    "    })\n",
    "\n",
    "# callbacks\n",
    "fe_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=MODEL_FEATURE_EXTRACTION_DIR, save_best_only=True, save_weights_only=True, verbose=0)\n",
    "history_callback = HistoryCallback(epochs=feature_extraction_epochs, wandb=wandb, is_wandb=is_wandb)\n",
    "\n",
    "# compile \n",
    "feature_extraction_model.compile(loss=loss_fn,\n",
    "                                 optimizer=optimizer, \n",
    "                                 metrics=[\"accuracy\"])\n",
    "\n",
    "logging.info(f\"len(train_10_percent_dataset): {len(train_10_percent_dataset)}\")\n",
    "logging.info(f\"steps_per_epoch: {len(train_10_percent_dataset) / batch_size}\")\n",
    "logging.info(f\"len(test_10_percent_dataset): {len(test_10_percent_dataset)}\")\n",
    "logging.info(f\"validation_steps: {len(test_10_percent_dataset)/ batch_size}\")\n",
    "\n",
    "# fit\n",
    "# 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "feature_extraction_history = feature_extraction_model.fit(train_10_percent_dataset, \n",
    "                                                           epochs=feature_extraction_epochs,\n",
    "                                                           steps_per_epoch=len(train_10_percent_dataset), \n",
    "                                                           validation_data=test_10_percent_dataset, \n",
    "                                                           validation_steps=len(test_10_percent_dataset), \n",
    "                                                           shuffle=True, verbose=0,\n",
    "                                                           workers=cpus, use_multiprocessing= True,\n",
    "                                                           callbacks=[fe_checkpoint_callback, history_callback])\n",
    "\n",
    "if is_wandb:\n",
    "    # finish wandb\n",
    "    wandb.finish()\n",
    "\n",
    "# display plot\n",
    "pd.DataFrame(feature_extraction_history.history).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gpu status\n",
    "get_gpu_util()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### History Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display history curves of feature extraction \n",
    "display_plot(feature_extraction_history=feature_extraction_history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model\n",
    "# finds the filename of latest saved checkpoint file and returns a path of the latest one\n",
    "latest_fe_ckpt_file_path = tf.train.latest_checkpoint(str(MODEL_FEATURE_EXTRACTION_DIR).replace(\"checkpoint.ckpt\", \"\"))\n",
    "logging.info(f\"Loading the feature extraction model from a saved path: {latest_fe_ckpt_file_path}\")\n",
    "\n",
    "# create an instance of the model\n",
    "loaded_fe_model = create_model()\n",
    "\n",
    "# load the latest weights \n",
    "loaded_fe_model.load_weights(filepath=latest_fe_ckpt_file_path)\n",
    "\n",
    "# load an image for evaluation\n",
    "test_image_file_path = DATA_DIR.joinpath('sushi_test.jpg')\n",
    "logging.info(f\"Using the {test_image_file_path} for evaluation\")\n",
    "if test_image_file_path.is_file():\n",
    "\n",
    "    # load an image and convert it to (224, 224)\n",
    "    pil_image = tf.keras.utils.load_img(path=test_image_file_path, target_size=(224, 224))\n",
    "    \n",
    "    # convert its image to array\n",
    "    arr_image = img_to_array(pil_image)\n",
    "\n",
    "    # create a batch\n",
    "    image_t = tf.expand_dims(arr_image, axis=0)\n",
    "\n",
    "    # y probabilities \n",
    "    y_probs = loaded_fe_model.predict(image_t)\n",
    "    y_predicted_prob = tf.reduce_max(y_probs)\n",
    "\n",
    "    # predicted label id\n",
    "    y_predicted_label_id = tf.argmax(y_probs, axis=1)\n",
    "    y_predicted_label = class_names[int(y_predicted_label_id)]\n",
    "    logging.info(f\"Predicted: {y_predicted_label}\")\n",
    "    logging.info(f\"Predicted probability: {y_predicted_prob: .2f}\")\n",
    "\n",
    "    # predicted probability\n",
    "    y_predicted_label_prob = tf.reduce_max(y_probs)\n",
    "\n",
    "    # save file\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.suptitle(f\"Feature Extraction - Prediction - {now_str}\")\n",
    "    plt.imshow(pil_image)\n",
    "    plt.title(f\"Predicted: {y_predicted_label} | Prob: {y_predicted_prob: .2f}\")\n",
    "    plt.savefig(FE_PREDICTION_FILE_PATH, bbox_inches=\"tight\")\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(\"There is no image in the given path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction with freezing all layers except top layers\n",
    "fine_tuning_model = create_model(model=feature_extraction_model)\n",
    "\n",
    "# epochs\n",
    "fine_tuning_epochs = feature_extraction_epochs + 10\n",
    "logging.info(f\"Fine tuning (epochs): {fine_tuning_epochs}\")\n",
    "\n",
    "# loss function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "logging.info(f\"Fine tuning (loss_fn): SparseCategoricalCrossentropy\")\n",
    "\n",
    "# optimizer\n",
    "fine_tuning_lr = 0.0001 # 10x lower\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=fine_tuning_lr)\n",
    "logging.info(f\"Fine tuning (optimizer): Adam\")\n",
    "logging.info(f\"Fine tuning (lr): {fine_tuning_lr}\")\n",
    "\n",
    "# init wandb\n",
    "wandb.init(project=\"tensorflow-deep-learning\", name=f\"tadac/{project_name}_fine_tuning\", config={\n",
    "    \"batch_size\": batch_size,\n",
    "    \"architecture\": model_name,\n",
    "    \"learing_rate\": fine_tuning_lr,\n",
    "    \"dataset\": \"food10\",\n",
    "    \"epochs\": fine_tuning_epochs,\n",
    "})\n",
    "\n",
    "if is_wandb:\n",
    "    # add logs from the feature history dict\n",
    "    for i in range(len(feature_extraction_history.history[\"accuracy\"])):\n",
    "        # wandb dict\n",
    "        fe_wantdb_dict = {\n",
    "            \"accuracy\": feature_extraction_history.history[\"accuracy\"][i], \n",
    "            \"loss\" : feature_extraction_history.history[\"loss\"][i],\n",
    "            \"val_accuracy\": feature_extraction_history.history[\"val_accuracy\"][i],\n",
    "            \"val_loss\": feature_extraction_history.history[\"val_loss\"][i]\n",
    "        }\n",
    "\n",
    "        # wandb logging\n",
    "        wandb.log(fe_wantdb_dict)\n",
    "\n",
    "# callbacks\n",
    "fine_tuning_model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=MODEL_FINE_TUNING_DIR, save_best_only=True, save_weights_only=True, verbose=0)\n",
    "\n",
    "history_callback = HistoryCallback(epochs=fine_tuning_epochs, wandb=wandb, is_wandb=is_wandb)\n",
    "\n",
    "# compile \n",
    "fine_tuning_model.compile(loss=loss_fn,\n",
    "                                 optimizer=optimizer, \n",
    "                                 metrics=[\"accuracy\"])\n",
    "\n",
    "logging.info(f\"len(train_full_data_dataset): {len(train_full_data_dataset)}\")\n",
    "logging.info(f\"steps_per_epoch: {len(train_full_data_dataset) / batch_size}\")\n",
    "logging.info(f\"len(test_full_data_dataset): {len(test_full_data_dataset)}\")\n",
    "logging.info(f\"validation_steps: {len(test_full_data_dataset)/ batch_size}\")\n",
    "\n",
    "# fit\n",
    "#  0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "fine_tuning_history = fine_tuning_model.fit(train_full_data_dataset, initial_epoch= feature_extraction_epochs,\n",
    "                                                           epochs=fine_tuning_epochs,\n",
    "                                                           steps_per_epoch=len(train_full_data_dataset), \n",
    "                                                           validation_data=test_full_data_dataset, \n",
    "                                                           validation_steps=len(test_full_data_dataset), \n",
    "                                                           shuffle=True, verbose=0,\n",
    "                                                           callbacks=[fine_tuning_model_checkpoint_callback, history_callback])\n",
    "\n",
    "\n",
    "if is_wandb:\n",
    "    # finish wandb\n",
    "    wandb.finish()\n",
    "\n",
    "# display plot\n",
    "pd.DataFrame(fine_tuning_history.history).plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gpu status\n",
    "get_gpu_util()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### History Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display history curves of both feature extraction and fine-tuning\n",
    "display_transfer_learning_plot(feature_extraction_history = feature_extraction_history.history, fine_tuning_history = fine_tuning_history.history, initial_epoch=feature_extraction_history.epoch[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model\n",
    "# finds the filename of latest saved checkpoint file and returns a path of the latest one\n",
    "latest_ft_ckpt_file_path = tf.train.latest_checkpoint(str(MODEL_FINE_TUNING_DIR).replace(\"checkpoint.ckpt\", \"\"))\n",
    "logging.info(f\"Loading the fine-tuning model from a saved path: {latest_ft_ckpt_file_path}\")\n",
    "\n",
    "# create an instance of the model\n",
    "loaded_ft_model = create_model()\n",
    "\n",
    "# load the latest weights \n",
    "loaded_ft_model.load_weights(filepath=latest_ft_ckpt_file_path)\n",
    "\n",
    "# load an image for evaluation\n",
    "test_image_file_path = DATA_DIR.joinpath('sushi_test.jpg')\n",
    "logging.info(f\"Using the {test_image_file_path} for evaluation\")\n",
    "if test_image_file_path.is_file():\n",
    "\n",
    "    # load an image and convert it to (224, 224)\n",
    "    pil_image = tf.keras.utils.load_img(path=test_image_file_path, target_size=(224, 224))\n",
    "    \n",
    "    # convert its image to array\n",
    "    arr_image = img_to_array(pil_image)\n",
    "\n",
    "    # create a batch\n",
    "    image_t = tf.expand_dims(arr_image, axis=0)\n",
    "\n",
    "    # y probabilities \n",
    "    y_probs = loaded_ft_model.predict(image_t)\n",
    "    y_predicted_prob = tf.reduce_max(y_probs)\n",
    "\n",
    "    # predicted label id\n",
    "    y_predicted_label_id = tf.argmax(y_probs, axis=1)\n",
    "    y_predicted_label = class_names[int(y_predicted_label_id)]\n",
    "    logging.info(f\"Predicted: {y_predicted_label}\")\n",
    "    logging.info(f\"Predicted probability: {y_predicted_prob: .2f}\")\n",
    "\n",
    "    # predicted probability\n",
    "    y_predicted_label_prob = tf.reduce_max(y_probs)\n",
    "\n",
    "    # save file\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.suptitle(f\"Fine-tuning - Prediction - {now_str}\")\n",
    "    plt.imshow(pil_image)\n",
    "    plt.title(f\"Predicted: {y_predicted_label} | Prob: {y_predicted_prob: .2f}\")\n",
    "    plt.savefig(FT_PREDICTION_FILE_PATH, bbox_inches=\"tight\")\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(\"There is no image in the given path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\n",
    "2. https://www.tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing \n",
    "3. https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tadac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
